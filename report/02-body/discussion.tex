% ---------------------------------------------------------------------
% Cloned from the HES-SO//Master canvas 2019
% ---------------------------------------------------------------------
\chapter{Discussion}
\label{chap:discussion}

\section{Next steps?}
Comparing the initial speculative plan [\ref{plan:initial}] and the effective plan [\ref{plan:effective}], it is observable that the aiming was made toward the expertise acquisition of the Word Embedding technology Word2Vec. Even if much knowledge has been gathered during the \gls{dp}, there is always more to be learned and experimented as a way to dive further into the Word2Vec expertise.

\subsection{Memoir}
One of the motivations for this \gls{dp} was to make a premise to the author's memoir, which will be using word embedding as a foundation. Indeed, the understanding of Word2Vec will help the incoming design thinking to make a deep retrieval chatbot using topic extractions.

\subsection{\gls{agi}}
Another motivation of this project was to explore and understand Word2Vec extensions initiating General Chatbots [\ref{sota:chatbots-general}] such as the recent Contextual Embedding [\ref{sota:contextual-embedding}] and the predicted Though Embedding [\ref{sota:though-embedding}]. Even if the author did not have the time to get into details of those promising evolutions, he would be keeping an eye on those, and even maybe be able to explore further during his memoir.

\subsection{\gls{dl}}
As discussed briefly during the state of art chapter, Word Embedding is an alien in the current \gls{ml} world, where almost everything is made out of \gls{dnn} [\ref{sota:rnn}]. However, the claim that the \textit{standard} {nn} from Word Embedding is more performant than a \gls{dnn} will be quickly updated. Indeed, with the current progress in \gls{s2s} due to a market need, frameworks like Keras which are backed by big companies such are Google is providing out the box indirectly Word Embedding as it is required for \gls{s2s}. Word Embedding is a solid foundation for incoming new types of \textit{Embeddings}, and its performance will more than probably increase over time with new algorithms able to catch better contextual information about words.

\subsection{Benchmarking}
The most regretted author's section, the lack of time could not allow diving into benchmarking. Indeed, various models have been computed [\ref{appendix:models-spreadsheet}] to evaluate and compare them to each other. Comparing Wikipedia models to find out which parameters are providing the best accuracy [\ref{analyse:evaluation}], and how it compares to public pre-trained models from Google, for instance.
