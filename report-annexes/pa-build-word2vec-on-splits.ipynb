{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start logging process at root level\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os, os.path\n",
    "\n",
    "chunk_size = 99999\n",
    "chunk_start_at = 5\n",
    "chunks_basepath = \"datasets/chunks/enwiki-chunks-\"+str(chunk_size)\n",
    "model_path = \"models/enwiki-full-dict-\"+str(chunk_size)+\".model\"\n",
    "dictionary_path = \"dictionaries/enwiki-english-lemmatized.txt.bz2\"\n",
    "chunks_count = len([name for name in os.listdir(chunks_basepath) if os.path.isfile(os.path.join(chunks_basepath, name))])\n",
    "#chunks_count = 4\n",
    "\n",
    "lemmatization = True\n",
    "w2v_size = 300\n",
    "w2v_window = 5\n",
    "#w2v_cores = multiprocessing.cpu_count()-10\n",
    "w2v_cores = 20\n",
    "w2v_min_count = 1\n",
    "w2v_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-03 00:21:45,633 : INFO : 'pattern' package found; tag filters are available for English\n"
     ]
    }
   ],
   "source": [
    "# Load dictionary from file\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary.load_from_text(dictionary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simple sentence iterator required for the Word2Vec model / memory savior\n",
    "class SentencesIterator:\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sentence in self.wiki.get_texts():\n",
    "            yield list(map(lambda x: x.decode('utf-8'), sentence)) #set encode('utf-8') when lemmatized = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "for e in range(chunk_start_at,chunks_count+1):\n",
    "    chunk_name = chunks_basepath+\"/\"+\"enwiki-chunk-\"+str(chunk_size)+\"-\"+str(e)+\".xml.bz2\"\n",
    "    model_backup_basepath = \"models/enwiki-full-dict-parts-\"+str(chunk_size)\n",
    "    model_backup_path = model_backup_basepath+\"/enwiki-full-dict-\"+str(chunk_size)+\"-part-\"+str(e)+\".model\"\n",
    "    \n",
    "    if not os.path.exists(model_backup_basepath):\n",
    "        print(\"Mkdir the model base path\")\n",
    "        os.mkdir(model_backup_basepath)\n",
    "        \n",
    "    # Build WikiCorpus based on the dictionary\n",
    "    wiki = WikiCorpus(chunk_name, dictionary=dictionary,lemmatize=lemmatization)\n",
    "    \n",
    "    # Set generator\n",
    "    sentences = SentencesIterator(wiki)\n",
    "    \n",
    "    print(\"Running with: \",str(w2v_cores),\" cores\")\n",
    "    print(\"Selected model:\",model_path)\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"Building model on:\",chunk_name)\n",
    "        # Build model on first part\n",
    "        model = Word2Vec(sentences=sentences,\n",
    "                         size=w2v_size,\n",
    "                         min_count=w2v_min_count,\n",
    "                         window=w2v_window,\n",
    "                         workers=w2v_cores,\n",
    "                         iter=w2v_epochs\n",
    "                        )\n",
    "        model.save(model_path)\n",
    "        model.save(model_backup_path)\n",
    "        print(\"Model saved\")\n",
    "    else:\n",
    "        # Train model on anthor part\n",
    "        print(\"Training model on:\",chunk_name)\n",
    "        model = Word2Vec.load(model_path)\n",
    "        model.train(sentences=sentences, total_examples=model.corpus_count, epochs=model.epochs, queue_factor=1)\n",
    "        model.save(model_path)\n",
    "        model.save(model_backup_path)\n",
    "        print(\"Model updated\")\n",
    "    \n",
    "    print(\"Backup model saved:\",model_backup_path)\n",
    "        \n",
    "    del wiki\n",
    "    del model\n",
    "    del sentences\n",
    "\n",
    "del dictionary\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
