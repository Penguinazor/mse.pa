{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Tutorial – A Complete Beginners Guide\n",
    "https://www.machinelearningplus.com/nlp/gensim-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a dictionary from a list of sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new document into an existing dictionary\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "\n",
    "print(dictionary)\n",
    "\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create gensim dictionary form a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 20:14:56,935 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-03-19 20:14:56,938 : INFO : built Dictionary(93 unique tokens: ['enjoy', 'capabilities', 'to', 'and', 'technology']...) from 11 documents (total 158 corpus positions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'according': 35,\n",
       " 'and': 22,\n",
       " 'appointment': 23,\n",
       " 'army': 0,\n",
       " 'as': 43,\n",
       " 'at': 24,\n",
       " 'better': 85,\n",
       " 'by': 36,\n",
       " 'capabilities': 86,\n",
       " 'ceremony': 25,\n",
       " 'china': 1,\n",
       " 'chinese': 2,\n",
       " 'civilian': 75,\n",
       " 'combat': 87,\n",
       " 'companies': 14,\n",
       " 'conduct': 48,\n",
       " 'contribute': 88,\n",
       " 'could': 89,\n",
       " 'counterparts': 66,\n",
       " 'daily': 15,\n",
       " 'deepening': 76,\n",
       " 'defense': 37,\n",
       " 'design': 49,\n",
       " 'development': 77,\n",
       " 'enhancement': 90,\n",
       " 'enjoy': 67,\n",
       " 'experts': 26,\n",
       " 'fellow': 44,\n",
       " 'fields': 50,\n",
       " 'firms': 68,\n",
       " 'five': 58,\n",
       " 'for': 59,\n",
       " 'force': 3,\n",
       " 'founding': 27,\n",
       " 'from': 16,\n",
       " 'hao': 28,\n",
       " 'his': 45,\n",
       " 'honored': 46,\n",
       " 'in': 78,\n",
       " 'innovation': 91,\n",
       " 'integration': 79,\n",
       " 'into': 51,\n",
       " 'launching': 60,\n",
       " 'letters': 29,\n",
       " 'liberation': 4,\n",
       " 'like': 52,\n",
       " 'make': 92,\n",
       " 'marks': 80,\n",
       " 'members': 53,\n",
       " 'military': 81,\n",
       " 'missile': 61,\n",
       " 'missiles': 62,\n",
       " 'national': 38,\n",
       " 'network': 63,\n",
       " 'new': 82,\n",
       " 'of': 5,\n",
       " 'on': 17,\n",
       " 'other': 30,\n",
       " 'overall': 54,\n",
       " 'owned': 69,\n",
       " 'panel': 39,\n",
       " 'people': 6,\n",
       " 'pla': 18,\n",
       " 'private': 19,\n",
       " 'published': 40,\n",
       " 'received': 31,\n",
       " 'recently': 7,\n",
       " 'recruited': 8,\n",
       " 'report': 41,\n",
       " 'reported': 20,\n",
       " 'research': 55,\n",
       " 'rocket': 9,\n",
       " 'said': 70,\n",
       " 'same': 71,\n",
       " 'saturday': 21,\n",
       " 'science': 32,\n",
       " 'serve': 56,\n",
       " 'state': 72,\n",
       " 'system': 64,\n",
       " 'tank': 10,\n",
       " 'technicians': 11,\n",
       " 'technology': 33,\n",
       " 'that': 83,\n",
       " 'the': 12,\n",
       " 'their': 73,\n",
       " 'think': 13,\n",
       " 'this': 84,\n",
       " 'to': 42,\n",
       " 'treatment': 74,\n",
       " 'which': 57,\n",
       " 'will': 47,\n",
       " 'years': 65,\n",
       " 'zhang': 34}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('sample.txt', encoding='utf-8'))\n",
    "\n",
    "# Token to Id map\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 20:14:58,195 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-03-19 20:14:58,321 : INFO : built Dictionary(525 unique tokens: ['invented', 'pan', 'commercially', 'prepared', 'once']...) from 19 documents (total 1204 corpus positions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accompanying': 355,\n",
       " 'according': 42,\n",
       " 'achaya': 43,\n",
       " 'across': 158,\n",
       " 'activity': 159,\n",
       " 'ad': 44,\n",
       " 'added': 356,\n",
       " 'advances': 509,\n",
       " 'advantage': 281,\n",
       " 'after': 401,\n",
       " 'ago': 357,\n",
       " 'aid': 470,\n",
       " 'all': 0,\n",
       " 'allow': 282,\n",
       " 'along': 1,\n",
       " 'already': 45,\n",
       " 'also': 79,\n",
       " 'alters': 283,\n",
       " 'although': 160,\n",
       " 'america': 254,\n",
       " 'amongst': 145,\n",
       " 'an': 203,\n",
       " 'ancient': 46,\n",
       " 'and': 2,\n",
       " 'another': 146,\n",
       " 'any': 402,\n",
       " 'are': 3,\n",
       " 'areas': 204,\n",
       " 'around': 47,\n",
       " 'as': 4,\n",
       " 'association': 80,\n",
       " 'associazione': 235,\n",
       " 'at': 284,\n",
       " 'attempts': 403,\n",
       " 'available': 471,\n",
       " 'back': 285,\n",
       " 'badminton': 161,\n",
       " 'baked': 205,\n",
       " 'baking': 447,\n",
       " 'ball': 286,\n",
       " 'baseball': 504,\n",
       " 'bases': 510,\n",
       " 'bat': 404,\n",
       " 'batsmen': 405,\n",
       " 'batter': 5,\n",
       " 'batting': 406,\n",
       " 'be': 6,\n",
       " 'beach': 162,\n",
       " 'beans': 7,\n",
       " 'became': 81,\n",
       " 'because': 82,\n",
       " 'become': 206,\n",
       " 'been': 358,\n",
       " 'between': 407,\n",
       " 'birthplace': 48,\n",
       " 'black': 111,\n",
       " 'body': 112,\n",
       " 'boiling': 359,\n",
       " 'both': 485,\n",
       " 'bounce': 287,\n",
       " 'bounces': 288,\n",
       " 'breakfast': 113,\n",
       " 'breaks': 114,\n",
       " 'broad': 448,\n",
       " 'but': 329,\n",
       " 'by': 83,\n",
       " 'cake': 115,\n",
       " 'cakes': 116,\n",
       " 'called': 408,\n",
       " 'calzone': 255,\n",
       " 'can': 8,\n",
       " 'casual': 163,\n",
       " 'categories': 449,\n",
       " 'cavatelli': 486,\n",
       " 'central': 207,\n",
       " 'centre': 409,\n",
       " 'century': 49,\n",
       " 'cereals': 450,\n",
       " 'cheese': 208,\n",
       " 'china': 360,\n",
       " 'chutney': 9,\n",
       " 'clockwise': 511,\n",
       " 'come': 487,\n",
       " 'commercially': 472,\n",
       " 'common': 164,\n",
       " 'commonly': 209,\n",
       " 'compiled': 84,\n",
       " 'completed': 410,\n",
       " 'composition': 361,\n",
       " 'condiments': 210,\n",
       " 'conjecture': 50,\n",
       " 'consisting': 117,\n",
       " 'consumed': 10,\n",
       " 'consumption': 362,\n",
       " 'cook': 256,\n",
       " 'cooked': 363,\n",
       " 'cooking': 364,\n",
       " 'counter': 512,\n",
       " 'countries': 118,\n",
       " 'country': 51,\n",
       " 'court': 165,\n",
       " 'cricket': 411,\n",
       " 'crispier': 85,\n",
       " 'cuisine': 451,\n",
       " 'cultural': 365,\n",
       " 'cultures': 330,\n",
       " 'cut': 331,\n",
       " 'dating': 452,\n",
       " 'day': 52,\n",
       " 'de': 119,\n",
       " 'decorative': 488,\n",
       " 'deep': 366,\n",
       " 'demands': 289,\n",
       " 'depending': 489,\n",
       " 'derives': 367,\n",
       " 'designated': 412,\n",
       " 'diameter': 120,\n",
       " 'diet': 11,\n",
       " 'different': 490,\n",
       " 'discussing': 368,\n",
       " 'dish': 86,\n",
       " 'dishes': 257,\n",
       " 'dismissed': 413,\n",
       " 'divided': 290,\n",
       " 'documented': 491,\n",
       " 'dosa': 12,\n",
       " 'dosai': 53,\n",
       " 'dosaka': 87,\n",
       " 'doubles': 166,\n",
       " 'dough': 332,\n",
       " 'down': 121,\n",
       " 'dried': 369,\n",
       " 'during': 414,\n",
       " 'durum': 453,\n",
       " 'each': 415,\n",
       " 'eat': 333,\n",
       " 'eaten': 334,\n",
       " 'eggs': 454,\n",
       " 'either': 258,\n",
       " 'eleven': 416,\n",
       " 'encyclopedia': 88,\n",
       " 'end': 417,\n",
       " 'ends': 418,\n",
       " 'enduri': 147,\n",
       " 'europe': 259,\n",
       " 'european': 236,\n",
       " 'evidence': 370,\n",
       " 'exact': 54,\n",
       " 'example': 492,\n",
       " 'except': 291,\n",
       " 'exist': 260,\n",
       " 'extracted': 335,\n",
       " 'extras': 419,\n",
       " 'extruded': 336,\n",
       " 'extrusion': 473,\n",
       " 'fails': 292,\n",
       " 'far': 337,\n",
       " 'fast': 261,\n",
       " 'fermentation': 122,\n",
       " 'fermented': 13,\n",
       " 'field': 420,\n",
       " 'fielding': 505,\n",
       " 'filled': 493,\n",
       " 'first': 89,\n",
       " 'five': 123,\n",
       " 'flat': 338,\n",
       " 'flatbread': 211,\n",
       " 'flour': 455,\n",
       " 'folded': 371,\n",
       " 'follows': 293,\n",
       " 'food': 55,\n",
       " 'for': 90,\n",
       " 'form': 339,\n",
       " 'formal': 167,\n",
       " 'formed': 456,\n",
       " 'forms': 168,\n",
       " 'forth': 294,\n",
       " 'found': 91,\n",
       " 'founded': 237,\n",
       " 'four': 124,\n",
       " 'fresca': 457,\n",
       " 'fresh': 262,\n",
       " 'fried': 372,\n",
       " 'from': 14,\n",
       " 'frozen': 263,\n",
       " 'future': 373,\n",
       " 'gaeta': 212,\n",
       " 'gained': 421,\n",
       " 'game': 169,\n",
       " 'games': 170,\n",
       " 'generally': 213,\n",
       " 'geo': 374,\n",
       " 'german': 375,\n",
       " 'giving': 295,\n",
       " 'goans': 148,\n",
       " 'grains': 458,\n",
       " 'great': 296,\n",
       " 'guaranteed': 238,\n",
       " 'half': 171,\n",
       " 'hand': 474,\n",
       " 'hard': 297,\n",
       " 'has': 376,\n",
       " 'have': 214,\n",
       " 'having': 494,\n",
       " 'headquarters': 239,\n",
       " 'helices': 377,\n",
       " 'historian': 56,\n",
       " 'hit': 172,\n",
       " 'hitter': 298,\n",
       " 'hitting': 513,\n",
       " 'home': 514,\n",
       " 'hot': 15,\n",
       " 'households': 125,\n",
       " 'husked': 126,\n",
       " 'idli': 16,\n",
       " 'iii': 92,\n",
       " 'in': 57,\n",
       " 'inches': 127,\n",
       " 'include': 495,\n",
       " 'including': 422,\n",
       " 'india': 58,\n",
       " 'indian': 17,\n",
       " 'indigenous': 59,\n",
       " 'indoor': 173,\n",
       " 'ingredients': 18,\n",
       " 'initial': 299,\n",
       " 'innings': 423,\n",
       " 'into': 340,\n",
       " 'invented': 215,\n",
       " 'is': 19,\n",
       " 'it': 20,\n",
       " 'italian': 459,\n",
       " 'italy': 216,\n",
       " 'item': 264,\n",
       " 'its': 21,\n",
       " 'karnataka': 60,\n",
       " 'kind': 22,\n",
       " 'known': 149,\n",
       " 'konkani': 150,\n",
       " 'landing': 174,\n",
       " 'lanka': 128,\n",
       " 'large': 475,\n",
       " 'larger': 175,\n",
       " 'latin': 217,\n",
       " 'least': 300,\n",
       " 'lentils': 129,\n",
       " 'lightweight': 301,\n",
       " 'like': 130,\n",
       " 'limits': 302,\n",
       " 'linked': 93,\n",
       " 'literature': 61,\n",
       " 'locale': 496,\n",
       " 'long': 378,\n",
       " 'machines': 476,\n",
       " 'made': 23,\n",
       " 'main': 24,\n",
       " 'manasollasa': 94,\n",
       " 'manuscript': 218,\n",
       " 'many': 219,\n",
       " 'material': 379,\n",
       " 'matter': 62,\n",
       " 'may': 176,\n",
       " 'meats': 220,\n",
       " 'metabolised': 131,\n",
       " 'miniature': 497,\n",
       " 'mixed': 460,\n",
       " 'modern': 221,\n",
       " 'more': 132,\n",
       " 'most': 177,\n",
       " 'must': 303,\n",
       " 'nair': 63,\n",
       " 'names': 498,\n",
       " 'naples': 222,\n",
       " 'napoletana': 240,\n",
       " 'neapolitan': 241,\n",
       " 'neighbouring': 133,\n",
       " 'net': 178,\n",
       " 'nine': 506,\n",
       " 'non': 242,\n",
       " 'noodle': 341,\n",
       " 'noodles': 342,\n",
       " 'north': 265,\n",
       " 'nudel': 380,\n",
       " 'number': 424,\n",
       " 'odisha': 151,\n",
       " 'of': 25,\n",
       " 'often': 179,\n",
       " 'oil': 381,\n",
       " 'oldest': 382,\n",
       " 'on': 180,\n",
       " 'once': 304,\n",
       " 'one': 181,\n",
       " 'opponent': 305,\n",
       " 'opponents': 425,\n",
       " 'opposing': 182,\n",
       " 'opposite': 306,\n",
       " 'options': 307,\n",
       " 'or': 183,\n",
       " 'organisation': 243,\n",
       " 'origin': 95,\n",
       " 'original': 96,\n",
       " 'originated': 64,\n",
       " 'other': 152,\n",
       " 'outdoor': 184,\n",
       " 'oven': 223,\n",
       " 'ovens': 266,\n",
       " 'over': 26,\n",
       " 'overs': 426,\n",
       " 'paddle': 308,\n",
       " 'pan': 383,\n",
       " 'pancake': 27,\n",
       " 'parcel': 28,\n",
       " 'part': 29,\n",
       " 'pasta': 461,\n",
       " 'pastas': 462,\n",
       " 'people': 153,\n",
       " 'per': 65,\n",
       " 'period': 427,\n",
       " 'phase': 428,\n",
       " 'ping': 309,\n",
       " 'pitch': 429,\n",
       " 'pitcher': 515,\n",
       " 'pitha': 154,\n",
       " 'pizza': 224,\n",
       " 'place': 310,\n",
       " 'plate': 516,\n",
       " 'play': 311,\n",
       " 'played': 185,\n",
       " 'player': 186,\n",
       " 'players': 187,\n",
       " 'plural': 343,\n",
       " 'podi': 30,\n",
       " 'point': 312,\n",
       " 'points': 188,\n",
       " 'pong': 313,\n",
       " 'popular': 31,\n",
       " 'portions': 267,\n",
       " 'possible': 430,\n",
       " 'preparation': 268,\n",
       " 'prepared': 269,\n",
       " 'present': 66,\n",
       " 'probably': 97,\n",
       " 'process': 134,\n",
       " 'produced': 477,\n",
       " 'products': 478,\n",
       " 'profit': 244,\n",
       " 'promotes': 245,\n",
       " 'protects': 246,\n",
       " 'quick': 314,\n",
       " 'racquet': 189,\n",
       " 'racquets': 190,\n",
       " 'reactions': 315,\n",
       " 'readily': 135,\n",
       " 'recipe': 98,\n",
       " 'recorded': 225,\n",
       " 'rectangular': 191,\n",
       " 'refer': 463,\n",
       " 'reference': 464,\n",
       " 'references': 67,\n",
       " 'refrigerated': 384,\n",
       " 'region': 68,\n",
       " 'request': 247,\n",
       " 'restaurants': 99,\n",
       " 'return': 316,\n",
       " 'returns': 517,\n",
       " 'rice': 32,\n",
       " 'roles': 431,\n",
       " 'rolled': 344,\n",
       " 'ruled': 100,\n",
       " 'rules': 317,\n",
       " 'run': 518,\n",
       " 'running': 519,\n",
       " 'runs': 432,\n",
       " 'safeguarded': 248,\n",
       " 'salt': 385,\n",
       " 'sambar': 33,\n",
       " 'sangam': 69,\n",
       " 'sanna': 155,\n",
       " 'sanskrit': 101,\n",
       " 'sauce': 226,\n",
       " 'savoury': 136,\n",
       " 'scale': 479,\n",
       " 'score': 433,\n",
       " 'scored': 192,\n",
       " 'scores': 434,\n",
       " 'secca': 465,\n",
       " 'second': 520,\n",
       " 'see': 345,\n",
       " 'selection': 227,\n",
       " 'series': 521,\n",
       " 'serve': 318,\n",
       " 'served': 34,\n",
       " 'serving': 346,\n",
       " 'set': 435,\n",
       " 'several': 270,\n",
       " 'shapes': 347,\n",
       " 'sheets': 466,\n",
       " 'shells': 386,\n",
       " 'short': 387,\n",
       " 'shuttlecock': 193,\n",
       " 'sicily': 467,\n",
       " 'side': 194,\n",
       " 'similar': 271,\n",
       " 'simple': 480,\n",
       " 'since': 228,\n",
       " 'single': 348,\n",
       " 'singles': 195,\n",
       " 'sited': 436,\n",
       " 'small': 319,\n",
       " 'so': 137,\n",
       " 'softer': 102,\n",
       " 'sold': 272,\n",
       " 'someshvara': 103,\n",
       " 'sometimes': 388,\n",
       " 'soup': 389,\n",
       " 'south': 35,\n",
       " 'speciality': 249,\n",
       " 'specialty': 499,\n",
       " 'specific': 500,\n",
       " 'specified': 390,\n",
       " 'spinning': 320,\n",
       " 'sport': 196,\n",
       " 'sri': 138,\n",
       " 'st': 70,\n",
       " 'staple': 349,\n",
       " 'starches': 139,\n",
       " 'steaming': 140,\n",
       " 'storage': 391,\n",
       " 'stored': 392,\n",
       " 'stretched': 350,\n",
       " 'striking': 197,\n",
       " 'strings': 393,\n",
       " 'strips': 394,\n",
       " 'stromboli': 273,\n",
       " 'stuffed': 501,\n",
       " 'stumps': 437,\n",
       " 'subcontinent': 36,\n",
       " 'such': 274,\n",
       " 'supermarkets': 481,\n",
       " 'swap': 438,\n",
       " 'swung': 522,\n",
       " 'table': 321,\n",
       " 'take': 507,\n",
       " 'takes': 322,\n",
       " 'tamil': 71,\n",
       " 'team': 439,\n",
       " 'teams': 198,\n",
       " 'ten': 440,\n",
       " 'tennis': 323,\n",
       " 'term': 229,\n",
       " 'th': 104,\n",
       " 'thankappan': 72,\n",
       " 'that': 73,\n",
       " 'the': 37,\n",
       " 'their': 324,\n",
       " 'them': 275,\n",
       " 'then': 441,\n",
       " 'they': 141,\n",
       " 'thicker': 105,\n",
       " 'thin': 395,\n",
       " 'thinner': 106,\n",
       " 'third': 523,\n",
       " 'three': 442,\n",
       " 'throughout': 142,\n",
       " 'thrown': 524,\n",
       " 'thus': 351,\n",
       " 'time': 325,\n",
       " 'to': 74,\n",
       " 'today': 482,\n",
       " 'tomato': 230,\n",
       " 'topped': 231,\n",
       " 'toward': 326,\n",
       " 'town': 75,\n",
       " 'tradition': 107,\n",
       " 'traditional': 143,\n",
       " 'traditionally': 38,\n",
       " 'trajectory': 327,\n",
       " 'true': 250,\n",
       " 'tubes': 396,\n",
       " 'turns': 508,\n",
       " 'two': 199,\n",
       " 'types': 276,\n",
       " 'typically': 468,\n",
       " 'udupi': 76,\n",
       " 'union': 251,\n",
       " 'unleavened': 352,\n",
       " 'upon': 252,\n",
       " 'urad': 39,\n",
       " 'use': 77,\n",
       " 'used': 277,\n",
       " 'using': 200,\n",
       " 'usually': 144,\n",
       " 'variably': 502,\n",
       " 'variant': 156,\n",
       " 'variants': 232,\n",
       " 'varieties': 278,\n",
       " 'variety': 353,\n",
       " 'various': 279,\n",
       " 'vary': 503,\n",
       " 'vegetables': 233,\n",
       " 'verace': 253,\n",
       " 'version': 108,\n",
       " 'very': 157,\n",
       " 'via': 483,\n",
       " 'was': 78,\n",
       " 'water': 397,\n",
       " 'waves': 398,\n",
       " 'well': 40,\n",
       " 'wheat': 469,\n",
       " 'when': 328,\n",
       " 'which': 109,\n",
       " 'while': 399,\n",
       " 'whilst': 443,\n",
       " 'who': 110,\n",
       " 'whole': 280,\n",
       " 'wicket': 444,\n",
       " 'widely': 484,\n",
       " 'winning': 445,\n",
       " 'with': 41,\n",
       " 'within': 201,\n",
       " 'wooden': 446,\n",
       " 'word': 354,\n",
       " 'world': 234,\n",
       " 'yard': 202,\n",
       " 'years': 400}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create gensim dictionary form a multiple files\n",
    "class ReadTxtFiles(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname), encoding='latin'):\n",
    "                yield simple_preprocess(line)\n",
    "\n",
    "path_to_text_directory = \"lsa_sports_food_docs\"\n",
    "\n",
    "dictionary = corpora.Dictionary(ReadTxtFiles(path_to_text_directory))\n",
    "\n",
    "# Token to Id map\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a bag of words corpus in gensim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus]\n",
    "pprint(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a bag of words corpus from a text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.filepath = path\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        global mydict  # OPTIONAL, only if updating the source dictionary.\n",
    "        for line in smart_open(self.filepath, encoding='latin'):\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "\n",
    "            # update the source dictionary (OPTIONAL)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow\n",
    "\n",
    "\n",
    "# Create the Dictionary\n",
    "mydict = corpora.Dictionary()\n",
    "\n",
    "# Create the Corpus\n",
    "bow_corpus = BoWCorpus('sample.txt', dictionary=mydict)  # memory friendly\n",
    "\n",
    "# Print the token_id and count for each line.\n",
    "for line in bow_corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to save a gensim dictionary and corpus to disk and load them back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Dict and Corpus\n",
    "mydict.save('mydict.dict')  # save dict to disk\n",
    "corpora.MmCorpus.serialize('bow_corpus.mm', bow_corpus)  # save corpus to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load them back\n",
    "loaded_dict = corpora.Dictionary.load('mydict.dict')\n",
    "\n",
    "corpus = corpora.MmCorpus('bow_corpus.mm')\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create the TFIDF matrix (corpus) in gensim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "documents = [\"This is the first line\",\n",
    "             \"This is the second sentence\",\n",
    "             \"This third document\"]\n",
    "\n",
    "# Create the Dictionary and Corpus\n",
    "mydict = corpora.Dictionary([simple_preprocess(line) for line in documents])\n",
    "corpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]\n",
    "\n",
    "# Show the Word Weights in Corpus\n",
    "for doc in corpus:\n",
    "    print([[mydict[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TF-IDF model\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
    "\n",
    "# Show the TF-IDF weights\n",
    "for doc in tfidf[corpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use gensim downloader API to load datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Get information about the model or dataset\n",
    "api.info('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "w2v_model.most_similar('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create bigrams and trigrams using Phraser models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = api.load(\"text8\")\n",
    "dataset = [wd for wd in dataset]\n",
    "\n",
    "dct = corpora.Dictionary(dataset)\n",
    "corpus = [dct.doc2bow(line) for line in dataset]\n",
    "\n",
    "# Build the bigram models\n",
    "bigram = gensim.models.phrases.Phrases(dataset, min_count=3, threshold=10)\n",
    "\n",
    "# Construct bigram\n",
    "print(bigram[dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the trigram models\n",
    "trigram = gensim.models.phrases.Phrases(bigram[dataset], threshold=10)\n",
    "\n",
    "# Construct trigram\n",
    "print(trigram[bigram[dataset[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create Topic Models with LDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Import packages and stopwords\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "import re\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['com', 'edu', 'subject', 'lines', 'organization', 'would', 'article', 'could']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<text8.Dataset object at 0x7f7b817592e8>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the dataset and get the text and real topic of each news article\n",
    "dataset = api.load(\"text8\")\n",
    "print(dataset)\n",
    "#data = [d for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5795983c7586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 3: Create the Inputs of LDA model: Dictionary and Corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_processed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_processed' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3: Create the Inputs of LDA model: Dictionary and Corpus\n",
    "dct = corpora.Dictionary(data_processed)\n",
    "corpus = [dct.doc2bow(line) for line in data_processed]\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the LDA model in Multicore\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dct,\n",
    "                         random_state=100,\n",
    "                         num_topics=7,\n",
    "                         passes=10,\n",
    "                         chunksize=1000,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "lda_model.save('lda_model.model')\n",
    "\n",
    "# See the topics\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4': Train the LDA model in Monocore, the results are not the same as multicore\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dct,\n",
    "                         random_state=100,\n",
    "                         num_topics=7,\n",
    "                         passes=10,\n",
    "                         chunksize=1000,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "lda_model.save('lda_model.model')\n",
    "\n",
    "# See the topics\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret the LDA Topic Model’s output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb\n",
    "for c in lda_model[corpus[5:8]]:\n",
    "    print(\"Document Topics      : \", c[0])      # [(Topics, Perc Contrib)]\n",
    "    print(\"Word id, Topics      : \", c[1][:3])  # [(Word id, [Topics])]\n",
    "    print(\"Phi Values (word id) : \", c[2][:2])  # [(Word id, [(Topic, Phi Value)])]\n",
    "    print(\"Word, Topics         : \", [(dct[wd], topic) for wd, topic in c[1][:2]])   # [(Word, [Topics])]\n",
    "    print(\"Phi Values (word)    : \", [(dct[wd], topic) for wd, topic in c[2][:2]])  # [(Word, [(Topic, Phi Value)])]\n",
    "    print(\"------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a LSI topic model using gensim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "# Build the LSI Model\n",
    "lsi_model = LsiModel(corpus=corpus, id2word=dct, num_topics=7, decay=0.5)\n",
    "\n",
    "# View Topics\n",
    "pprint(lsi_model.print_topics(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train Word2Vec model using gensim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download dataset\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# Split the data into 2 parts. Part 2 will be used later to update the model\n",
    "data_part1 = data[:1000]\n",
    "data_part2 = data[1000:]\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "model = Word2Vec(data_part1, min_count = 0, workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word vector for given word\n",
    "model['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Model\n",
    "model.save('newmodel')\n",
    "model = Word2Vec.load('newmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to update an existing Word2Vec model with new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model with new data.\n",
    "model.build_vocab(data_part2, update=True)\n",
    "model.train(data_part2, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to extract word vectors using pre-trained Word2Vec and FastText models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:27:07,339 : INFO : loading projection weights from /home/cuda_user/gensim-data/fasttext-wiki-news-subwords-300/fasttext-wiki-news-subwords-300.gz\n",
      "2019-03-19 19:31:19,555 : INFO : loaded (999999, 300) matrix from /home/cuda_user/gensim-data/fasttext-wiki-news-subwords-300/fasttext-wiki-news-subwords-300.gz\n"
     ]
    }
   ],
   "source": [
    "# Download the models\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model300 = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model300 = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:31:25,547 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('suppport', 0.8470075726509094),\n",
       " ('supporting', 0.837419331073761),\n",
       " ('supports', 0.8059129118919373),\n",
       " ('suport', 0.7670574188232422),\n",
       " ('supportin', 0.7664282321929932),\n",
       " ('supported', 0.7623896598815918),\n",
       " ('non-support', 0.7357579469680786),\n",
       " ('suppor', 0.732284665107727),\n",
       " ('suppports', 0.7290899157524109),\n",
       " ('suppporting', 0.7196788191795349)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try fasttext_model300 word embeddings\n",
    "fasttext_model300.most_similar('support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:31:28,170 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('supporting', 0.6251285076141357),\n",
       " ('suport', 0.6071150302886963),\n",
       " ('suppport', 0.6053199768066406),\n",
       " ('Support', 0.6044273376464844),\n",
       " ('supported', 0.6009396910667419),\n",
       " ('backing', 0.6007589101791382),\n",
       " ('supports', 0.5269277095794678),\n",
       " ('assistance', 0.5207138061523438),\n",
       " ('sup_port', 0.5192490220069885),\n",
       " ('supportive', 0.5110025405883789)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try word2vec_model300 word embeddings\n",
    "word2vec_model300.most_similar('support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:31:37,004 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('supported', 0.740031898021698),\n",
       " ('supporting', 0.6803102493286133),\n",
       " ('backing', 0.6659233570098877),\n",
       " ('supports', 0.6377385258674622),\n",
       " ('provide', 0.6045100092887878),\n",
       " ('assistance', 0.587337076663971),\n",
       " ('efforts', 0.5793647766113281),\n",
       " ('providing', 0.561307430267334),\n",
       " ('strong', 0.5610021352767944),\n",
       " ('help', 0.5547006130218506)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try glove_model300 word embeddings\n",
    "glove_model300.most_similar('support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:32:01,340 : INFO : Evaluating word analogies for top 300000 words in the model on questions-words.txt\n",
      "2019-03-19 19:32:08,392 : INFO : capital-common-countries: 83.2% (421/506)\n",
      "2019-03-19 19:33:00,714 : INFO : capital-world: 81.3% (3552/4368)\n",
      "2019-03-19 19:33:10,395 : INFO : currency: 28.5% (230/808)\n",
      "2019-03-19 19:33:41,292 : INFO : city-in-state: 72.1% (1779/2467)\n",
      "2019-03-19 19:33:47,886 : INFO : family: 86.2% (436/506)\n",
      "2019-03-19 19:34:00,192 : INFO : gram1-adjective-to-adverb: 29.2% (290/992)\n",
      "2019-03-19 19:34:10,390 : INFO : gram2-opposite: 43.5% (353/812)\n",
      "2019-03-19 19:34:26,561 : INFO : gram3-comparative: 91.3% (1216/1332)\n",
      "2019-03-19 19:34:40,330 : INFO : gram4-superlative: 88.0% (987/1122)\n",
      "2019-03-19 19:34:53,059 : INFO : gram5-present-participle: 78.5% (829/1056)\n",
      "2019-03-19 19:35:12,952 : INFO : gram6-nationality-adjective: 90.2% (1442/1599)\n",
      "2019-03-19 19:35:31,531 : INFO : gram7-past-tense: 65.4% (1020/1560)\n",
      "2019-03-19 19:35:47,349 : INFO : gram8-plural: 87.0% (1159/1332)\n",
      "2019-03-19 19:35:57,521 : INFO : gram9-plural-verbs: 68.2% (593/870)\n",
      "2019-03-19 19:35:57,526 : INFO : Quadruplets with out-of-vocabulary words: 1.1%\n",
      "2019-03-19 19:35:57,529 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2019-03-19 19:35:57,530 : INFO : Total accuracy: 74.0% (14307/19330)\n",
      "2019-03-19 19:35:57,910 : INFO : Evaluating word analogies for top 300000 words in the model on questions-words.txt\n",
      "2019-03-19 19:36:04,655 : INFO : capital-common-countries: 98.6% (499/506)\n",
      "2019-03-19 19:37:02,816 : INFO : capital-world: 97.5% (4409/4524)\n",
      "2019-03-19 19:37:12,771 : INFO : currency: 33.3% (269/808)\n",
      "2019-03-19 19:37:41,123 : INFO : city-in-state: 83.4% (2057/2467)\n",
      "2019-03-19 19:37:47,109 : INFO : family: 85.8% (434/506)\n",
      "2019-03-19 19:37:59,808 : INFO : gram1-adjective-to-adverb: 70.4% (698/992)\n",
      "2019-03-19 19:38:09,737 : INFO : gram2-opposite: 63.3% (514/812)\n",
      "2019-03-19 19:38:26,716 : INFO : gram3-comparative: 97.0% (1292/1332)\n",
      "2019-03-19 19:38:40,993 : INFO : gram4-superlative: 99.2% (1113/1122)\n",
      "2019-03-19 19:38:53,883 : INFO : gram5-present-participle: 97.7% (1032/1056)\n",
      "2019-03-19 19:39:13,552 : INFO : gram6-nationality-adjective: 92.9% (1486/1599)\n",
      "2019-03-19 19:39:32,680 : INFO : gram7-past-tense: 84.6% (1320/1560)\n",
      "2019-03-19 19:39:49,136 : INFO : gram8-plural: 93.8% (1249/1332)\n",
      "2019-03-19 19:40:00,200 : INFO : gram9-plural-verbs: 95.4% (830/870)\n",
      "2019-03-19 19:40:00,205 : INFO : Quadruplets with out-of-vocabulary words: 0.3%\n",
      "2019-03-19 19:40:00,206 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2019-03-19 19:40:00,208 : INFO : Total accuracy: 88.3% (17202/19486)\n",
      "2019-03-19 19:40:00,588 : INFO : Evaluating word analogies for top 300000 words in the model on questions-words.txt\n",
      "2019-03-19 19:40:08,013 : INFO : capital-common-countries: 94.9% (480/506)\n",
      "2019-03-19 19:41:00,731 : INFO : capital-world: 96.0% (4342/4524)\n",
      "2019-03-19 19:41:11,585 : INFO : currency: 17.1% (138/808)\n",
      "2019-03-19 19:41:39,745 : INFO : city-in-state: 59.3% (1463/2467)\n",
      "2019-03-19 19:41:45,153 : INFO : family: 88.1% (446/506)\n",
      "2019-03-19 19:41:57,349 : INFO : gram1-adjective-to-adverb: 22.6% (224/992)\n",
      "2019-03-19 19:42:07,460 : INFO : gram2-opposite: 27.3% (222/812)\n",
      "2019-03-19 19:42:22,958 : INFO : gram3-comparative: 88.1% (1174/1332)\n",
      "2019-03-19 19:42:35,848 : INFO : gram4-superlative: 72.2% (810/1122)\n",
      "2019-03-19 19:42:48,878 : INFO : gram5-present-participle: 70.0% (739/1056)\n",
      "2019-03-19 19:43:06,339 : INFO : gram6-nationality-adjective: 92.6% (1480/1599)\n",
      "2019-03-19 19:43:25,070 : INFO : gram7-past-tense: 61.2% (954/1560)\n",
      "2019-03-19 19:43:40,961 : INFO : gram8-plural: 78.1% (1040/1332)\n",
      "2019-03-19 19:43:51,032 : INFO : gram9-plural-verbs: 58.5% (509/870)\n",
      "2019-03-19 19:43:51,037 : INFO : Quadruplets with out-of-vocabulary words: 0.3%\n",
      "2019-03-19 19:43:51,038 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2019-03-19 19:43:51,039 : INFO : Total accuracy: 72.0% (14021/19486)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7195422354510931"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate which one performs better\n",
    "\n",
    "# Word2ec_accuracy\n",
    "word2vec_model300.evaluate_word_analogies(analogies=\"questions-words.txt\")[0]\n",
    "\n",
    "# fasttext_accuracy\n",
    "fasttext_model300.evaluate_word_analogies(analogies=\"questions-words.txt\")[0]\n",
    "\n",
    "# GloVe accuracy\n",
    "glove_model300.evaluate_word_analogies(analogies=\"questions-words.txt\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create document vectors using Doc2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download dataset\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing', 'interpretations', 'of', 'what', 'this', 'means', 'anarchism', 'also', 'refers', 'to', 'related', 'social', 'movements', 'that', 'advocate', 'the', 'elimination', 'of', 'authoritarian', 'institutions', 'particularly', 'the', 'state', 'the', 'word', 'anarchy', 'as', 'most', 'anarchists', 'use', 'it', 'does', 'not', 'imply', 'chaos', 'nihilism', 'or', 'anomie', 'but', 'rather', 'a', 'harmonious', 'anti', 'authoritarian', 'society', 'in', 'place', 'of', 'what', 'are', 'regarded', 'as', 'authoritarian', 'political', 'structures', 'and', 'coercive', 'economic', 'institutions', 'anarchists', 'advocate', 'social', 'relations', 'based', 'upon', 'voluntary', 'association', 'of', 'autonomous', 'individuals', 'mutual', 'aid', 'and', 'self', 'governance', 'while', 'anarchism', 'is', 'most', 'easily', 'defined', 'by', 'what', 'it', 'is', 'against', 'anarchists', 'also', 'offer', 'positive', 'visions', 'of', 'what', 'they', 'believe', 'to', 'be', 'a', 'truly', 'free', 'society', 'however', 'ideas', 'about', 'how', 'an', 'anarchist', 'society', 'might', 'work', 'vary', 'considerably', 'especially', 'with', 'respect', 'to', 'economics', 'there', 'is', 'also', 'disagreement', 'about', 'how', 'a', 'free', 'society', 'might', 'be', 'brought', 'about', 'origins', 'and', 'predecessors', 'kropotkin', 'and', 'others', 'argue', 'that', 'before', 'recorded', 'history', 'human', 'society', 'was', 'organized', 'on', 'anarchist', 'principles', 'most', 'anthropologists', 'follow', 'kropotkin', 'and', 'engels', 'in', 'believing', 'that', 'hunter', 'gatherer', 'bands', 'were', 'egalitarian', 'and', 'lacked', 'division', 'of', 'labour', 'accumulated', 'wealth', 'or', 'decreed', 'law', 'and', 'had', 'equal', 'access', 'to', 'resources', 'william', 'godwin', 'anarchists', 'including', 'the', 'the', 'anarchy', 'organisation', 'and', 'rothbard', 'find', 'anarchist', 'attitudes', 'in', 'taoism', 'from', 'ancient', 'china', 'kropotkin', 'found', 'similar', 'ideas', 'in', 'stoic', 'zeno', 'of', 'citium', 'according', 'to', 'kropotkin', 'zeno', 'repudiated', 'the', 'omnipotence', 'of', 'the', 'state', 'its', 'intervention', 'and', 'regimentation', 'and', 'proclaimed', 'the', 'sovereignty', 'of', 'the', 'moral', 'law', 'of', 'the', 'individual', 'the', 'anabaptists', 'of', 'one', 'six', 'th', 'century', 'europe', 'are', 'sometimes', 'considered', 'to', 'be', 'religious', 'forerunners', 'of', 'modern', 'anarchism', 'bertrand', 'russell', 'in', 'his', 'history', 'of', 'western', 'philosophy', 'writes', 'that', 'the', 'anabaptists', 'repudiated', 'all', 'law', 'since', 'they', 'held', 'that', 'the', 'good', 'man', 'will', 'be', 'guided', 'at', 'every', 'moment', 'by', 'the', 'holy', 'spirit', 'from', 'this', 'premise', 'they', 'arrive', 'at', 'communism', 'the', 'diggers', 'or', 'true', 'levellers', 'were', 'an', 'early', 'communistic', 'movement', 'during', 'the', 'time', 'of', 'the', 'english', 'civil', 'war', 'and', 'are', 'considered', 'by', 'some', 'as', 'forerunners', 'of', 'modern', 'anarchism', 'in', 'the', 'modern', 'era', 'the', 'first', 'to', 'use', 'the', 'term', 'to', 'mean', 'something', 'other', 'than', 'chaos', 'was', 'louis', 'armand', 'baron', 'de', 'lahontan', 'in', 'his', 'nouveaux', 'voyages', 'dans', 'l', 'am', 'rique', 'septentrionale', 'one', 'seven', 'zero', 'three', 'where', 'he', 'described', 'the', 'indigenous', 'american', 'society', 'which', 'had', 'no', 'state', 'laws', 'prisons', 'priests', 'or', 'private', 'property', 'as', 'being', 'in', 'anarchy', 'russell', 'means', 'a', 'libertarian', 'and', 'leader', 'in', 'the', 'american', 'indian', 'movement', 'has', 'repeatedly', 'stated', 'that', 'he', 'is', 'an', 'anarchist', 'and', 'so', 'are', 'all', 'his', 'ancestors', 'in', 'one', 'seven', 'nine', 'three', 'in', 'the', 'thick', 'of', 'the', 'french', 'revolution', 'william', 'godwin', 'published', 'an', 'enquiry', 'concerning', 'political', 'justice', 'although', 'godwin', 'did', 'not', 'use', 'the', 'word', 'anarchism', 'many', 'later', 'anarchists', 'have', 'regarded', 'this', 'book', 'as', 'the', 'first', 'major', 'anarchist', 'text', 'and', 'godwin', 'as', 'the', 'founder', 'of', 'philosophical', 'anarchism', 'but', 'at', 'this', 'point', 'no', 'anarchist', 'movement', 'yet', 'existed', 'and', 'the', 'term', 'anarchiste', 'was', 'known', 'mainly', 'as', 'an', 'insult', 'hurled', 'by', 'the', 'bourgeois', 'girondins', 'at', 'more', 'radical', 'elements', 'in', 'the', 'french', 'revolution', 'the', 'first', 'self', 'labelled', 'anarchist', 'pierre', 'joseph', 'proudhon', 'it', 'is', 'commonly', 'held', 'that', 'it', 'wasn', 't', 'until', 'pierre', 'joseph', 'proudhon', 'published', 'what', 'is', 'property', 'in', 'one', 'eight', 'four', 'zero', 'that', 'the', 'term', 'anarchist', 'was', 'adopted', 'as', 'a', 'self', 'description', 'it', 'is', 'for', 'this', 'reason', 'that', 'some', 'claim', 'proudhon', 'as', 'the', 'founder', 'of', 'modern', 'anarchist', 'theory', 'in', 'what', 'is', 'property', 'proudhon', 'answers', 'with', 'the', 'famous', 'accusation', 'property', 'is', 'theft', 'in', 'this', 'work', 'he', 'opposed', 'the', 'institution', 'of', 'decreed', 'property', 'propri', 't', 'where', 'owners', 'have', 'complete', 'rights', 'to', 'use', 'and', 'abuse', 'their', 'property', 'as', 'they', 'wish', 'such', 'as', 'exploiting', 'workers', 'for', 'profit', 'in', 'its', 'place', 'proudhon', 'supported', 'what', 'he', 'called', 'possession', 'individuals', 'can', 'have', 'limited', 'rights', 'to', 'use', 'resources', 'capital', 'and', 'goods', 'in', 'accordance', 'with', 'principles', 'of', 'equality', 'and', 'justice', 'proudhon', 's', 'vision', 'of', 'anarchy', 'which', 'he', 'called', 'mutualism', 'mutuellisme', 'involved', 'an', 'exchange', 'economy', 'where', 'individuals', 'and', 'groups', 'could', 'trade', 'the', 'products', 'of', 'their', 'labor', 'using', 'labor', 'notes', 'which', 'represented', 'the', 'amount', 'of', 'working', 'time', 'involved', 'in', 'production', 'this', 'would', 'ensure', 'that', 'no', 'one', 'would', 'profit', 'from', 'the', 'labor', 'of', 'others', 'workers', 'could', 'freely', 'join', 'together', 'in', 'co', 'operative', 'workshops', 'an', 'interest', 'free', 'bank', 'would', 'be', 'set', 'up', 'to', 'provide', 'everyone', 'with', 'access', 'to', 'the', 'means', 'of', 'production', 'proudhon', 's', 'ideas', 'were', 'influential', 'within', 'french', 'working', 'class', 'movements', 'and', 'his', 'followers', 'were', 'active', 'in', 'the', 'revolution', 'of', 'one', 'eight', 'four', 'eight', 'in', 'france', 'proudhon', 's', 'philosophy', 'of', 'property', 'is', 'complex', 'it', 'was', 'developed', 'in', 'a', 'number', 'of', 'works', 'over', 'his', 'lifetime', 'and', 'there', 'are', 'differing', 'interpretations', 'of', 'some', 'of', 'his', 'ideas', 'for', 'more', 'detailed', 'discussion', 'see', 'here', 'max', 'stirner', 's', 'egoism', 'in', 'his', 'the', 'ego', 'and', 'its', 'own', 'stirner', 'argued', 'that', 'most', 'commonly', 'accepted', 'social', 'institutions', 'including', 'the', 'notion', 'of', 'state', 'property', 'as', 'a', 'right', 'natural', 'rights', 'in', 'general', 'and', 'the', 'very', 'notion', 'of', 'society', 'were', 'mere', 'illusions', 'or', 'ghosts', 'in', 'the', 'mind', 'saying', 'of', 'society', 'that', 'the', 'individuals', 'are', 'its', 'reality', 'he', 'advocated', 'egoism', 'and', 'a', 'form', 'of', 'amoralism', 'in', 'which', 'individuals', 'would', 'unite', 'in', 'associations', 'of', 'egoists', 'only', 'when', 'it', 'was', 'in', 'their', 'self', 'interest', 'to', 'do', 'so', 'for', 'him', 'property', 'simply', 'comes', 'about', 'through', 'might', 'whoever', 'knows', 'how', 'to', 'take', 'to', 'defend', 'the', 'thing', 'to', 'him', 'belongs', 'property', 'and', 'what', 'i', 'have', 'in', 'my', 'power', 'that', 'is', 'my', 'own', 'so', 'long', 'as', 'i', 'assert', 'myself', 'as', 'holder', 'i', 'am', 'the', 'proprietor', 'of', 'the', 'thing', 'stirner', 'never', 'called', 'himself', 'an', 'anarchist', 'he', 'accepted', 'only', 'the', 'label', 'egoist', 'nevertheless', 'his', 'ideas', 'were', 'influential', 'on', 'many', 'individualistically', 'inclined', 'anarchists', 'although', 'interpretations', 'of', 'his', 'thought', 'are', 'diverse', 'american', 'individualist', 'anarchism', 'benjamin', 'tucker', 'in', 'one', 'eight', 'two', 'five', 'josiah', 'warren', 'had', 'participated', 'in', 'a', 'communitarian', 'experiment', 'headed', 'by', 'robert', 'owen', 'called', 'new', 'harmony', 'which', 'failed', 'in', 'a', 'few', 'years', 'amidst', 'much', 'internal', 'conflict', 'warren', 'blamed', 'the', 'community', 's', 'failure', 'on', 'a', 'lack', 'of', 'individual', 'sovereignty', 'and', 'a', 'lack', 'of', 'private', 'property', 'warren', 'proceeded', 'to', 'organise', 'experimenal', 'anarchist', 'communities', 'which', 'respected', 'what', 'he', 'called', 'the', 'sovereignty', 'of', 'the', 'individual', 'at', 'utopia', 'and', 'modern', 'times', 'in', 'one', 'eight', 'three', 'three', 'warren', 'wrote', 'and', 'published', 'the', 'peaceful', 'revolutionist', 'which', 'some', 'have', 'noted', 'to', 'be', 'the', 'first', 'anarchist', 'periodical', 'ever', 'published', 'benjamin', 'tucker', 'says', 'that', 'warren', 'was', 'the', 'first', 'man', 'to', 'expound', 'and', 'formulate', 'the', 'doctrine', 'now', 'known', 'as', 'anarchism', 'liberty', 'xiv', 'december', 'one', 'nine', 'zero', 'zero', 'one', 'benjamin', 'tucker', 'became', 'interested', 'in', 'anarchism', 'through', 'meeting', 'josiah', 'warren', 'and', 'william', 'b', 'greene', 'he', 'edited', 'and', 'published', 'liberty', 'from', 'august', 'one', 'eight', 'eight', 'one', 'to', 'april', 'one', 'nine', 'zero', 'eight', 'it', 'is', 'widely', 'considered', 'to', 'be', 'the', 'finest', 'individualist', 'anarchist', 'periodical', 'ever', 'issued', 'in', 'the', 'english', 'language', 'tucker', 's', 'conception', 'of', 'individualist', 'anarchism', 'incorporated', 'the', 'ideas', 'of', 'a', 'variety', 'of', 'theorists', 'greene', 's', 'ideas', 'on', 'mutual', 'banking', 'warren', 's', 'ideas', 'on', 'cost', 'as', 'the', 'limit', 'of', 'price', 'a', 'heterodox', 'variety', 'of', 'labour', 'theory', 'of', 'value', 'proudhon', 's', 'market', 'anarchism', 'max', 'stirner', 's', 'egoism', 'and', 'herbert', 'spencer', 's', 'law', 'of', 'equal', 'freedom', 'tucker', 'strongly', 'supported', 'the', 'individual', 's', 'right', 'to', 'own', 'the', 'product', 'of', 'his', 'or', 'her', 'labour', 'as', 'private', 'property', 'and', 'believed', 'in', 'a', 'market', 'economy', 'for', 'trading', 'this', 'property', 'he', 'argued', 'that', 'in', 'a', 'truly', 'free', 'market', 'system', 'without', 'the', 'state', 'the', 'abundance', 'of', 'competition', 'would', 'eliminate', 'profits', 'and', 'ensure', 'that', 'all', 'workers', 'received', 'the', 'full', 'value', 'of', 'their', 'labor', 'other', 'one', 'nine', 'th', 'century', 'individualists', 'included', 'lysander', 'spooner', 'stephen', 'pearl', 'andrews', 'and', 'victor', 'yarros', 'the', 'first', 'international', 'mikhail', 'bakunin', 'one', 'eight', 'one', 'four', 'one', 'eight', 'seven', 'six', 'in', 'europe', 'harsh', 'reaction', 'followed', 'the', 'revolutions', 'of', 'one', 'eight', 'four', 'eight', 'twenty', 'years', 'later', 'in', 'one', 'eight', 'six', 'four', 'the', 'international', 'workingmen', 's', 'association', 'sometimes', 'called', 'the', 'first', 'international', 'united', 'some', 'diverse', 'european', 'revolutionary', 'currents', 'including', 'anarchism', 'due', 'to', 'its', 'genuine', 'links', 'to', 'active', 'workers', 'movements', 'the', 'international', 'became', 'signficiant', 'from', 'the', 'start', 'karl', 'marx', 'was', 'a', 'leading', 'figure', 'in', 'the', 'international', 'he', 'was', 'elected', 'to', 'every', 'succeeding', 'general', 'council', 'of', 'the', 'association', 'the', 'first', 'objections', 'to', 'marx', 'came', 'from', 'the', 'mutualists', 'who', 'opposed', 'communism', 'and', 'statism', 'shortly', 'after', 'mikhail', 'bakunin', 'and', 'his', 'followers', 'joined', 'in', 'one', 'eight', 'six', 'eight', 'the', 'first', 'international', 'became', 'polarised', 'into', 'two', 'camps', 'with', 'marx', 'and', 'bakunin', 'as', 'their', 'respective', 'figureheads', 'the', 'clearest', 'difference', 'between', 'the', 'camps', 'was', 'over', 'strategy', 'the', 'anarchists', 'around', 'bakunin', 'favoured', 'in', 'kropotkin', 's', 'words', 'direct', 'economical', 'struggle', 'against', 'capitalism', 'without', 'interfering', 'in', 'the', 'political', 'parliamentary', 'agitation', 'at', 'that', 'time', 'marx', 'and', 'his', 'followers', 'focused', 'on', 'parliamentary', 'activity', 'bakunin', 'characterised', 'marx', 's', 'ideas', 'as', 'authoritarian', 'and', 'predicted', 'that', 'if', 'a', 'marxist', 'party', 'gained', 'to', 'power', 'its', 'leaders', 'would', 'end', 'up', 'as', 'bad', 'as', 'the', 'ruling', 'class', 'they', 'had', 'fought', 'against', 'in', 'one', 'eight', 'seven', 'two', 'the', 'conflict', 'climaxed', 'with', 'a', 'final', 'split', 'between', 'the', 'two', 'groups', 'at', 'the', 'hague', 'congress', 'this', 'is', 'often', 'cited', 'as', 'the', 'origin', 'of', 'the', 'conflict', 'between', 'anarchists', 'and', 'marxists', 'from', 'this', 'moment', 'the', 'social', 'democratic', 'and', 'libertarian', 'currents', 'of', 'socialism', 'had', 'distinct', 'organisations', 'including', 'rival', 'internationals', 'anarchist', 'communism', 'peter', 'kropotkin', 'proudhon', 'and', 'bakunin', 'both', 'opposed', 'communism', 'associating', 'it', 'with', 'statism', 'however', 'in', 'the', 'one', 'eight', 'seven', 'zero', 's', 'many', 'anarchists', 'moved', 'away', 'from', 'bakunin', 's', 'economic', 'thinking', 'called', 'collectivism', 'and', 'embraced', 'communist', 'concepts', 'communists', 'believed', 'the', 'means', 'of', 'production', 'should', 'be', 'owned', 'collectively', 'and', 'that', 'goods', 'be', 'distributed', 'by', 'need', 'not', 'labor', 'an', 'early', 'anarchist', 'communist', 'was', 'joseph', 'd', 'jacque', 'the', 'first', 'person', 'to', 'describe', 'himself', 'as', 'libertarian', 'unlike', 'proudhon', 'he', 'argued', 'that', 'it', 'is', 'not', 'the', 'product', 'of', 'his', 'or', 'her', 'labor', 'that', 'the', 'worker', 'has', 'a', 'right', 'to', 'but', 'to', 'the', 'satisfaction', 'of', 'his', 'or', 'her', 'needs', 'whatever', 'may', 'be', 'their', 'nature', 'he', 'announced', 'his', 'ideas', 'in', 'his', 'us', 'published', 'journal', 'le', 'libertaire', 'one', 'eight', 'five', 'eight', 'one', 'eight', 'six', 'one', 'peter', 'kropotkin', 'often', 'seen', 'as', 'the', 'most', 'important', 'theorist', 'outlined', 'his', 'economic', 'ideas', 'in', 'the', 'conquest', 'of', 'bread', 'and', 'fields', 'factories', 'and', 'workshops', 'he', 'felt', 'co', 'operation', 'is', 'more', 'beneficial', 'than', 'competition', 'illustrated', 'in', 'nature', 'in', 'mutual', 'aid', 'a', 'factor', 'of', 'evolution', 'one', 'eight', 'nine', 'seven', 'subsequent', 'anarchist', 'communists', 'include', 'emma', 'goldman', 'and', 'alexander', 'berkman', 'many', 'in', 'the', 'anarcho', 'syndicalist', 'movements', 'see', 'below', 'saw', 'anarchist', 'communism', 'as', 'their', 'objective', 'isaac', 'puente', 's', 'one', 'nine', 'three', 'two', 'comunismo', 'libertario', 'was', 'adopted', 'by', 'the', 'spanish', 'cnt', 'as', 'its', 'manifesto', 'for', 'a', 'post', 'revolutionary', 'society', 'some', 'anarchists', 'disliked', 'merging', 'communism', 'with', 'anarchism', 'several', 'individualist', 'anarchists', 'maintained', 'that', 'abolition', 'of', 'private', 'property', 'was', 'not', 'consistent', 'with', 'liberty', 'for', 'example', 'benjamin', 'tucker', 'whilst', 'professing', 'respect', 'for', 'kropotkin', 'and', 'publishing', 'his', 'work', 'described', 'communist', 'anarchism', 'as', 'pseudo', 'anarchism', 'propaganda', 'of', 'the', 'deed', 'johann', 'most', 'was', 'an', 'outspoken', 'advocate', 'of', 'violence', 'anarchists', 'have', 'often', 'been', 'portrayed', 'as', 'dangerous', 'and', 'violent', 'due', 'mainly', 'to', 'a', 'number', 'of', 'high', 'profile', 'violent', 'acts', 'including', 'riots', 'assassinations', 'insurrections', 'and', 'terrorism', 'by', 'some', 'anarchists', 'some', 'revolutionaries', 'of', 'the', 'late', 'one', 'nine', 'th', 'century', 'encouraged', 'acts', 'of', 'political', 'violence', 'such', 'as', 'bombings', 'and', 'the', 'assassinations', 'of', 'heads', 'of', 'state', 'to', 'further', 'anarchism', 'such', 'actions', 'have', 'sometimes', 'been', 'called', 'propaganda', 'by', 'the', 'deed', 'one', 'of', 'the', 'more', 'outspoken', 'advocates', 'of', 'this', 'strategy', 'was', 'johann', 'most', 'who', 'said', 'the', 'existing', 'system', 'will', 'be', 'quickest', 'and', 'most', 'radically', 'overthrown', 'by', 'the', 'annihilation', 'of', 'its', 'exponents', 'therefore', 'massacres', 'of', 'the', 'enemies', 'of', 'the', 'people', 'must', 'be', 'set', 'in', 'motion', 'most', 's', 'preferred', 'method', 'of', 'terrorism', 'dynamite', 'earned', 'him', 'the', 'moniker', 'dynamost', 'however', 'there', 'is', 'no', 'consensus', 'on', 'the', 'legitimacy', 'or', 'utility', 'of', 'violence', 'in', 'general', 'mikhail', 'bakunin', 'and', 'errico', 'malatesta', 'for', 'example', 'wrote', 'of', 'violence', 'as', 'a', 'necessary', 'and', 'sometimes', 'desirable', 'force', 'in', 'revolutionary', 'settings', 'but', 'at', 'the', 'same', 'time', 'they', 'denounced', 'acts', 'of', 'individual', 'terrorism', 'malatesta', 'in', 'on', 'violence', 'and', 'bakunin', 'when', 'he', 'refuted', 'nechaev', 'other', 'anarchists', 'sometimes', 'identified', 'as', 'pacifist', 'anarchists', 'advocated', 'complete', 'nonviolence', 'leo', 'tolstoy', 'whose', 'philosophy', 'is', 'often', 'viewed', 'as', 'a', 'form', 'of', 'christian', 'anarchism', 'see', 'below', 'was', 'a', 'notable', 'exponent', 'of', 'nonviolent', 'resistance', 'anarchism', 'in', 'the', 'labour', 'movement', 'the', 'red', 'and', 'black', 'flag', 'coming', 'from', 'the', 'experience', 'of', 'anarchists', 'in', 'the', 'labour', 'movement', 'is', 'particularly', 'associated', 'with', 'anarcho', 'syndicalism', 'anarcho', 'syndicalism', 'was', 'an', 'early', 'two', 'zero', 'th', 'century', 'working', 'class', 'movement', 'seeking', 'to', 'overthrow', 'capitalism', 'and', 'the', 'state', 'to', 'institute', 'a', 'worker', 'controlled', 'society', 'the', 'movement', 'pursued', 'industrial', 'actions', 'such', 'as', 'general', 'strike', 'as', 'a', 'primary', 'strategy', 'many', 'anarcho', 'syndicalists', 'believed', 'in', 'anarchist', 'communism', 'though', 'not', 'all', 'communists', 'believed', 'in', 'syndicalism', 'after', 'the', 'one', 'eight', 'seven', 'one', 'repression', 'french', 'anarchism', 'reemerged', 'influencing', 'the', 'bourses', 'de', 'travails', 'of', 'autonomous', 'workers', 'groups', 'and', 'trade', 'unions', 'from', 'this', 'movement', 'the', 'conf', 'd', 'ration', 'g', 'n', 'rale', 'du', 'travail', 'general', 'confederation', 'of', 'work', 'cgt', 'was', 'formed', 'in', 'one', 'eight', 'nine', 'five', 'as', 'the', 'first', 'major', 'anarcho', 'syndicalist', 'movement', 'emile', 'pataud', 'and', 'emile', 'pouget', 's', 'writing', 'for', 'the', 'cgt', 'saw', 'libertarian', 'communism', 'developing', 'from', 'a', 'general', 'strike', 'after', 'one', 'nine', 'one', 'four', 'the', 'cgt', 'moved', 'away', 'from', 'anarcho', 'syndicalism', 'due', 'to', 'the', 'appeal', 'of', 'bolshevism', 'french', 'style', 'syndicalism', 'was', 'a', 'significant', 'movement', 'in', 'europe', 'prior', 'to', 'one', 'nine', 'two', 'one', 'and', 'remained', 'a', 'significant', 'movement', 'in', 'spain', 'until', 'the', 'mid', 'one', 'nine', 'four', 'zero', 's', 'the', 'industrial', 'workers', 'of', 'the', 'world', 'iww', 'founded', 'in', 'one', 'nine', 'zero', 'five', 'in', 'the', 'us', 'espoused', 'unionism', 'and', 'sought', 'a', 'general', 'strike', 'to', 'usher', 'in', 'a', 'stateless', 'society', 'in', 'one', 'nine', 'two', 'three', 'one', 'zero', 'zero', 'zero', 'zero', 'zero', 'members', 'existed', 'with', 'the', 'support', 'of', 'up', 'to', 'three', 'zero', 'zero', 'zero', 'zero', 'zero', 'though', 'not', 'explicitly', 'anarchist', 'they', 'organized', 'by', 'rank', 'and', 'file', 'democracy', 'embodying', 'a', 'spirit', 'of', 'resistance', 'that', 'has', 'inspired', 'many', 'anglophone', 'syndicalists', 'cnt', 'propaganda', 'from', 'april', 'two', 'zero', 'zero', 'four', 'reads', 'don', 't', 'let', 'the', 'politicians', 'rule', 'our', 'lives', 'you', 'vote', 'and', 'they', 'decide', 'don', 't', 'allow', 'it', 'unity', 'action', 'self', 'management', 'spanish', 'anarchist', 'trade', 'union', 'federations', 'were', 'formed', 'in', 'the', 'one', 'eight', 'seven', 'zero', 's', 'one', 'nine', 'zero', 'zero', 'and', 'one', 'nine', 'one', 'zero', 'the', 'most', 'successful', 'was', 'the', 'confederaci', 'n', 'nacional', 'del', 'trabajo', 'national', 'confederation', 'of', 'labour', 'cnt', 'founded', 'in', 'one', 'nine', 'one', 'zero', 'prior', 'to', 'the', 'one', 'nine', 'four', 'zero', 's', 'the', 'cnt', 'was', 'the', 'major', 'force', 'in', 'spanish', 'working', 'class', 'politics', 'with', 'a', 'membership', 'of', 'one', 'five', 'eight', 'million', 'in', 'one', 'nine', 'three', 'four', 'the', 'cnt', 'played', 'a', 'major', 'role', 'in', 'the', 'spanish', 'civil', 'war', 'see', 'also', 'anarchism', 'in', 'spain', 'syndicalists', 'like', 'ricardo', 'flores', 'mag', 'n', 'were', 'key', 'figures', 'in', 'the', 'mexican', 'revolution', 'latin', 'american', 'anarchism', 'was', 'strongly', 'influenced', 'extending', 'to', 'the', 'zapatista', 'rebellion', 'and', 'the', 'factory', 'occupation', 'movements', 'in', 'argentina', 'in', 'berlin', 'in', 'one', 'nine', 'two', 'two', 'the', 'cnt', 'was', 'joined', 'with', 'the', 'international', 'workers', 'association', 'an', 'anarcho', 'syndicalist', 'successor', 'to', 'the', 'first', 'international', 'contemporary', 'anarcho', 'syndicalism', 'continues', 'as', 'a', 'minor', 'force', 'in', 'many', 'socities', 'much', 'smaller', 'than', 'in', 'the', 'one', 'nine', 'one', 'zero', 's', 'two', 'zero', 's', 'and', 'three', 'zero', 's', 'the', 'largest', 'organised', 'anarchist', 'movement', 'today', 'is', 'in', 'spain', 'in', 'the', 'form', 'of', 'the', 'confederaci', 'n', 'general', 'del', 'trabajo', 'and', 'the', 'cnt', 'the', 'cgt', 'claims', 'a', 'paid', 'up', 'membership', 'of', 'six', 'zero', 'zero', 'zero', 'zero', 'and', 'received', 'over', 'a', 'million', 'votes', 'in', 'spanish', 'syndical', 'elections', 'other', 'active', 'syndicalist', 'movements', 'include', 'the', 'us', 'workers', 'solidarity', 'alliance', 'and', 'the', 'uk', 'solidarity', 'federation', 'the', 'revolutionary', 'industrial', 'unionist', 'industrial', 'workers', 'of', 'the', 'world', 'also', 'exists', 'claiming', 'two', 'zero', 'zero', 'zero', 'paid', 'members', 'contemporary', 'critics', 'of', 'anarcho', 'syndicalism', 'and', 'revolutionary', 'industrial', 'unionism', 'claim', 'that', 'they', 'are', 'workerist', 'and', 'fail', 'to', 'deal', 'with', 'economic', 'life', 'outside', 'work', 'post', 'leftist', 'critics', 'such', 'as', 'bob', 'black', 'claim', 'anarcho', 'syndicalism', 'advocates', 'oppressive', 'social', 'structures', 'such', 'as', 'work', 'and', 'the', 'workplace', 'anarcho', 'syndicalists', 'in', 'general', 'uphold', 'principles', 'of', 'workers', 'solidarity', 'direct', 'action', 'and', 'self', 'management', 'the', 'russian', 'revolution', 'the', 'russian', 'revolution', 'of', 'one', 'nine', 'one', 'seven', 'was', 'a', 'seismic', 'event', 'in', 'the', 'development', 'of', 'anarchism', 'as', 'a', 'movement', 'and', 'as', 'a', 'philosophy', 'anarchists', 'participated', 'alongside', 'the', 'bolsheviks', 'in', 'both', 'february', 'and', 'october', 'revolutions', 'many', 'anarchists', 'initially', 'supporting', 'the', 'bolshevik', 'coup', 'however', 'the', 'bolsheviks', 'soon', 'turned', 'against', 'the', 'anarchists', 'and', 'other', 'left', 'wing', 'opposition', 'a', 'conflict', 'which', 'culminated', 'in', 'the', 'one', 'nine', 'one', 'eight', 'kronstadt', 'rebellion', 'anarchists', 'in', 'central', 'russia', 'were', 'imprisoned', 'or', 'driven', 'underground', 'or', 'joined', 'the', 'victorious', 'bolsheviks', 'in', 'ukraine', 'anarchists', 'fought', 'in', 'the', 'civil', 'war', 'against', 'both', 'whites', 'and', 'bolsheviks', 'within', 'the', 'makhnovshchina', 'peasant', 'army', 'led', 'by', 'nestor', 'makhno', 'expelled', 'american', 'anarchists', 'emma', 'goldman', 'and', 'alexander', 'berkman', 'before', 'leaving', 'russia', 'were', 'amongst', 'those', 'agitating', 'in', 'response', 'to', 'bolshevik', 'policy', 'and', 'the', 'suppression', 'of', 'the', 'kronstadt', 'uprising', 'both', 'wrote', 'classic', 'accounts', 'of', 'their', 'experiences', 'in', 'russia', 'aiming', 'to', 'expose', 'the', 'reality', 'of', 'bolshevik', 'control', 'for', 'them', 'bakunin', 's', 'predictions', 'about', 'the', 'consequences', 'of', 'marxist', 'rule', 'had', 'proved', 'all', 'too', 'true', 'the', 'victory', 'of', 'the', 'bolsheviks', 'in', 'the', 'october', 'revolution', 'and', 'the', 'resulting', 'russian', 'civil', 'war', 'did', 'serious', 'damage', 'to', 'anarchist', 'movements', 'internationally', 'many', 'workers', 'and', 'activists', 'saw', 'bolshevik', 'success', 'as', 'setting', 'an', 'example', 'communist', 'parties', 'grew', 'at', 'the', 'expense', 'of', 'anarchism', 'and', 'other', 'socialist', 'movements', 'in', 'france', 'and', 'the', 'us', 'for', 'example', 'the', 'major', 'syndicalist', 'movements', 'of', 'the', 'cgt', 'and', 'iww', 'began', 'to', 'realign', 'themselves', 'away', 'from', 'anarchism', 'and', 'towards', 'the', 'communist', 'international', 'in', 'paris', 'the', 'dielo', 'truda', 'group', 'of', 'russian', 'anarchist', 'exiles', 'which', 'included', 'nestor', 'makhno', 'concluded', 'that', 'anarchists', 'needed', 'to', 'develop', 'new', 'forms', 'of', 'organisation', 'in', 'response', 'to', 'the', 'structures', 'of', 'bolshevism', 'their', 'one', 'nine', 'two', 'six', 'manifesto', 'known', 'as', 'the', 'organisational', 'platform', 'of', 'the', 'libertarian', 'communists', 'was', 'supported', 'by', 'some', 'communist', 'anarchists', 'though', 'opposed', 'by', 'many', 'others', 'the', 'platform', 'continues', 'to', 'inspire', 'some', 'contemporary', 'anarchist', 'groups', 'who', 'believe', 'in', 'an', 'anarchist', 'movement', 'organised', 'around', 'its', 'principles', 'of', 'theoretical', 'unity', 'tactical', 'unity', 'collective', 'responsibility', 'and', 'federalism', 'platformist', 'groups', 'today', 'include', 'the', 'workers', 'solidarity', 'movement', 'in', 'ireland', 'the', 'uk', 's', 'anarchist', 'federation', 'and', 'the', 'late', 'north', 'eastern', 'federation', 'of', 'anarchist', 'communists', 'in', 'the', 'northeastern', 'united', 'states', 'and', 'bordering', 'canada', 'the', 'fight', 'against', 'fascism', 'spain', 'one', 'nine', 'three', 'six', 'members', 'of', 'the', 'cnt', 'construct', 'armoured', 'cars', 'to', 'fight', 'against', 'the', 'fascists', 'in', 'one', 'of', 'the', 'collectivised', 'factories', 'in', 'the', 'one', 'nine', 'two', 'zero', 's', 'and', 'one', 'nine', 'three', 'zero', 's', 'the', 'familiar', 'dynamics', 'of', 'anarchism', 's', 'conflict', 'with', 'the', 'state', 'were', 'transformed', 'by', 'the', 'rise', 'of', 'fascism', 'in', 'europe', 'in', 'many', 'cases', 'european', 'anarchists', 'faced', 'difficult', 'choices', 'should', 'they', 'join', 'in', 'popular', 'fronts', 'with', 'reformist', 'democrats', 'and', 'soviet', 'led', 'communists', 'against', 'a', 'common', 'fascist', 'enemy', 'luigi', 'fabbri', 'an', 'exile', 'from', 'italian', 'fascism', 'was', 'amongst', 'those', 'arguing', 'that', 'fascism', 'was', 'something', 'different', 'fascism', 'is', 'not', 'just', 'another', 'form', 'of', 'government', 'which', 'like', 'all', 'others', 'uses', 'violence', 'it', 'is', 'the', 'most', 'authoritarian', 'and', 'the', 'most', 'violent', 'form', 'of', 'government', 'imaginable', 'it', 'represents', 'the', 'utmost', 'glorification', 'of', 'the', 'theory', 'and', 'practice', 'of', 'the', 'principle', 'of', 'authority', 'in', 'france', 'where', 'the', 'fascists', 'came', 'close', 'to', 'insurrection', 'in', 'the', 'february', 'one', 'nine', 'three', 'four', 'riots', 'anarchists', 'divided', 'over', 'a', 'united', 'front', 'policy', 'in', 'spain', 'the', 'cnt', 'initially', 'refused', 'to', 'join', 'a', 'popular', 'front', 'electoral', 'alliance', 'and', 'abstention', 'by', 'cnt', 'supporters', 'led', 'to', 'a', 'right', 'wing', 'election', 'victory', 'but', 'in', 'one', 'nine', 'three', 'six', 'the', 'cnt', 'changed', 'its', 'policy', 'and', 'anarchist', 'votes', 'helped', 'bring', 'the', 'popular', 'front', 'back', 'to', 'power', 'months', 'later', 'the', 'ruling', 'class', 'responded', 'with', 'an', 'attempted', 'coup', 'and', 'the', 'spanish', 'civil', 'war', 'one', 'nine', 'three', 'six', 'three', 'nine', 'was', 'underway', 'in', 'reponse', 'to', 'the', 'army', 'rebellion', 'an', 'anarchist', 'inspired', 'movement', 'of', 'peasants', 'and', 'workers', 'supported', 'by', 'armed', 'militias', 'took', 'control', 'of', 'the', 'major', 'city', 'of', 'barcelona', 'and', 'of', 'large', 'areas', 'of', 'rural', 'spain', 'where', 'they', 'collectivized', 'the', 'land', 'but', 'even', 'before', 'the', 'eventual', 'fascist', 'victory', 'in', 'one', 'nine', 'three', 'nine', 'the', 'anarchists', 'were', 'losing', 'ground', 'in', 'a', 'bitter', 'struggle', 'with', 'the', 'stalinists', 'the', 'cnt', 'leadership', 'often', 'appeared', 'confused', 'and', 'divided', 'with', 'some', 'members', 'controversially', 'entering', 'the', 'government', 'stalinist', 'led', 'troops', 'suppressed', 'the', 'collectives', 'and', 'persecuted', 'both', 'dissident', 'marxists', 'and', 'anarchists', 'since', 'the', 'late', 'one', 'nine', 'seven', 'zero', 's', 'anarchists', 'have', 'been', 'involved', 'in', 'fighting', 'the', 'rise', 'of', 'neo', 'fascist', 'groups', 'in', 'germany', 'and', 'the', 'united', 'kingdom', 'some', 'anarchists', 'worked', 'within', 'militant', 'anti', 'fascist', 'groups', 'alongside', 'members', 'of', 'the', 'marxist', 'left', 'they', 'advocated', 'directly', 'combating', 'fascists', 'with', 'physical', 'force', 'rather', 'than', 'relying', 'on', 'the', 'state', 'since', 'the', 'late', 'one', 'nine', 'nine', 'zero', 's', 'a', 'similar', 'tendency', 'has', 'developed', 'within', 'us', 'anarchism', 'see', 'also', 'anti', 'racist', 'action', 'us', 'anti', 'fascist', 'action', 'uk', 'antifa', 'religious', 'anarchism', 'leo', 'tolstoy', 'one', 'eight', 'two', 'eight', 'one', 'nine', 'one', 'zero', 'most', 'anarchist', 'culture', 'tends', 'to', 'be', 'secular', 'if', 'not', 'outright', 'anti', 'religious', 'however', 'the', 'combination', 'of', 'religious', 'social', 'conscience', 'historical', 'religiousity', 'amongst', 'oppressed', 'social', 'classes', 'and', 'the', 'compatibility', 'of', 'some', 'interpretations', 'of', 'religious', 'traditions', 'with', 'anarchism', 'has', 'resulted', 'in', 'religious', 'anarchism', 'christian', 'anarchists', 'believe', 'that', 'there', 'is', 'no', 'higher', 'authority', 'than', 'god', 'and', 'oppose', 'earthly', 'authority', 'such', 'as', 'government', 'and', 'established', 'churches', 'they', 'believe', 'that', 'jesus', 'teachings', 'were', 'clearly', 'anarchistic', 'but', 'were', 'corrupted', 'when', 'christianity', 'was', 'declared', 'the', 'official', 'religion', 'of', 'rome', 'christian', 'anarchists', 'who', 'follow', 'jesus', 'directive', 'to', 'turn', 'the', 'other', 'cheek', 'are', 'strict', 'pacifists', 'the', 'most', 'famous', 'advocate', 'of', 'christian', 'anarchism', 'was', 'leo', 'tolstoy', 'author', 'of', 'the', 'kingdom', 'of', 'god', 'is', 'within', 'you', 'who', 'called', 'for', 'a', 'society', 'based', 'on', 'compassion', 'nonviolent', 'principles', 'and', 'freedom', 'christian', 'anarchists', 'tend', 'to', 'form', 'experimental', 'communities', 'they', 'also', 'occasionally', 'resist', 'taxation', 'many', 'christian', 'anarchists', 'are', 'vegetarian', 'or', 'vegan', 'christian', 'anarchy', 'can', 'be', 'said', 'to', 'have', 'roots', 'as', 'old', 'as', 'the', 'religion', 's', 'birth', 'as', 'the', 'early', 'church', 'exhibits', 'many', 'anarchistic', 'tendencies', 'such', 'as', 'communal', 'goods', 'and', 'wealth', 'by', 'aiming', 'to', 'obey', 'utterly', 'certain', 'of', 'the', 'bible', 's', 'teachings', 'certain', 'anabaptist', 'groups', 'of', 'sixteenth', 'century', 'europe', 'attempted', 'to', 'emulate', 'the', 'early', 'church', 's', 'social', 'economic', 'organisation', 'and', 'philosophy', 'by', 'regarding', 'it', 'as', 'the', 'only', 'social', 'structure', 'capable', 'of', 'true', 'obediance', 'to', 'jesus', 'teachings', 'and', 'utterly', 'rejected', 'in', 'theory', 'all', 'earthly', 'hierarchies', 'and', 'authority', 'and', 'indeed', 'non', 'anabaptists', 'in', 'general', 'and', 'violence', 'as', 'ungodly', 'such', 'groups', 'for', 'example', 'the', 'hutterites', 'typically', 'went', 'from', 'initially', 'anarchistic', 'beginnings', 'to', 'as', 'their', 'movements', 'stabalised', 'more', 'authoritarian', 'social', 'models', 'chinese', 'anarchism', 'was', 'most', 'influential', 'in', 'the', 'one', 'nine', 'two', 'zero', 's', 'strands', 'of', 'chinese', 'anarchism', 'included', 'tai', 'xu', 's', 'buddhist', 'anarchism', 'which', 'was', 'influenced', 'by', 'tolstoy', 'and', 'the', 'well', 'field', 'system', 'neopaganism', 'with', 'its', 'focus', 'on', 'the', 'environment', 'and', 'equality', 'along', 'with', 'its', 'often', 'decentralized', 'nature', 'has', 'lead', 'to', 'a', 'number', 'of', 'neopagan', 'anarchists', 'one', 'of', 'the', 'most', 'prominent', 'is', 'starhawk', 'who', 'writes', 'extensively', 'about', 'both', 'spirituality', 'and', 'activism', 'anarchism', 'and', 'feminism', 'emma', 'goldman', 'early', 'french', 'feminists', 'such', 'as', 'jenny', 'd', 'h', 'ricourt', 'and', 'juliette', 'adam', 'criticised', 'the', 'mysogyny', 'in', 'the', 'anarchism', 'of', 'proudhon', 'during', 'the', 'one', 'eight', 'five', 'zero', 's', 'anarcha', 'feminism', 'is', 'a', 'kind', 'of', 'radical', 'feminism', 'that', 'espouses', 'the', 'belief', 'that', 'patriarchy', 'is', 'a', 'fundamental', 'problem', 'in', 'society', 'while', 'anarchist', 'feminism', 'has', 'existed', 'for', 'more', 'than', 'a', 'hundred', 'years', 'its', 'explicit', 'formulation', 'as', 'anarcha', 'feminism', 'dates', 'back', 'to', 'the', 'early', 'seven', 'zero', 's', 'during', 'the', 'second', 'wave', 'feminist', 'movement', 'anarcha', 'feminism', 'views', 'patriarchy', 'as', 'the', 'first', 'manifestation', 'of', 'hierarchy', 'in', 'human', 'history', 'thus', 'the', 'first', 'form', 'of', 'oppression', 'occurred', 'in', 'the', 'dominance', 'of', 'male', 'over', 'female', 'anarcha', 'feminists', 'then', 'conclude', 'that', 'if', 'feminists', 'are', 'against', 'patriarchy', 'they', 'must', 'also', 'be', 'against', 'all', 'forms', 'of', 'hierarchy', 'and', 'therefore', 'must', 'reject', 'the', 'authoritarian', 'nature', 'of', 'the', 'state', 'and', 'capitalism', 'anarcho', 'primitivists', 'see', 'the', 'creation', 'of', 'gender', 'roles', 'and', 'patriarchy', 'a', 'creation', 'of', 'the', 'start', 'of', 'civilization', 'and', 'therefore', 'consider', 'primitivism', 'to', 'also', 'be', 'an', 'anarchist', 'school', 'of', 'thought', 'that', 'addresses', 'feminist', 'concerns', 'eco', 'feminism', 'is', 'often', 'considered', 'a', 'feminist', 'variant', 'of', 'green', 'anarchist', 'feminist', 'thought', 'anarcha', 'feminism', 'is', 'most', 'often', 'associated', 'with', 'early', 'two', 'zero', 'th', 'century', 'authors', 'and', 'theorists', 'such', 'as', 'emma', 'goldman', 'and', 'voltairine', 'de', 'cleyre', 'although', 'even', 'early', 'first', 'wave', 'feminist', 'mary', 'wollstonecraft', 'held', 'proto', 'anarchist', 'views', 'and', 'william', 'godwin', 'is', 'often', 'considered', 'a', 'feminist', 'anarchist', 'precursor', 'it', 'should', 'be', 'noted', 'that', 'goldman', 'and', 'de', 'cleyre', 'though', 'they', 'both', 'opposed', 'the', 'state', 'had', 'opposing', 'philosophies', 'as', 'de', 'cleyre', 'explains', 'miss', 'goldman', 'is', 'a', 'communist', 'i', 'am', 'an', 'individualist', 'she', 'wishes', 'to', 'destroy', 'the', 'right', 'of', 'property', 'i', 'wish', 'to', 'assert', 'it', 'i', 'make', 'my', 'war', 'upon', 'privilege', 'and', 'authority', 'whereby', 'the', 'right', 'of', 'property', 'the', 'true', 'right', 'in', 'that', 'which', 'is', 'proper', 'to', 'the', 'individual', 'is', 'annihilated', 'she', 'believes', 'that', 'co', 'operation', 'would', 'entirely', 'supplant', 'competition', 'i', 'hold', 'that', 'competition', 'in', 'one', 'form', 'or', 'another', 'will', 'always', 'exist', 'and', 'that', 'it', 'is', 'highly', 'desirable', 'it', 'should', 'in', 'the', 'spanish', 'civil', 'war', 'an', 'anarcha', 'feminist', 'group', 'free', 'women', 'organized', 'to', 'defend', 'both', 'anarchist', 'and', 'feminist', 'ideas', 'in', 'the', 'modern', 'day', 'anarchist', 'movement', 'many', 'anarchists', 'male', 'or', 'female', 'consider', 'themselves', 'feminists', 'and', 'anarcha', 'feminist', 'ideas', 'are', 'growing', 'the', 'publishing', 'of', 'quiet', 'rumors', 'an', 'anarcha', 'feminist', 'reader', 'has', 'helped', 'to', 'spread', 'various', 'kinds', 'of', 'anti', 'authoritarian', 'and', 'anarchist', 'feminist', 'ideas', 'to', 'the', 'broader', 'movement', 'wendy', 'mcelroy', 'has', 'popularized', 'an', 'individualist', 'anarchism', 'take', 'on', 'feminism', 'in', 'her', 'books', 'articles', 'and', 'individualist', 'feminist', 'website', 'anarcho', 'capitalism', 'murray', 'rothbard', 'one', 'nine', 'two', 'six', 'one', 'nine', 'nine', 'five', 'anarcho', 'capitalism', 'is', 'a', 'predominantly', 'united', 'states', 'based', 'theoretical', 'tradition', 'that', 'desires', 'a', 'stateless', 'society', 'with', 'the', 'economic', 'system', 'of', 'free', 'market', 'capitalism', 'unlike', 'other', 'branches', 'of', 'anarchism', 'it', 'does', 'not', 'oppose', 'profit', 'or', 'capitalism', 'consequently', 'most', 'anarchists', 'do', 'not', 'recognise', 'anarcho', 'capitalism', 'as', 'a', 'form', 'of', 'anarchism', 'murray', 'rothbard', 's', 'synthesis', 'of', 'classical', 'liberalism', 'and', 'austrian', 'economics', 'was', 'germinal', 'for', 'the', 'development', 'of', 'contemporary', 'anarcho', 'capitalist', 'theory', 'he', 'defines', 'anarcho', 'capitalism', 'in', 'terms', 'of', 'the', 'non', 'aggression', 'principle', 'based', 'on', 'the', 'concept', 'of', 'natural', 'law', 'competiting', 'theorists', 'use', 'egoism', 'utilitarianism', 'used', 'by', 'david', 'friedman', 'or', 'contractarianism', 'used', 'by', 'jan', 'narveson', 'some', 'minarchists', 'such', 'as', 'ayn', 'rand', 'robert', 'nozick', 'and', 'robert', 'a', 'heinlein', 'have', 'influenced', 'anarcho', 'capitalism', 'some', 'anarcho', 'capitalists', 'along', 'with', 'some', 'right', 'wing', 'libertarian', 'historians', 'such', 'as', 'david', 'hart', 'and', 'ralph', 'raico', 'considered', 'similar', 'philosophies', 'existing', 'prior', 'to', 'rothbard', 'to', 'be', 'anarcho', 'capitalist', 'such', 'as', 'those', 'of', 'gustave', 'de', 'molinari', 'and', 'auberon', 'herbert', 'opponents', 'of', 'anarcho', 'capitalists', 'dispute', 'these', 'claims', 'the', 'place', 'of', 'anarcho', 'capitalism', 'within', 'anarchism', 'and', 'indeed', 'whether', 'it', 'is', 'a', 'form', 'of', 'anarchism', 'at', 'all', 'is', 'highly', 'controversial', 'for', 'more', 'on', 'this', 'debate', 'see', 'anarchism', 'and', 'anarcho', 'capitalism', 'anarchism', 'and', 'the', 'environment', 'since', 'the', 'late', 'one', 'nine', 'seven', 'zero', 's', 'anarchists', 'in', 'anglophone', 'and', 'european', 'countries', 'have', 'been', 'taking', 'action', 'for', 'the', 'natural', 'environment', 'eco', 'anarchists', 'or', 'green', 'anarchists', 'believe', 'in', 'deep', 'ecology', 'this', 'is', 'a', 'worldview', 'that', 'embraces', 'biodiversity', 'and', 'sustainability', 'eco', 'anarchists', 'often', 'use', 'direct', 'action', 'against', 'what', 'they', 'see', 'as', 'earth', 'destroying', 'institutions', 'of', 'particular', 'importance', 'is', 'the', 'earth', 'first', 'movement', 'that', 'takes', 'action', 'such', 'as', 'tree', 'sitting', 'another', 'important', 'component', 'is', 'ecofeminism', 'which', 'sees', 'the', 'domination', 'of', 'nature', 'as', 'a', 'metaphor', 'for', 'the', 'domination', 'of', 'women', 'green', 'anarchism', 'also', 'involves', 'a', 'critique', 'of', 'industrial', 'capitalism', 'and', 'for', 'some', 'green', 'anarchists', 'civilization', 'itself', 'primitivism', 'is', 'a', 'predominantly', 'western', 'philosophy', 'that', 'advocates', 'a', 'return', 'to', 'a', 'pre', 'industrial', 'and', 'usually', 'pre', 'agricultural', 'society', 'it', 'develops', 'a', 'critique', 'of', 'industrial', 'civilization', 'in', 'this', 'critique', 'technology', 'and', 'development', 'have', 'alienated', 'people', 'from', 'the', 'natural', 'world', 'this', 'philosophy', 'develops', 'themes', 'present', 'in', 'the', 'political', 'action', 'of', 'the', 'luddites', 'and', 'the', 'writings', 'of', 'jean', 'jacques', 'rousseau', 'primitivism', 'developed', 'in', 'the', 'context', 'of', 'the', 'reclaim', 'the', 'streets', 'earth', 'first', 'and', 'the', 'earth', 'liberation', 'front', 'movements', 'john', 'zerzan', 'wrote', 'that', 'civilization', 'not', 'just', 'the', 'state', 'would', 'need', 'to', 'fall', 'for', 'anarchy', 'to', 'be', 'achieved', 'anarcho', 'primitivists', 'point', 'to', 'the', 'anti', 'authoritarian', 'nature', 'of', 'many', 'primitive', 'or', 'hunter', 'gatherer', 'societies', 'throughout', 'the', 'world', 's', 'history', 'as', 'examples', 'of', 'anarchist', 'societies', 'other', 'branches', 'and', 'offshoots', 'anarchism', 'generates', 'many', 'eclectic', 'and', 'syncretic', 'philosophies', 'and', 'movements', 'since', 'the', 'western', 'social', 'formet', 'in', 'the', 'one', 'nine', 'six', 'zero', 's', 'and', 'one', 'nine', 'seven', 'zero', 's', 'a', 'number', 'new', 'of', 'movements', 'and', 'schools', 'have', 'appeared', 'most', 'of', 'these', 'stances', 'are', 'limited', 'to', 'even', 'smaller', 'numbers', 'than', 'the', 'schools', 'and', 'movements', 'listed', 'above', 'hakim', 'bey', 'post', 'left', 'anarchy', 'post', 'left', 'anarchy', 'also', 'called', 'egoist', 'anarchism', 'seeks', 'to', 'distance', 'itself', 'from', 'the', 'traditional', 'left', 'communists', 'liberals', 'social', 'democrats', 'etc', 'and', 'to', 'escape', 'the', 'confines', 'of', 'ideology', 'in', 'general', 'post', 'leftists', 'argue', 'that', 'anarchism', 'has', 'been', 'weakened', 'by', 'its', 'long', 'attachment', 'to', 'contrary', 'leftist', 'movements', 'and', 'single', 'issue', 'causes', 'anti', 'war', 'anti', 'nuclear', 'etc', 'it', 'calls', 'for', 'a', 'synthesis', 'of', 'anarchist', 'thought', 'and', 'a', 'specifically', 'anti', 'authoritarian', 'revolutionary', 'movement', 'outside', 'of', 'the', 'leftist', 'milieu', 'it', 'often', 'focuses', 'on', 'the', 'individual', 'rather', 'than', 'speaking', 'in', 'terms', 'of', 'class', 'or', 'other', 'broad', 'generalizations', 'and', 'shuns', 'organizational', 'tendencies', 'in', 'favor', 'of', 'the', 'complete', 'absence', 'of', 'explicit', 'hierarchy', 'important', 'groups', 'and', 'individuals', 'associated', 'with', 'post', 'left', 'anarchy', 'include', 'crimethinc', 'the', 'magazine', 'anarchy', 'a', 'journal', 'of', 'desire', 'armed', 'and', 'its', 'editor', 'jason', 'mcquinn', 'bob', 'black', 'hakim', 'bey', 'and', 'others', 'for', 'more', 'information', 'see', 'infoshop', 'org', 's', 'anarchy', 'after', 'leftism', 'section', 'and', 'the', 'post', 'left', 'section', 'on', 'anarchism', 'ws', 'see', 'also', 'post', 'left', 'anarchy', 'post', 'structuralism', 'the', 'term', 'postanarchism', 'was', 'originated', 'by', 'saul', 'newman', 'first', 'receiving', 'popular', 'attention', 'in', 'his', 'book', 'from', 'bakunin', 'to', 'lacan', 'to', 'refer', 'to', 'a', 'theoretical', 'move', 'towards', 'a', 'synthesis', 'of', 'classical', 'anarchist', 'theory', 'and', 'poststructuralist', 'thought', 'subsequent', 'to', 'newman', 's', 'use', 'of', 'the', 'term', 'however', 'it', 'has', 'taken', 'on', 'a', 'life', 'of', 'its', 'own', 'and', 'a', 'wide', 'range', 'of', 'ideas', 'including', 'autonomism', 'post', 'left', 'anarchy', 'situationism', 'post', 'colonialism', 'and', 'zapatismo', 'by', 'its', 'very', 'nature', 'post', 'anarchism', 'rejects', 'the', 'idea', 'that', 'it', 'should', 'be', 'a', 'coherent', 'set', 'of', 'doctrines', 'and', 'beliefs', 'as', 'such', 'it', 'is', 'difficult', 'if', 'not', 'impossible', 'to', 'state', 'with', 'any', 'degree', 'of', 'certainty', 'who', 'should', 'or', 'shouldn', 't', 'be', 'grouped', 'under', 'the', 'rubric', 'nonetheless', 'key', 'thinkers', 'associated', 'with', 'post', 'anarchism', 'include', 'saul', 'newman', 'todd', 'may', 'gilles', 'deleuze', 'and', 'f', 'lix', 'guattari', 'external', 'reference', 'postanarchism', 'clearinghouse', 'see', 'also', 'post', 'anarchism', 'insurrectionary', 'anarchism', 'insurrectionary', 'anarchism', 'is', 'a', 'form', 'of', 'revolutionary', 'anarchism', 'critical', 'of', 'formal', 'anarchist', 'labor', 'unions', 'and', 'federations', 'insurrectionary', 'anarchists', 'advocate', 'informal', 'organization', 'including', 'small', 'affinity', 'groups', 'carrying', 'out', 'acts', 'of', 'resistance', 'in', 'various', 'struggles', 'and', 'mass', 'organizations', 'called', 'base', 'structures', 'which', 'can', 'include', 'exploited', 'individuals', 'who', 'are', 'not', 'anarchists', 'proponents', 'include', 'wolfi', 'landstreicher', 'and', 'alfredo', 'm', 'bonanno', 'author', 'of', 'works', 'including', 'armed', 'joy', 'and', 'the', 'anarchist', 'tension', 'this', 'tendency', 'is', 'represented', 'in', 'the', 'us', 'in', 'magazines', 'such', 'as', 'willful', 'disobedience', 'and', 'killing', 'king', 'abacus', 'see', 'also', 'insurrectionary', 'anarchism', 'small', 'a', 'anarchism', 'small', 'a', 'anarchism', 'is', 'a', 'term', 'used', 'in', 'two', 'different', 'but', 'not', 'unconnected', 'contexts', 'dave', 'neal', 'posited', 'the', 'term', 'in', 'opposition', 'to', 'big', 'a', 'anarchism', 'in', 'the', 'article', 'anarchism', 'ideology', 'or', 'methodology', 'while', 'big', 'a', 'anarchism', 'referred', 'to', 'ideological', 'anarchists', 'small', 'a', 'anarchism', 'was', 'applied', 'to', 'their', 'methodological', 'counterparts', 'those', 'who', 'viewed', 'anarchism', 'as', 'a', 'way', 'of', 'acting', 'or', 'a', 'historical', 'tendency', 'against', 'illegitimate', 'authority', 'as', 'an', 'anti', 'ideological', 'position', 'small', 'a', 'anarchism', 'shares', 'some', 'similarities', 'with', 'post', 'left', 'anarchy', 'david', 'graeber', 'and', 'andrej', 'grubacic', 'offer', 'an', 'alternative', 'use', 'of', 'the', 'term', 'applying', 'it', 'to', 'groups', 'and', 'movements', 'organising', 'according', 'to', 'or', 'acting', 'in', 'a', 'manner', 'consistent', 'with', 'anarchist', 'principles', 'of', 'decentralisation', 'voluntary', 'association', 'mutual', 'aid', 'the', 'network', 'model', 'and', 'crucially', 'the', 'rejection', 'of', 'any', 'idea', 'that', 'the', 'end', 'justifies', 'the', 'means', 'let', 'alone', 'that', 'the', 'business', 'of', 'a', 'revolutionary', 'is', 'to', 'seize', 'state', 'power', 'and', 'then', 'begin', 'imposing', 'one', 's', 'vision', 'at', 'the', 'point', 'of', 'a', 'gun', 'other', 'issues', 'conceptions', 'of', 'an', 'anarchist', 'society', 'many', 'political', 'philosophers', 'justify', 'support', 'of', 'the', 'state', 'as', 'a', 'means', 'of', 'regulating', 'violence', 'so', 'that', 'the', 'destruction', 'caused', 'by', 'human', 'conflict', 'is', 'minimized', 'and', 'fair', 'relationships', 'are', 'established', 'anarchists', 'argue', 'that', 'pursuit', 'of', 'these', 'ends', 'does', 'not', 'justify', 'the', 'establishment', 'of', 'a', 'state', 'many', 'argue', 'that', 'the', 'state', 'is', 'incompatible', 'with', 'those', 'goals', 'and', 'the', 'cause', 'of', 'chaos', 'violence', 'and', 'war', 'anarchists', 'argue', 'that', 'the', 'state', 'helps', 'to', 'create', 'a', 'monopoly', 'on', 'violence', 'and', 'uses', 'violence', 'to', 'advance', 'elite', 'interests', 'much', 'effort', 'has', 'been', 'dedicated', 'to', 'explaining', 'how', 'anarchist', 'societies', 'would', 'handle', 'criminality', 'see', 'also', 'anarchism', 'and', 'society', 'civil', 'rights', 'and', 'cultural', 'sovereignty', 'black', 'anarchism', 'opposes', 'the', 'existence', 'of', 'a', 'state', 'capitalism', 'and', 'subjugation', 'and', 'domination', 'of', 'people', 'of', 'color', 'and', 'favors', 'a', 'non', 'hierarchical', 'organization', 'of', 'society', 'theorists', 'include', 'ashanti', 'alston', 'lorenzo', 'komboa', 'ervin', 'and', 'sam', 'mbah', 'anarchist', 'people', 'of', 'color', 'was', 'created', 'as', 'a', 'forum', 'for', 'non', 'caucasian', 'anarchists', 'to', 'express', 'their', 'thoughts', 'about', 'racial', 'issues', 'within', 'the', 'anarchist', 'movement', 'particularly', 'within', 'the', 'united', 'states', 'national', 'anarchism', 'is', 'a', 'political', 'view', 'which', 'seeks', 'to', 'unite', 'cultural', 'or', 'ethnic', 'preservation', 'with', 'anarchist', 'views', 'its', 'adherents', 'propose', 'that', 'those', 'preventing', 'ethnic', 'groups', 'or', 'races', 'from', 'living', 'in', 'separate', 'autonomous', 'groupings', 'should', 'be', 'resisted', 'anti', 'racist', 'action', 'is', 'not', 'an', 'anarchist', 'group', 'but', 'many', 'anarchists', 'are', 'involved', 'it', 'focuses', 'on', 'publicly', 'confronting', 'racist', 'agitators', 'the', 'zapatista', 'movement', 'of', 'chiapas', 'mexico', 'is', 'a', 'cultural', 'sovereignty', 'group', 'with', 'some', 'anarchist', 'proclivities', 'neocolonialism', 'and', 'globalization', 'nearly', 'all', 'anarchists', 'oppose', 'neocolonialism', 'as', 'an', 'attempt', 'to', 'use', 'economic', 'coercion', 'on', 'a', 'global', 'scale', 'carried', 'out', 'through', 'state', 'institutions', 'such', 'as', 'the', 'world', 'bank', 'world', 'trade', 'organization', 'group', 'of', 'eight', 'and', 'the', 'world', 'economic', 'forum', 'globalization', 'is', 'an', 'ambiguous', 'term', 'that', 'has', 'different', 'meanings', 'to', 'different', 'anarchist', 'factions', 'most', 'anarchists', 'use', 'the', 'term', 'to', 'mean', 'neocolonialism', 'and', 'or', 'cultural', 'imperialism', 'which', 'they', 'may', 'see', 'as', 'related', 'many', 'are', 'active', 'in', 'the', 'anti', 'globalization', 'movement', 'others', 'particularly', 'anarcho', 'capitalists', 'use', 'globalization', 'to', 'mean', 'the', 'worldwide', 'expansion', 'of', 'the', 'division', 'of', 'labor', 'and', 'trade', 'which', 'they', 'see', 'as', 'beneficial', 'so', 'long', 'as', 'governments', 'do', 'not', 'intervene', 'parallel', 'structures', 'many', 'anarchists', 'try', 'to', 'set', 'up', 'alternatives', 'to', 'state', 'supported', 'institutions', 'and', 'outposts', 'such', 'as', 'food', 'not', 'bombs', 'infoshops', 'educational', 'systems', 'such', 'as', 'home', 'schooling', 'neighborhood', 'mediation', 'arbitration', 'groups', 'and', 'so', 'on', 'the', 'idea', 'is', 'to', 'create', 'the', 'structures', 'for', 'a', 'new', 'anti', 'authoritarian', 'society', 'in', 'the', 'shell', 'of', 'the', 'old', 'authoritarian', 'one', 'technology', 'recent', 'technological', 'developments', 'have', 'made', 'the', 'anarchist', 'cause', 'both', 'easier', 'to', 'advance', 'and', 'more', 'conceivable', 'to', 'people', 'many', 'people', 'use', 'the', 'internet', 'to', 'form', 'on', 'line', 'communities', 'intellectual', 'property', 'is', 'undermined', 'and', 'a', 'gift', 'culture', 'supported', 'by', 'sharing', 'music', 'files', 'open', 'source', 'programming', 'and', 'the', 'free', 'software', 'movement', 'these', 'cyber', 'communities', 'include', 'the', 'gnu', 'linux', 'indymedia', 'and', 'wiki', 'some', 'anarchists', 'see', 'information', 'technology', 'as', 'the', 'best', 'weapon', 'to', 'defeat', 'authoritarianism', 'some', 'even', 'think', 'the', 'information', 'age', 'makes', 'eventual', 'anarchy', 'inevitable', 'see', 'also', 'crypto', 'anarchism', 'and', 'cypherpunk', 'pacifism', 'some', 'anarchists', 'consider', 'pacifism', 'opposition', 'to', 'war', 'to', 'be', 'inherent', 'in', 'their', 'philosophy', 'anarcho', 'pacifists', 'take', 'it', 'further', 'and', 'follow', 'leo', 'tolstoy', 's', 'belief', 'in', 'non', 'violence', 'anarchists', 'see', 'war', 'as', 'an', 'activity', 'in', 'which', 'the', 'state', 'seeks', 'to', 'gain', 'and', 'consolidate', 'power', 'both', 'domestically', 'and', 'in', 'foreign', 'lands', 'and', 'subscribe', 'to', 'randolph', 'bourne', 's', 'view', 'that', 'war', 'is', 'the', 'health', 'of', 'the', 'state', 'a', 'lot', 'of', 'anarchist', 'activity', 'has', 'been', 'anti', 'war', 'based', 'parliamentarianism', 'in', 'general', 'terms', 'the', 'anarchist', 'ethos', 'opposes', 'voting', 'in', 'elections', 'because', 'voting', 'amounts', 'to', 'condoning', 'the', 'state', 'voluntaryism', 'is', 'an', 'anarchist', 'school', 'of', 'thought', 'which', 'emphasizes', 'tending', 'your', 'own', 'garden', 'and', 'neither', 'ballots', 'nor', 'bullets', 'the', 'anarchist', 'case', 'against', 'voting', 'is', 'explained', 'in', 'the', 'ethics', 'of', 'voting', 'by', 'george', 'h', 'smith', 'also', 'see', 'voting', 'anarchists', 'an', 'oxymoron', 'or', 'what', 'by', 'joe', 'peacott', 'and', 'writings', 'by', 'fred', 'woodworth', 'sectarianism', 'most', 'anarchist', 'schools', 'of', 'thought', 'are', 'to', 'some', 'degree', 'sectarian', 'there', 'is', 'often', 'a', 'difference', 'of', 'opinion', 'within', 'each', 'school', 'about', 'how', 'to', 'react', 'to', 'or', 'interact', 'with', 'other', 'schools', 'some', 'such', 'as', 'panarchists', 'believe', 'that', 'it', 'is', 'possible', 'for', 'a', 'variety', 'of', 'modes', 'of', 'social', 'life', 'to', 'coexist', 'and', 'compete', 'some', 'anarchists', 'view', 'opposing', 'schools', 'as', 'a', 'social', 'impossibility', 'and', 'resist', 'interaction', 'others', 'see', 'opportunities', 'for', 'coalition', 'building', 'or', 'at', 'least', 'temporary', 'alliances', 'for', 'specific', 'purposes', 'see', 'anarchism', 'without', 'adjectives', 'criticisms', 'of', 'anarchism', 'main', 'article', 'criticisms', 'of', 'anarchism', 'violence', 'since', 'anarchism', 'has', 'often', 'been', 'associated', 'with', 'violence', 'and', 'destruction', 'some', 'people', 'have', 'seen', 'it', 'as', 'being', 'too', 'violent', 'on', 'the', 'other', 'hand', 'hand', 'frederick', 'engels', 'criticsed', 'anarchists', 'for', 'not', 'being', 'violent', 'enough', 'a', 'revolution', 'is', 'certainly', 'the', 'most', 'authoritarian', 'thing', 'there', 'is', 'it', 'is', 'the', 'act', 'whereby', 'one', 'part', 'of', 'the', 'population', 'imposes', 'its', 'will', 'upon', 'the', 'other', 'part', 'by', 'means', 'of', 'rifles', 'bayonets', 'and', 'cannon', 'authoritarian', 'means', 'if', 'such', 'there', 'be', 'at', 'all', 'and', 'if', 'the', 'victorious', 'party', 'does', 'not', 'want', 'to', 'have', 'fought', 'in', 'vain', 'it', 'must', 'maintain', 'this', 'rule', 'by', 'means', 'of', 'the', 'terror', 'which', 'its', 'arms', 'inspire', 'in', 'the', 'reactionists', 'would', 'the', 'paris', 'commune', 'have', 'lasted', 'a', 'single', 'day', 'if', 'it', 'had', 'not', 'made', 'use', 'of', 'this', 'authority', 'of', 'the', 'armed', 'people', 'against', 'the', 'bourgeois', 'utopianism', 'anarchism', 'is', 'often', 'criticised', 'as', 'unfeasible', 'or', 'plain', 'utopian', 'even', 'by', 'many', 'who', 'agree', 'that', 'it', 's', 'a', 'nice', 'idea', 'in', 'principle', 'for', 'example', 'carl', 'landauer', 'in', 'his', 'book', 'european', 'socialism', 'criticizes', 'anarchism', 'as', 'being', 'unrealistically', 'utopian', 'and', 'holds', 'that', 'government', 'is', 'a', 'lesser', 'evil', 'than', 'a', 'society', 'without', 'repressive', 'force', 'he', 'holds', 'that', 'the', 'belief', 'that', 'ill', 'intentions', 'will', 'cease', 'if', 'repressive', 'force', 'disappears', 'is', 'an', 'absurdity', 'however', 'it', 'must', 'be', 'noted', 'that', 'not', 'all', 'anarchists', 'have', 'such', 'a', 'utopian', 'view', 'of', 'anarchism', 'for', 'example', 'some', 'such', 'as', 'benjamin', 'tucker', 'advocate', 'privately', 'funded', 'institutions', 'that', 'defend', 'individual', 'liberty', 'and', 'property', 'however', 'other', 'anarchists', 'such', 'as', 'sir', 'herbert', 'read', 'proudly', 'accept', 'the', 'characterization', 'utopian', 'class', 'character', 'marxists', 'have', 'characterised', 'anarchism', 'as', 'an', 'expression', 'of', 'the', 'class', 'interests', 'of', 'the', 'petite', 'bourgeoisie', 'or', 'perhaps', 'the', 'lumpenproletariat', 'see', 'e', 'g', 'plekhanov', 'for', 'a', 'marxist', 'critique', 'of', 'one', 'eight', 'nine', 'five', 'anarchists', 'have', 'also', 'been', 'characterised', 'as', 'spoilt', 'middle', 'class', 'dilettantes', 'most', 'recently', 'in', 'relation', 'to', 'anti', 'capitalist', 'protesters', 'tacit', 'authoritarianism', 'in', 'recent', 'decades', 'anarchism', 'has', 'been', 'criticised', 'by', 'situationists', 'post', 'anarchists', 'and', 'others', 'of', 'preserving', 'tacitly', 'statist', 'authoritarian', 'or', 'bureaucratic', 'tendencies', 'behind', 'a', 'dogmatic', 'facade', 'hypocrisy', 'some', 'critics', 'point', 'to', 'the', 'sexist', 'and', 'racist', 'views', 'of', 'some', 'prominent', 'anarchists', 'notably', 'proudhon', 'and', 'bakunin', 'as', 'examples', 'of', 'hypocrisy', 'inherent', 'within', 'anarchism', 'while', 'many', 'anarchists', 'however', 'dismiss', 'that', 'the', 'personal', 'prejudices', 'of', 'one', 'nine', 'th', 'century', 'theorists', 'influence', 'the', 'beliefs', 'of', 'present', 'day', 'anarchists', 'others', 'criticise', 'modern', 'anarchism', 'for', 'continuing', 'to', 'be', 'eurocentric', 'and', 'reference', 'the', 'impact', 'of', 'anarchist', 'thinkers', 'like', 'proudhon', 'on', 'fascism', 'through', 'groups', 'like', 'cercle', 'proudhon', 'anarcho', 'capitalist', 'bryan', 'caplan', 'argues', 'that', 'the', 'treatment', 'of', 'fascists', 'and', 'suspected', 'fascist', 'sympathizers', 'by', 'spanish', 'anarchists', 'in', 'the', 'spanish', 'civil', 'war', 'was', 'a', 'form', 'of', 'illegitimate', 'coercion', 'making', 'the', 'proffessed', 'anarchists', 'ultimately', 'just', 'a', 'third', 'faction', 'of', 'totalitarians', 'alongside', 'the', 'communists', 'and', 'fascists', 'he', 'also', 'criticizes', 'the', 'willingness', 'of', 'the', 'cnt', 'to', 'join', 'the', 'statist', 'republican', 'government', 'during', 'the', 'civil', 'war', 'and', 'references', 'stanley', 'g', 'payne', 's', 'book', 'on', 'the', 'franco', 'regime', 'which', 'claims', 'that', 'the', 'cnt', 'entered', 'negotiations', 'with', 'the', 'fascist', 'government', 'six', 'years', 'after', 'the', 'war', 'cultural', 'phenomena', 'noam', 'chomsky', 'one', 'nine', 'two', 'eight', 'the', 'kind', 'of', 'anarchism', 'that', 'is', 'most', 'easily', 'encountered', 'in', 'popular', 'culture', 'is', 'represented', 'by', 'celebrities', 'who', 'publicly', 'identify', 'themselves', 'as', 'anarchists', 'although', 'some', 'anarchists', 'reject', 'any', 'focus', 'on', 'such', 'famous', 'living', 'individuals', 'as', 'inherently', 'litist', 'the', 'following', 'figures', 'are', 'examples', 'of', 'prominent', 'publicly', 'self', 'avowed', 'anarchists', 'the', 'mit', 'professor', 'of', 'linguistics', 'noam', 'chomsky', 'the', 'science', 'fiction', 'author', 'ursula', 'k', 'le', 'guin', 'the', 'social', 'historian', 'howard', 'zinn', 'entertainer', 'and', 'author', 'hans', 'alfredsson', 'the', 'avant', 'garde', 'artist', 'nicol', 's', 'rossell', 'in', 'denmark', 'the', 'freetown', 'christiania', 'was', 'created', 'in', 'downtown', 'copenhagen', 'the', 'housing', 'and', 'employment', 'crisis', 'in', 'most', 'of', 'western', 'europe', 'led', 'to', 'the', 'formation', 'of', 'communes', 'and', 'squatter', 'movements', 'like', 'the', 'one', 'still', 'thriving', 'in', 'barcelona', 'in', 'catalonia', 'militant', 'resistance', 'to', 'neo', 'nazi', 'groups', 'in', 'places', 'like', 'germany', 'and', 'the', 'uprisings', 'of', 'autonomous', 'marxism', 'situationist', 'and', 'autonomist', 'groups', 'in', 'france', 'and', 'italy', 'also', 'helped', 'to', 'give', 'popularity', 'to', 'anti', 'authoritarian', 'non', 'capitalist', 'ideas', 'in', 'various', 'musical', 'styles', 'anarchism', 'rose', 'in', 'popularity', 'most', 'famous', 'for', 'the', 'linking', 'of', 'anarchist', 'ideas', 'and', 'music', 'has', 'been', 'punk', 'rock', 'although', 'in', 'the', 'modern', 'age', 'hip', 'hop', 'and', 'folk', 'music', 'are', 'also', 'becoming', 'important', 'mediums', 'for', 'the', 'spreading', 'of', 'the', 'anarchist', 'message', 'in', 'the', 'uk', 'this', 'was', 'associated', 'with', 'the', 'punk', 'rock', 'movement', 'the', 'band', 'crass', 'is', 'celebrated', 'for', 'its', 'anarchist', 'and', 'pacifist', 'ideas', 'the', 'dutch', 'punk', 'band', 'the', 'ex', 'further', 'exemplifies', 'this', 'expression', 'for', 'further', 'details', 'see', 'anarcho', 'punk', 'see', 'also', 'there', 'are', 'many', 'concepts', 'relevant', 'to', 'the', 'topic', 'of', 'anarchism', 'this', 'is', 'a', 'brief', 'summary', 'there', 'is', 'also', 'a', 'more', 'extensive', 'list', 'of', 'anarchist', 'concepts', 'individualist', 'anarchism', 'anarcho', 'communism', 'anarcho', 'syndicalism', 'anarcho', 'capitalism', 'mutualism', 'christian', 'anarchism', 'anarcha', 'feminism', 'green', 'anarchism', 'nihilist', 'anarchism', 'anarcho', 'nationalism', 'black', 'anarchism', 'national', 'anarchism', 'post', 'anarchism', 'post', 'left', 'anarchism', 'libertarian', 'socialism', 'anarchist', 'symbolism', 'list', 'of', 'anarchism', 'links', 'list', 'of', 'anarchists', 'list', 'of', 'anarchist', 'organizations', 'major', 'conflicts', 'within', 'anarchist', 'thought', 'past', 'and', 'present', 'anarchist', 'communities', 'historical', 'events', 'paris', 'commune', 'one', 'eight', 'seven', 'one', 'haymarket', 'riot', 'one', 'eight', 'eight', 'six', 'the', 'makhnovschina', 'one', 'nine', 'one', 'seven', 'one', 'nine', 'two', 'one', 'kronstadt', 'rebellion', 'one', 'nine', 'two', 'one', 'spanish', 'revolution', 'one', 'nine', 'three', 'six', 'see', 'anarchism', 'in', 'spain', 'and', 'spanish', 'revolution', 'may', 'one', 'nine', 'six', 'eight', 'france', 'one', 'nine', 'six', 'eight', 'wto', 'meeting', 'in', 'seattle', 'one', 'nine', 'nine', 'nine', 'books', 'the', 'following', 'is', 'a', 'sample', 'of', 'books', 'that', 'have', 'been', 'referenced', 'in', 'this', 'page', 'a', 'more', 'complete', 'list', 'can', 'be', 'found', 'at', 'the', 'list', 'of', 'anarchist', 'books', 'mikhail', 'bakunin', 'god', 'and', 'the', 'state', 'emma', 'goldman', 'anarchism', 'other', 'essays', 'peter', 'kropotkin', 'mutual', 'aid', 'pierre', 'joseph', 'proudhon', 'what', 'is', 'property', 'rudolf', 'rocker', 'anarcho', 'syndicalism', 'murray', 'rothbard', 'the', 'ethics', 'of', 'liberty', 'max', 'stirner', 'the', 'ego', 'and', 'its', 'own', 'leo', 'tolstoy', 'the', 'kingdom', 'of', 'god', 'is', 'within', 'you', 'anarchism', 'by', 'region', 'culture', 'african', 'anarchism', 'anarchism', 'in', 'spain', 'anarchism', 'in', 'the', 'english', 'tradition', 'chinese', 'anarchism', 'references', 'these', 'notes', 'have', 'no', 'corresponding', 'reference', 'in', 'the', 'article', 'they', 'might', 'be', 're', 'used', 'against', 'politics', 'appleton', 'boston', 'anarchists', 'yarros', 'victor', 'liberty', 'vii', 'january', 'two', 'one', 'eight', 'nine', 'two', 'noam', 'chomsky', 'on', 'anarchism', 'by', 'noam', 'chomsky', 'external', 'links', 'the', 'overwhelming', 'diversity', 'and', 'number', 'of', 'links', 'relating', 'to', 'anarchism', 'is', 'extensively', 'covered', 'on', 'the', 'links', 'subpage', 'anarchoblogs', 'blogs', 'by', 'anarchists', 'anarchy', 'archives', 'extensively', 'archives', 'information', 'relating', 'to', 'famous', 'anarchists', 'this', 'includes', 'many', 'of', 'their', 'books', 'and', 'other', 'publications', 'hundreds', 'of', 'anarchists', 'are', 'listed', 'with', 'short', 'bios', 'links', 'dedicated', 'pages', 'at', 'the', 'daily', 'bleed', 's', 'anarchist', 'encyclopedia', 'infoshop', 'org', 'wikipedia', 'page', 'industrial', 'workers', 'of', 'the', 'world', 'anarchism', 'forms', 'of', 'government', 'political', 'ideology', 'entry', 'points', 'political', 'theories', 'social', 'philosophy', 'autism', 'is', 'classified', 'as', 'a', 'neurodevelopmental', 'disorder', 'that', 'manifests', 'itself', 'in', 'markedly', 'abnormal', 'social', 'interaction', 'communication', 'ability', 'patterns', 'of', 'interests', 'and', 'patterns', 'of', 'behavior', 'although', 'the', 'specific', 'etiology', 'of', 'autism', 'is', 'unknown', 'many', 'researchers', 'suspect', 'that', 'autism', 'results', 'from', 'genetically', 'mediated', 'vulnerabilities', 'to', 'environmental', 'triggers', 'and', 'while', 'there', 'is', 'disagreement', 'about', 'the', 'magnitude', 'nature', 'and', 'mechanisms', 'for', 'such', 'environmental', 'factors', 'researchers', 'have', 'found', 'at', 'least', 'seven', 'major', 'genes', 'prevalent', 'among', 'individuals', 'diagnosed', 'as', 'autistic', 'some', 'estimate', 'that', 'autism', 'occurs', 'in', 'as', 'many', 'as', 'one', 'united', 'states', 'child', 'in', 'one', 'six', 'six', 'however', 'the', 'national', 'institute', 'of', 'mental', 'health', 'gives', 'a', 'more', 'conservative', 'estimate', 'of', 'one', 'in', 'one', 'zero', 'zero', 'zero', 'for', 'families', 'that', 'already', 'have', 'one', 'autistic', 'child', 'the', 'odds', 'of', 'a', 'second', 'autistic', 'child', 'may', 'be', 'as', 'high', 'as', 'one', 'in', 'twenty', 'diagnosis', 'is', 'based', 'on', 'a', 'list', 'of', 'psychiatric', 'criteria', 'and', 'a', 'series', 'of', 'standardized', 'clinical', 'tests', 'may', 'also', 'be', 'used', 'autism', 'may', 'not', 'be', 'physiologically', 'obvious', 'a', 'complete', 'physical', 'and', 'neurological', 'evaluation', 'will', 'typically', 'be', 'part', 'of', 'diagnosing', 'autism', 'some', 'now', 'speculate', 'that', 'autism', 'is', 'not', 'a', 'single', 'condition', 'but', 'a', 'group', 'of', 'several', 'distinct', 'conditions', 'that', 'manifest', 'in', 'similar', 'ways', 'by', 'definition', 'autism', 'must', 'manifest', 'delays', 'in', 'social', 'interaction', 'language', 'as', 'used', 'in', 'social', 'communication', 'or', 'symbolic', 'or', 'imaginative', 'play', 'with', 'onset', 'prior', 'to', 'age', 'three', 'years', 'according', 'to', 'the', 'diagnostic', 'and', 'statistical', 'manual', 'of', 'mental', 'disorders', 'the', 'icd', 'one', 'zero', 'also', 'says', 'that', 'symptoms', 'must', 'manifest', 'before', 'the', 'age', 'of', 'three', 'years', 'there', 'have', 'been', 'large', 'increases', 'in', 'the', 'reported', 'incidence', 'of', 'autism', 'for', 'reasons', 'that', 'are', 'heavily', 'debated', 'by', 'researchers', 'in', 'psychology', 'and', 'related', 'fields', 'within', 'the', 'scientific', 'community', 'some', 'children', 'with', 'autism', 'have', 'improved', 'their', 'social', 'and', 'other', 'skills', 'to', 'the', 'point', 'where', 'they', 'can', 'fully', 'participate', 'in', 'mainstream', 'education', 'and', 'social', 'events', 'but', 'there', 'are', 'lingering', 'concerns', 'that', 'an', 'absolute', 'cure', 'from', 'autism', 'is', 'impossible', 'with', 'current', 'technology', 'however', 'many', 'autistic', 'children', 'and', 'adults', 'who', 'are', 'able', 'to', 'communicate', 'at', 'least', 'in', 'writing', 'are', 'opposed', 'to', 'attempts', 'to', 'cure', 'their', 'conditions', 'and', 'see', 'such', 'conditions', 'as', 'part', 'of', 'who', 'they', 'are', 'history', 'dr', 'hans', 'asperger', 'described', 'a', 'form', 'of', 'autism', 'in', 'the', 'one', 'nine', 'four', 'zero', 's', 'that', 'later', 'became', 'known', 'as', 'asperger', 's', 'syndrome', 'the', 'word', 'autism', 'was', 'first', 'used', 'in', 'the', 'english', 'language', 'by', 'swiss', 'psychiatrist', 'eugene', 'bleuler', 'in', 'a', 'one', 'nine', 'one', 'two', 'number', 'of', 'the', 'american', 'journal', 'of', 'insanity', 'it', 'comes', 'from', 'the', 'greek', 'word', 'for', 'self', 'however', 'the', 'classification', 'of', 'autism', 'did', 'not', 'occur', 'until', 'the', 'middle', 'of', 'the', 'twentieth', 'century', 'when', 'in', 'one', 'nine', 'four', 'three', 'psychiatrist', 'dr', 'leo', 'kanner', 'of', 'the', 'johns', 'hopkins', 'hospital', 'in', 'baltimore', 'reported', 'on', 'one', 'one', 'child', 'patients', 'with', 'striking', 'behavioral', 'similarities', 'and', 'introduced', 'the', 'label', 'early', 'infantile', 'autism', 'he', 'suggested', 'autism', 'from', 'the', 'greek', 'autos', 'meaning', 'self', 'to', 'describe', 'the', 'fact', 'that', 'the', 'children', 'seemed', 'to', 'lack', 'interest', 'in', 'other', 'people', 'although', 'kanner', 's', 'first', 'paper', 'on', 'the', 'subject', 'was', 'published', 'in', 'a', 'now', 'defunct', 'journal', 'the', 'nervous', 'child', 'almost', 'every', 'characteristic', 'he', 'originally', 'described', 'is', 'still', 'regarded', 'as', 'typical', 'of', 'the', 'autistic', 'spectrum', 'of', 'disorders', 'at', 'the', 'same', 'time', 'an', 'austrian', 'scientist', 'dr', 'hans', 'asperger', 'described', 'a', 'different', 'form', 'of', 'autism', 'that', 'became', 'known', 'as', 'asperger', 's', 'syndrome', 'but', 'the', 'widespread', 'recognition', 'of', 'asperger', 's', 'work', 'was', 'delayed', 'by', 'world', 'war', 'ii', 'in', 'germany', 'and', 'by', 'the', 'fact', 'that', 'his', 'seminal', 'paper', 'wasn', 't', 'translated', 'into', 'english', 'for', 'almost', 'five', 'zero', 'years', 'the', 'majority', 'of', 'his', 'work', 'wasn', 't', 'widely', 'read', 'until', 'one', 'nine', 'nine', 'seven', 'thus', 'these', 'two', 'conditions', 'were', 'described', 'and', 'are', 'today', 'listed', 'in', 'the', 'diagnostic', 'and', 'statistical', 'manual', 'of', 'mental', 'disorders', 'dsm', 'iv', 'tr', 'fourth', 'edition', 'text', 'revision', 'one', 'as', 'two', 'of', 'the', 'five', 'pervasive', 'developmental', 'disorders', 'pdd', 'more', 'often', 'referred', 'to', 'today', 'as', 'autism', 'spectrum', 'disorders', 'asd', 'all', 'of', 'these', 'conditions', 'are', 'characterized', 'by', 'varying', 'degrees', 'of', 'difference', 'in', 'communication', 'skills', 'social', 'interactions', 'and', 'restricted', 'repetitive', 'and', 'stereotyped', 'patterns', 'of', 'behavior', 'few', 'clinicians', 'today', 'solely', 'use', 'the', 'dsm', 'iv', 'criteria', 'for', 'determining', 'a', 'diagnosis', 'of', 'autism', 'which', 'are', 'based', 'on', 'the', 'absence', 'or', 'delay', 'of', 'certain', 'developmental', 'milestones', 'many', 'clinicians', 'instead', 'use', 'an', 'alternate', 'means', 'or', 'a', 'combination', 'thereof', 'to', 'more', 'accurately', 'determine', 'a', 'diagnosis', 'terminology', 'when', 'referring', 'to', 'someone', 'diagnosed', 'with', 'autism', 'the', 'term', 'autistic', 'is', 'often', 'used', 'however', 'the', 'term', 'person', 'with', 'autism', 'can', 'be', 'used', 'instead', 'this', 'is', 'referred', 'to', 'as', 'person', 'first', 'terminology', 'the', 'autistic', 'community', 'generally', 'prefers', 'the', 'term', 'autistic', 'for', 'reasons', 'that', 'are', 'fairly', 'controversial', 'this', 'article', 'uses', 'the', 'term', 'autistic', 'see', 'talk', 'page', 'characteristics', 'dr', 'leo', 'kanner', 'introduced', 'the', 'label', 'early', 'infantile', 'autism', 'in', 'one', 'nine', 'four', 'three', 'there', 'is', 'a', 'great', 'diversity', 'in', 'the', 'skills', 'and', 'behaviors', 'of', 'individuals', 'diagnosed', 'as', 'autistic', 'and', 'physicians', 'will', 'often', 'arrive', 'at', 'different', 'conclusions', 'about', 'the', 'appropriate', 'diagnosis', 'much', 'of', 'this', 'is', 'due', 'to', 'the', 'sensory', 'system', 'of', 'an', 'autistic', 'which', 'is', 'quite', 'different', 'from', 'the', 'sensory', 'system', 'of', 'other', 'people', 'since', 'certain', 'stimulations', 'can', 'affect', 'an', 'autistic', 'differently', 'than', 'a', 'non', 'autistic', 'and', 'the', 'degree', 'to', 'which', 'the', 'sensory', 'system', 'is', 'affected', 'varies', 'wildly', 'from', 'one', 'autistic', 'person', 'to', 'another', 'nevertheless', 'professionals', 'within', 'pediatric', 'care', 'and', 'development', 'often', 'look', 'for', 'early', 'indicators', 'of', 'autism', 'in', 'order', 'to', 'initiate', 'treatment', 'as', 'early', 'as', 'possible', 'however', 'some', 'people', 'do', 'not', 'believe', 'in', 'treatment', 'for', 'autism', 'either', 'because', 'they', 'do', 'not', 'believe', 'autism', 'is', 'a', 'disorder', 'or', 'because', 'they', 'believe', 'treatment', 'can', 'do', 'more', 'harm', 'than', 'good', 'social', 'development', 'typically', 'developing', 'infants', 'are', 'social', 'beings', 'early', 'in', 'life', 'they', 'do', 'such', 'things', 'as', 'gaze', 'at', 'people', 'turn', 'toward', 'voices', 'grasp', 'a', 'finger', 'and', 'even', 'smile', 'in', 'contrast', 'most', 'autistic', 'children', 'prefer', 'objects', 'to', 'faces', 'and', 'seem', 'to', 'have', 'tremendous', 'difficulty', 'learning', 'to', 'engage', 'in', 'the', 'give', 'and', 'take', 'of', 'everyday', 'human', 'interaction', 'even', 'in', 'the', 'first', 'few', 'months', 'of', 'life', 'many', 'seem', 'indifferent', 'to', 'other', 'people', 'because', 'they', 'avoid', 'eye', 'contact', 'and', 'do', 'not', 'interact', 'with', 'them', 'as', 'often', 'as', 'non', 'autistic', 'children', 'children', 'with', 'autism', 'often', 'appear', 'to', 'prefer', 'being', 'alone', 'to', 'the', 'company', 'of', 'others', 'and', 'may', 'passively', 'accept', 'such', 'things', 'as', 'hugs', 'and', 'cuddling', 'without', 'reciprocating', 'or', 'resist', 'attention', 'altogether', 'later', 'they', 'seldom', 'seek', 'comfort', 'from', 'others', 'or', 'respond', 'to', 'parents', 'displays', 'of', 'anger', 'or', 'affection', 'in', 'a', 'typical', 'way', 'research', 'has', 'suggested', 'that', 'although', 'autistic', 'children', 'are', 'attached', 'to', 'their', 'parents', 'their', 'expression', 'of', 'this', 'attachment', 'is', 'unusual', 'and', 'difficult', 'to', 'interpret', 'parents', 'who', 'looked', 'forward', 'to', 'the', 'joys', 'of', 'cuddling', 'teaching', 'and', 'playing', 'with', 'their', 'child', 'may', 'feel', 'crushed', 'by', 'this', 'lack', 'of', 'expected', 'attachment', 'behavior', 'children', 'with', 'autism', 'appear', 'to', 'lack', 'theory', 'of', 'mind', 'the', 'ability', 'to', 'see', 'things', 'from', 'another', 'person', 's', 'perspective', 'a', 'behavior', 'cited', 'as', 'exclusive', 'to', 'human', 'beings', 'above', 'the', 'age', 'of', 'five', 'and', 'possibly', 'other', 'higher', 'primates', 'such', 'as', 'adult', 'gorillas', 'chimpanzees', 'and', 'bonobos', 'typical', 'five', 'year', 'olds', 'can', 'develop', 'insights', 'into', 'other', 'people', 's', 'different', 'knowledge', 'feelings', 'and', 'intentions', 'interpretations', 'based', 'upon', 'social', 'cues', 'e', 'g', 'gestures', 'facial', 'expressions', 'an', 'individual', 'with', 'autism', 'seems', 'to', 'lack', 'these', 'interpretation', 'skills', 'an', 'inability', 'that', 'leaves', 'them', 'unable', 'to', 'predict', 'or', 'understand', 'other', 'people', 's', 'actions', 'the', 'social', 'alienation', 'of', 'autistic', 'and', 'asperger', 's', 'people', 'is', 'so', 'intense', 'from', 'childhood', 'that', 'many', 'of', 'them', 'have', 'imaginary', 'friends', 'as', 'companionship', 'however', 'having', 'an', 'imaginary', 'friend', 'is', 'not', 'necessarily', 'a', 'sign', 'of', 'autism', 'and', 'also', 'occurs', 'in', 'non', 'autistic', 'children', 'although', 'not', 'universal', 'it', 'is', 'common', 'for', 'autistic', 'people', 'to', 'not', 'regulate', 'their', 'behavior', 'this', 'can', 'take', 'the', 'form', 'of', 'crying', 'or', 'verbal', 'outbursts', 'that', 'may', 'seem', 'out', 'of', 'proportion', 'to', 'the', 'situation', 'individuals', 'with', 'autism', 'generally', 'prefer', 'consistent', 'routines', 'and', 'environments', 'they', 'may', 'react', 'negatively', 'to', 'changes', 'in', 'them', 'it', 'is', 'not', 'uncommon', 'for', 'these', 'individuals', 'to', 'exhibit', 'aggression', 'increased', 'levels', 'of', 'self', 'stimulatory', 'behavior', 'self', 'injury', 'or', 'extensive', 'withdrawal', 'in', 'overwhelming', 'situations', 'sensory', 'system', 'a', 'key', 'indicator', 'to', 'clinicians', 'making', 'a', 'proper', 'assessment', 'for', 'autism', 'would', 'include', 'looking', 'for', 'symptoms', 'much', 'like', 'those', 'found', 'in', 'sensory', 'integration', 'dysfunction', 'children', 'will', 'exhibit', 'problems', 'coping', 'with', 'the', 'normal', 'sensory', 'input', 'indicators', 'of', 'this', 'disorder', 'include', 'oversensitivity', 'or', 'underreactivity', 'to', 'touch', 'movement', 'sights', 'or', 'sounds', 'physical', 'clumsiness', 'or', 'carelessness', 'poor', 'body', 'awareness', 'a', 'tendency', 'to', 'be', 'easily', 'distracted', 'impulsive', 'physical', 'or', 'verbal', 'behavior', 'an', 'activity', 'level', 'that', 'is', 'unusually', 'high', 'or', 'low', 'not', 'unwinding', 'or', 'calming', 'oneself', 'difficulty', 'learning', 'new', 'movements', 'difficulty', 'in', 'making', 'transitions', 'from', 'one', 'situation', 'to', 'another', 'social', 'and', 'or', 'emotional', 'problems', 'delays', 'in', 'speech', 'language', 'or', 'motor', 'skills', 'specific', 'learning', 'difficulties', 'delays', 'in', 'academic', 'achievement', 'one', 'common', 'example', 'is', 'an', 'individual', 'with', 'autism', 'hearing', 'a', 'person', 'with', 'autism', 'may', 'have', 'trouble', 'hearing', 'certain', 'people', 'while', 'other', 'people', 'are', 'louder', 'than', 'usual', 'or', 'the', 'person', 'with', 'autism', 'may', 'be', 'unable', 'to', 'filter', 'out', 'sounds', 'in', 'certain', 'situations', 'such', 'as', 'in', 'a', 'large', 'crowd', 'of', 'people', 'see', 'cocktail', 'party', 'effect', 'however', 'this', 'is', 'perhaps', 'the', 'part', 'of', 'the', 'autism', 'that', 'tends', 'to', 'vary', 'the', 'most', 'from', 'person', 'to', 'person', 'so', 'these', 'examples', 'may', 'not', 'apply', 'to', 'every', 'autistic', 'it', 'should', 'be', 'noted', 'that', 'sensory', 'difficulties', 'although', 'reportedly', 'common', 'in', 'autistics', 'are', 'not', 'part', 'of', 'the', 'dsm', 'iv', 'diagnostic', 'criteria', 'for', 'autistic', 'disorder', 'communication', 'difficulties', 'by', 'age', 'three', 'typical', 'children', 'have', 'passed', 'predictable', 'language', 'learning', 'milestones', 'one', 'of', 'the', 'earliest', 'is', 'babbling', 'by', 'the', 'first', 'birthday', 'a', 'typical', 'toddler', 'says', 'words', 'turns', 'when', 'he', 'or', 'she', 'hears', 'his', 'or', 'her', 'name', 'points', 'when', 'he', 'or', 'she', 'wants', 'a', 'toy', 'and', 'when', 'offered', 'something', 'distasteful', 'makes', 'it', 'clear', 'that', 'the', 'answer', 'is', 'no', 'speech', 'development', 'in', 'people', 'with', 'autism', 'takes', 'different', 'paths', 'some', 'remain', 'mute', 'throughout', 'their', 'lives', 'while', 'being', 'fully', 'literate', 'and', 'able', 'to', 'communicate', 'in', 'other', 'ways', 'images', 'sign', 'language', 'and', 'typing', 'are', 'far', 'more', 'natural', 'to', 'them', 'some', 'infants', 'who', 'later', 'show', 'signs', 'of', 'autism', 'coo', 'and', 'babble', 'during', 'the', 'first', 'few', 'months', 'of', 'life', 'but', 'stop', 'soon', 'afterwards', 'others', 'may', 'be', 'delayed', 'developing', 'language', 'as', 'late', 'as', 'the', 'teenage', 'years', 'still', 'inability', 'to', 'speak', 'does', 'not', 'mean', 'that', 'people', 'with', 'autism', 'are', 'unintelligent', 'or', 'unaware', 'once', 'given', 'appropriate', 'accommodations', 'many', 'will', 'happily', 'converse', 'for', 'hours', 'and', 'can', 'often', 'be', 'found', 'in', 'online', 'chat', 'rooms', 'discussion', 'boards', 'or', 'websites', 'and', 'even', 'using', 'communication', 'devices', 'at', 'autism', 'community', 'social', 'events', 'such', 'as', 'autreat', 'those', 'who', 'do', 'speak', 'often', 'use', 'language', 'in', 'unusual', 'ways', 'retaining', 'features', 'of', 'earlier', 'stages', 'of', 'language', 'development', 'for', 'long', 'periods', 'or', 'throughout', 'their', 'lives', 'some', 'speak', 'only', 'single', 'words', 'while', 'others', 'repeat', 'the', 'same', 'phrase', 'over', 'and', 'over', 'some', 'repeat', 'what', 'they', 'hear', 'a', 'condition', 'called', 'echolalia', 'sing', 'song', 'repetitions', 'in', 'particular', 'are', 'a', 'calming', 'joyous', 'activity', 'that', 'many', 'autistic', 'adults', 'engage', 'in', 'many', 'people', 'with', 'autism', 'have', 'a', 'strong', 'tonal', 'sense', 'and', 'can', 'often', 'understand', 'spoken', 'language', 'some', 'children', 'may', 'exhibit', 'only', 'slight', 'delays', 'in', 'language', 'or', 'even', 'seem', 'to', 'have', 'precocious', 'language', 'and', 'unusually', 'large', 'vocabularies', 'but', 'have', 'great', 'difficulty', 'in', 'sustaining', 'typical', 'conversations', 'the', 'give', 'and', 'take', 'of', 'non', 'autistic', 'conversation', 'is', 'hard', 'for', 'them', 'although', 'they', 'often', 'carry', 'on', 'a', 'monologue', 'on', 'a', 'favorite', 'subject', 'giving', 'no', 'one', 'else', 'an', 'opportunity', 'to', 'comment', 'when', 'given', 'the', 'chance', 'to', 'converse', 'with', 'other', 'autistics', 'they', 'comfortably', 'do', 'so', 'in', 'parallel', 'monologue', 'taking', 'turns', 'expressing', 'views', 'and', 'information', 'just', 'as', 'neurotypicals', 'people', 'without', 'autism', 'have', 'trouble', 'understanding', 'autistic', 'body', 'languages', 'vocal', 'tones', 'or', 'phraseology', 'people', 'with', 'autism', 'similarly', 'have', 'trouble', 'with', 'such', 'things', 'in', 'people', 'without', 'autism', 'in', 'particular', 'autistic', 'language', 'abilities', 'tend', 'to', 'be', 'highly', 'literal', 'people', 'without', 'autism', 'often', 'inappropriately', 'attribute', 'hidden', 'meaning', 'to', 'what', 'people', 'with', 'autism', 'say', 'or', 'expect', 'the', 'person', 'with', 'autism', 'to', 'sense', 'such', 'unstated', 'meaning', 'in', 'their', 'own', 'words', 'the', 'body', 'language', 'of', 'people', 'with', 'autism', 'can', 'be', 'difficult', 'for', 'other', 'people', 'to', 'understand', 'facial', 'expressions', 'movements', 'and', 'gestures', 'may', 'be', 'easily', 'understood', 'by', 'some', 'other', 'people', 'with', 'autism', 'but', 'do', 'not', 'match', 'those', 'used', 'by', 'other', 'people', 'also', 'their', 'tone', 'of', 'voice', 'has', 'a', 'much', 'more', 'subtle', 'inflection', 'in', 'reflecting', 'their', 'feelings', 'and', 'the', 'auditory', 'system', 'of', 'a', 'person', 'without', 'autism', 'often', 'cannot', 'sense', 'the', 'fluctuations', 'what', 'seems', 'to', 'non', 'autistic', 'people', 'like', 'a', 'high', 'pitched', 'sing', 'song', 'or', 'flat', 'robot', 'like', 'voice', 'is', 'common', 'in', 'autistic', 'children', 'some', 'autistic', 'children', 'with', 'relatively', 'good', 'language', 'skills', 'speak', 'like', 'little', 'adults', 'rather', 'than', 'communicating', 'at', 'their', 'current', 'age', 'level', 'which', 'is', 'one', 'of', 'the', 'things', 'that', 'can', 'lead', 'to', 'problems', 'since', 'non', 'autistic', 'people', 'are', 'often', 'unfamiliar', 'with', 'the', 'autistic', 'body', 'language', 'and', 'since', 'autistic', 'natural', 'language', 'may', 'not', 'tend', 'towards', 'speech', 'autistic', 'people', 'often', 'struggle', 'to', 'let', 'other', 'people', 'know', 'what', 'they', 'need', 'as', 'anybody', 'might', 'do', 'in', 'such', 'a', 'situation', 'they', 'may', 'scream', 'in', 'frustration', 'or', 'resort', 'to', 'grabbing', 'what', 'they', 'want', 'while', 'waiting', 'for', 'non', 'autistic', 'people', 'to', 'learn', 'to', 'communicate', 'with', 'them', 'people', 'with', 'autism', 'do', 'whatever', 'they', 'can', 'to', 'get', 'through', 'to', 'them', 'communication', 'difficulties', 'may', 'contribute', 'to', 'autistic', 'people', 'becoming', 'socially', 'anxious', 'or', 'depressed', 'repetitive', 'behaviors', 'although', 'people', 'with', 'autism', 'usually', 'appear', 'physically', 'normal', 'and', 'have', 'good', 'muscle', 'control', 'unusual', 'repetitive', 'motions', 'known', 'as', 'self', 'stimulation', 'or', 'stimming', 'may', 'set', 'them', 'apart', 'these', 'behaviors', 'might', 'be', 'extreme', 'and', 'highly', 'apparent', 'or', 'more', 'subtle', 'some', 'children', 'and', 'older', 'individuals', 'spend', 'a', 'lot', 'of', 'time', 'repeatedly', 'flapping', 'their', 'arms', 'or', 'wiggling', 'their', 'toes', 'others', 'suddenly', 'freeze', 'in', 'position', 'as', 'children', 'they', 'might', 'spend', 'hours', 'lining', 'up', 'their', 'cars', 'and', 'trains', 'in', 'a', 'certain', 'way', 'not', 'using', 'them', 'for', 'pretend', 'play', 'if', 'someone', 'accidentally', 'moves', 'one', 'of', 'these', 'toys', 'the', 'child', 'may', 'be', 'tremendously', 'upset', 'autistic', 'children', 'often', 'need', 'and', 'demand', 'absolute', 'consistency', 'in', 'their', 'environment', 'a', 'slight', 'change', 'in', 'any', 'routine', 'in', 'mealtimes', 'dressing', 'taking', 'a', 'bath', 'or', 'going', 'to', 'school', 'at', 'a', 'certain', 'time', 'and', 'by', 'the', 'same', 'route', 'can', 'be', 'extremely', 'disturbing', 'people', 'with', 'autism', 'sometimes', 'have', 'a', 'persistent', 'intense', 'preoccupation', 'for', 'example', 'the', 'child', 'might', 'be', 'obsessed', 'with', 'learning', 'all', 'about', 'vacuum', 'cleaners', 'train', 'schedules', 'or', 'lighthouses', 'often', 'they', 'show', 'great', 'interest', 'in', 'different', 'languages', 'numbers', 'symbols', 'or', 'science', 'topics', 'repetitive', 'behaviors', 'can', 'also', 'extend', 'into', 'the', 'spoken', 'word', 'as', 'well', 'perseveration', 'of', 'a', 'single', 'word', 'or', 'phrase', 'even', 'for', 'a', 'specific', 'number', 'of', 'times', 'can', 'also', 'become', 'a', 'part', 'of', 'the', 'child', 's', 'daily', 'routine', 'effects', 'in', 'education', 'children', 'with', 'autism', 'are', 'affected', 'with', 'these', 'symptoms', 'every', 'day', 'these', 'unusual', 'characteristics', 'set', 'them', 'apart', 'from', 'the', 'everyday', 'normal', 'student', 'because', 'they', 'have', 'trouble', 'understanding', 'people', 's', 'thoughts', 'and', 'feelings', 'they', 'have', 'trouble', 'understanding', 'what', 'their', 'teacher', 'may', 'be', 'telling', 'them', 'they', 'do', 'not', 'understand', 'that', 'facial', 'expressions', 'and', 'vocal', 'variations', 'hold', 'meanings', 'and', 'may', 'misinterpret', 'what', 'emotion', 'their', 'instructor', 'is', 'displaying', 'this', 'inability', 'to', 'fully', 'decipher', 'the', 'world', 'around', 'them', 'makes', 'education', 'stressful', 'teachers', 'need', 'to', 'be', 'aware', 'of', 'a', 'student', 's', 'disorder', 'so', 'that', 'they', 'are', 'able', 'to', 'help', 'the', 'student', 'get', 'the', 'best', 'out', 'of', 'the', 'lessons', 'being', 'taught', 'some', 'students', 'learn', 'better', 'with', 'visual', 'aids', 'as', 'they', 'are', 'better', 'able', 'to', 'understand', 'material', 'presented', 'this', 'way', 'because', 'of', 'this', 'many', 'teachers', 'create', 'visual', 'schedules', 'for', 'their', 'autistic', 'students', 'this', 'allows', 'the', 'student', 'to', 'know', 'what', 'is', 'going', 'on', 'throughout', 'the', 'day', 'so', 'they', 'know', 'what', 'to', 'prepare', 'for', 'and', 'what', 'activity', 'they', 'will', 'be', 'doing', 'next', 'some', 'autistic', 'children', 'have', 'trouble', 'going', 'from', 'one', 'activity', 'to', 'the', 'next', 'so', 'this', 'visual', 'schedule', 'can', 'help', 'to', 'reduce', 'stress', 'research', 'has', 'shown', 'that', 'working', 'in', 'pairs', 'may', 'be', 'beneficial', 'to', 'autistic', 'children', 'autistic', 'students', 'have', 'problems', 'in', 'schools', 'not', 'only', 'with', 'language', 'and', 'communication', 'but', 'with', 'socialization', 'as', 'well', 'they', 'feel', 'self', 'conscious', 'about', 'themselves', 'and', 'many', 'feel', 'that', 'they', 'will', 'always', 'be', 'outcasts', 'by', 'allowing', 'them', 'to', 'work', 'with', 'peers', 'they', 'can', 'make', 'friends', 'which', 'in', 'turn', 'can', 'help', 'them', 'cope', 'with', 'the', 'problems', 'that', 'arise', 'by', 'doing', 'so', 'they', 'can', 'become', 'more', 'integrated', 'into', 'the', 'mainstream', 'environment', 'of', 'the', 'classroom', 'a', 'teacher', 's', 'aide', 'can', 'also', 'be', 'useful', 'to', 'the', 'student', 'the', 'aide', 'is', 'able', 'to', 'give', 'more', 'elaborate', 'directions', 'that', 'the', 'teacher', 'may', 'not', 'have', 'time', 'to', 'explain', 'to', 'the', 'autistic', 'child', 'the', 'aide', 'can', 'also', 'facilitate', 'the', 'autistic', 'child', 'in', 'such', 'a', 'way', 'as', 'to', 'allow', 'them', 'to', 'stay', 'at', 'a', 'similar', 'level', 'to', 'the', 'rest', 'of', 'the', 'class', 'this', 'allows', 'a', 'partially', 'one', 'on', 'one', 'lesson', 'structure', 'so', 'that', 'the', 'child', 'is', 'still', 'able', 'to', 'stay', 'in', 'a', 'normal', 'classroom', 'but', 'be', 'given', 'the', 'extra', 'help', 'that', 'they', 'need', 'there', 'are', 'many', 'different', 'techniques', 'that', 'teachers', 'can', 'use', 'to', 'assist', 'their', 'students', 'a', 'teacher', 'needs', 'to', 'become', 'familiar', 'with', 'the', 'child', 's', 'disorder', 'to', 'know', 'what', 'will', 'work', 'best', 'with', 'that', 'particular', 'child', 'every', 'child', 'is', 'going', 'to', 'be', 'different', 'and', 'teachers', 'have', 'to', 'be', 'able', 'to', 'adjust', 'with', 'every', 'one', 'of', 'them', 'students', 'with', 'autism', 'spectrum', 'disorders', 'typically', 'have', 'high', 'levels', 'of', 'anxiety', 'and', 'stress', 'particularly', 'in', 'social', 'environments', 'like', 'school', 'if', 'a', 'student', 'exhibits', 'aggressive', 'or', 'explosive', 'behavior', 'it', 'is', 'important', 'for', 'educational', 'teams', 'to', 'recognize', 'the', 'impact', 'of', 'stress', 'and', 'anxiety', 'preparing', 'students', 'for', 'new', 'situations', 'by', 'writing', 'social', 'stories', 'can', 'lower', 'anxiety', 'teaching', 'social', 'and', 'emotional', 'concepts', 'using', 'systematic', 'teaching', 'approaches', 'such', 'as', 'the', 'incredible', 'five', 'point', 'scale', 'or', 'other', 'cognitive', 'behavioral', 'strategies', 'can', 'increase', 'a', 'student', 's', 'ability', 'to', 'control', 'excessive', 'behavioral', 'reactions', 'dsm', 'definition', 'autism', 'is', 'defined', 'in', 'section', 'two', 'nine', 'nine', 'zero', 'zero', 'of', 'the', 'diagnostic', 'and', 'statistical', 'manual', 'of', 'mental', 'disorders', 'dsm', 'iv', 'as', 'a', 'total', 'of', 'six', 'or', 'more', 'items', 'from', 'one', 'two', 'and', 'three', 'with', 'at', 'least', 'two', 'from', 'one', 'and', 'one', 'each', 'from', 'two', 'and', 'three', 'qualitative', 'impairment', 'in', 'social', 'interaction', 'as', 'manifested', 'by', 'at', 'least', 'two', 'of', 'the', 'following', 'marked', 'impairment', 'in', 'the', 'use', 'of', 'multiple', 'nonverbal', 'behaviors', 'such', 'as', 'eye', 'to', 'eye', 'gaze', 'facial', 'expression', 'body', 'postures', 'and', 'gestures', 'to', 'regulate', 'social', 'interaction', 'failure', 'to', 'develop', 'peer', 'relationships', 'appropriate', 'to', 'developmental', 'level', 'a', 'lack', 'of', 'spontaneous', 'seeking', 'to', 'share', 'enjoyment', 'interests', 'or', 'achievements', 'with', 'other', 'people', 'e', 'g', 'by', 'a', 'lack', 'of', 'showing', 'bringing', 'or', 'pointing', 'out', 'objects', 'of', 'interest', 'lack', 'of', 'social', 'or', 'emotional'], tags=[0])]\n"
     ]
    }
   ],
   "source": [
    "# Create the tagged document needed for Doc2Vec\n",
    "def create_tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "train_data = list(create_tagged_document(data))\n",
    "\n",
    "print(train_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:47:21,277 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-03-19 19:47:21,280 : INFO : collecting all words and their counts\n",
      "2019-03-19 19:47:21,282 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-03-19 19:47:24,860 : INFO : collected 253854 word types and 1701 unique tags from a corpus of 1701 examples and 17005207 words\n",
      "2019-03-19 19:47:24,863 : INFO : Loading a fresh vocabulary\n",
      "2019-03-19 19:47:25,481 : INFO : effective_min_count=2 retains 135335 unique words (53% of original 253854, drops 118519)\n",
      "2019-03-19 19:47:25,483 : INFO : effective_min_count=2 leaves 16886688 word corpus (99% of original 17005207, drops 118519)\n",
      "2019-03-19 19:47:25,981 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2019-03-19 19:47:25,998 : INFO : sample=0.001 downsamples 37 most-common words\n",
      "2019-03-19 19:47:25,999 : INFO : downsampling leaves estimated 12689806 word corpus (75.1% of prior 16886688)\n",
      "2019-03-19 19:47:26,537 : INFO : estimated required memory for 135335 words and 50 dimensions: 122141700 bytes\n",
      "2019-03-19 19:47:26,538 : INFO : resetting layer weights\n",
      "2019-03-19 19:47:28,318 : INFO : training model with 3 workers on 135335 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-19 19:47:29,325 : INFO : EPOCH 1 - PROGRESS: at 7.41% examples, 932823 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:30,329 : INFO : EPOCH 1 - PROGRESS: at 14.76% examples, 929096 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:31,339 : INFO : EPOCH 1 - PROGRESS: at 22.16% examples, 929543 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:32,341 : INFO : EPOCH 1 - PROGRESS: at 29.69% examples, 938399 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:47:33,346 : INFO : EPOCH 1 - PROGRESS: at 37.57% examples, 951141 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:34,349 : INFO : EPOCH 1 - PROGRESS: at 45.56% examples, 961622 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:35,350 : INFO : EPOCH 1 - PROGRESS: at 53.44% examples, 967884 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:47:36,358 : INFO : EPOCH 1 - PROGRESS: at 61.20% examples, 969721 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:47:37,358 : INFO : EPOCH 1 - PROGRESS: at 69.25% examples, 975589 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:47:38,362 : INFO : EPOCH 1 - PROGRESS: at 77.48% examples, 980717 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:39,366 : INFO : EPOCH 1 - PROGRESS: at 85.66% examples, 985327 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:40,378 : INFO : EPOCH 1 - PROGRESS: at 93.83% examples, 988362 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:41,131 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:47:41,135 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:47:41,139 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:47:41,140 : INFO : EPOCH - 1 : training on 17005207 raw words (12693597 effective words) took 12.8s, 990423 effective words/s\n",
      "2019-03-19 19:47:42,151 : INFO : EPOCH 2 - PROGRESS: at 7.94% examples, 999973 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:47:43,152 : INFO : EPOCH 2 - PROGRESS: at 15.76% examples, 994142 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:44,161 : INFO : EPOCH 2 - PROGRESS: at 23.46% examples, 986225 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:45,173 : INFO : EPOCH 2 - PROGRESS: at 31.57% examples, 996776 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:46,173 : INFO : EPOCH 2 - PROGRESS: at 39.15% examples, 991013 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:47:47,181 : INFO : EPOCH 2 - PROGRESS: at 46.85% examples, 988275 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:47:48,200 : INFO : EPOCH 2 - PROGRESS: at 54.44% examples, 982904 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:49,205 : INFO : EPOCH 2 - PROGRESS: at 62.14% examples, 981859 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-19 19:47:50,214 : INFO : EPOCH 2 - PROGRESS: at 70.31% examples, 987013 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:47:51,223 : INFO : EPOCH 2 - PROGRESS: at 78.13% examples, 985322 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:52,241 : INFO : EPOCH 2 - PROGRESS: at 86.01% examples, 984890 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:53,248 : INFO : EPOCH 2 - PROGRESS: at 93.83% examples, 984720 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:47:54,020 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:47:54,029 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:47:54,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:47:54,035 : INFO : EPOCH - 2 : training on 17005207 raw words (12691625 effective words) took 12.9s, 985050 effective words/s\n",
      "2019-03-19 19:47:55,049 : INFO : EPOCH 3 - PROGRESS: at 7.35% examples, 917291 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:56,053 : INFO : EPOCH 3 - PROGRESS: at 14.93% examples, 935784 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:57,063 : INFO : EPOCH 3 - PROGRESS: at 22.57% examples, 943735 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:58,065 : INFO : EPOCH 3 - PROGRESS: at 30.04% examples, 947110 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:47:59,072 : INFO : EPOCH 3 - PROGRESS: at 37.62% examples, 950399 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:48:00,081 : INFO : EPOCH 3 - PROGRESS: at 45.15% examples, 950384 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:01,084 : INFO : EPOCH 3 - PROGRESS: at 53.26% examples, 962146 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:02,088 : INFO : EPOCH 3 - PROGRESS: at 61.38% examples, 970436 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:03,090 : INFO : EPOCH 3 - PROGRESS: at 68.84% examples, 967853 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:04,092 : INFO : EPOCH 3 - PROGRESS: at 76.60% examples, 968457 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:48:05,095 : INFO : EPOCH 3 - PROGRESS: at 84.60% examples, 971731 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:06,098 : INFO : EPOCH 3 - PROGRESS: at 92.36% examples, 972515 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:07,033 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:48:07,037 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:48:07,039 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:48:07,040 : INFO : EPOCH - 3 : training on 17005207 raw words (12691105 effective words) took 13.0s, 976149 effective words/s\n",
      "2019-03-19 19:48:08,049 : INFO : EPOCH 4 - PROGRESS: at 7.70% examples, 966112 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:09,052 : INFO : EPOCH 4 - PROGRESS: at 15.81% examples, 994353 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:10,069 : INFO : EPOCH 4 - PROGRESS: at 23.63% examples, 989360 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:11,075 : INFO : EPOCH 4 - PROGRESS: at 31.10% examples, 979712 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:12,079 : INFO : EPOCH 4 - PROGRESS: at 38.92% examples, 982963 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:13,092 : INFO : EPOCH 4 - PROGRESS: at 47.03% examples, 989329 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:14,092 : INFO : EPOCH 4 - PROGRESS: at 55.03% examples, 993715 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:15,094 : INFO : EPOCH 4 - PROGRESS: at 62.96% examples, 995517 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:16,102 : INFO : EPOCH 4 - PROGRESS: at 70.66% examples, 992776 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:17,105 : INFO : EPOCH 4 - PROGRESS: at 78.78% examples, 994693 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:18,119 : INFO : EPOCH 4 - PROGRESS: at 86.60% examples, 993071 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:19,123 : INFO : EPOCH 4 - PROGRESS: at 94.71% examples, 995429 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:19,778 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:48:19,782 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:48:19,785 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:48:19,786 : INFO : EPOCH - 4 : training on 17005207 raw words (12690368 effective words) took 12.7s, 995968 effective words/s\n",
      "2019-03-19 19:48:20,808 : INFO : EPOCH 5 - PROGRESS: at 7.94% examples, 996973 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:21,811 : INFO : EPOCH 5 - PROGRESS: at 15.87% examples, 998766 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:22,814 : INFO : EPOCH 5 - PROGRESS: at 23.93% examples, 1006582 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:23,815 : INFO : EPOCH 5 - PROGRESS: at 31.10% examples, 984593 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:48:24,819 : INFO : EPOCH 5 - PROGRESS: at 38.62% examples, 979220 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:25,823 : INFO : EPOCH 5 - PROGRESS: at 46.27% examples, 977635 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:26,827 : INFO : EPOCH 5 - PROGRESS: at 54.20% examples, 982350 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:27,836 : INFO : EPOCH 5 - PROGRESS: at 62.26% examples, 986378 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:28,841 : INFO : EPOCH 5 - PROGRESS: at 70.08% examples, 986574 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:29,850 : INFO : EPOCH 5 - PROGRESS: at 77.95% examples, 985629 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:30,856 : INFO : EPOCH 5 - PROGRESS: at 85.24% examples, 979365 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:31,860 : INFO : EPOCH 5 - PROGRESS: at 93.12% examples, 980732 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:32,730 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:48:32,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:48:32,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:48:32,743 : INFO : EPOCH - 5 : training on 17005207 raw words (12690214 effective words) took 12.9s, 980722 effective words/s\n",
      "2019-03-19 19:48:33,766 : INFO : EPOCH 6 - PROGRESS: at 7.58% examples, 943717 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:34,780 : INFO : EPOCH 6 - PROGRESS: at 15.52% examples, 966562 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:35,780 : INFO : EPOCH 6 - PROGRESS: at 22.99% examples, 960632 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:36,781 : INFO : EPOCH 6 - PROGRESS: at 30.63% examples, 965249 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:48:37,783 : INFO : EPOCH 6 - PROGRESS: at 38.68% examples, 977740 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:48:38,786 : INFO : EPOCH 6 - PROGRESS: at 46.62% examples, 982864 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:39,790 : INFO : EPOCH 6 - PROGRESS: at 54.26% examples, 981550 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:48:40,793 : INFO : EPOCH 6 - PROGRESS: at 62.20% examples, 984622 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:41,800 : INFO : EPOCH 6 - PROGRESS: at 69.96% examples, 984002 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:48:42,815 : INFO : EPOCH 6 - PROGRESS: at 77.90% examples, 983546 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:48:43,816 : INFO : EPOCH 6 - PROGRESS: at 85.89% examples, 986101 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:44,822 : INFO : EPOCH 6 - PROGRESS: at 93.94% examples, 988297 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:48:45,573 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:48:45,575 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:48:45,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:48:45,581 : INFO : EPOCH - 6 : training on 17005207 raw words (12692466 effective words) took 12.8s, 989348 effective words/s\n",
      "2019-03-19 19:48:46,597 : INFO : EPOCH 7 - PROGRESS: at 7.64% examples, 952586 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:47,599 : INFO : EPOCH 7 - PROGRESS: at 15.70% examples, 984049 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:48,607 : INFO : EPOCH 7 - PROGRESS: at 23.93% examples, 1002590 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:49,608 : INFO : EPOCH 7 - PROGRESS: at 31.80% examples, 1003976 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:50,620 : INFO : EPOCH 7 - PROGRESS: at 39.62% examples, 1000659 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:51,620 : INFO : EPOCH 7 - PROGRESS: at 47.50% examples, 1001129 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:52,627 : INFO : EPOCH 7 - PROGRESS: at 55.32% examples, 999833 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:53,631 : INFO : EPOCH 7 - PROGRESS: at 63.14% examples, 998736 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:48:54,635 : INFO : EPOCH 7 - PROGRESS: at 71.08% examples, 999408 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:48:55,640 : INFO : EPOCH 7 - PROGRESS: at 79.37% examples, 1002602 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:56,646 : INFO : EPOCH 7 - PROGRESS: at 87.13% examples, 1000304 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:57,650 : INFO : EPOCH 7 - PROGRESS: at 94.83% examples, 997831 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:48:58,288 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:48:58,297 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:48:58,302 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:48:58,303 : INFO : EPOCH - 7 : training on 17005207 raw words (12690850 effective words) took 12.7s, 997879 effective words/s\n",
      "2019-03-19 19:48:59,309 : INFO : EPOCH 8 - PROGRESS: at 7.88% examples, 991955 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:00,312 : INFO : EPOCH 8 - PROGRESS: at 15.46% examples, 974125 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:01,313 : INFO : EPOCH 8 - PROGRESS: at 23.52% examples, 990502 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:02,329 : INFO : EPOCH 8 - PROGRESS: at 31.63% examples, 999059 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:03,334 : INFO : EPOCH 8 - PROGRESS: at 39.74% examples, 1005449 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-19 19:49:04,338 : INFO : EPOCH 8 - PROGRESS: at 47.09% examples, 993355 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:05,342 : INFO : EPOCH 8 - PROGRESS: at 54.91% examples, 993535 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:06,345 : INFO : EPOCH 8 - PROGRESS: at 63.14% examples, 999834 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:07,346 : INFO : EPOCH 8 - PROGRESS: at 70.84% examples, 997426 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:08,350 : INFO : EPOCH 8 - PROGRESS: at 78.89% examples, 997908 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:09,351 : INFO : EPOCH 8 - PROGRESS: at 86.77% examples, 997919 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:10,353 : INFO : EPOCH 8 - PROGRESS: at 94.71% examples, 998179 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:10,997 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:49:11,007 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:49:11,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:49:11,012 : INFO : EPOCH - 8 : training on 17005207 raw words (12691311 effective words) took 12.7s, 998990 effective words/s\n",
      "2019-03-19 19:49:12,029 : INFO : EPOCH 9 - PROGRESS: at 7.76% examples, 966163 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:13,034 : INFO : EPOCH 9 - PROGRESS: at 15.58% examples, 974744 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:14,037 : INFO : EPOCH 9 - PROGRESS: at 23.05% examples, 965340 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-19 19:49:15,040 : INFO : EPOCH 9 - PROGRESS: at 30.92% examples, 975832 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:16,056 : INFO : EPOCH 9 - PROGRESS: at 38.74% examples, 977427 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:17,069 : INFO : EPOCH 9 - PROGRESS: at 46.80% examples, 983497 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:49:18,084 : INFO : EPOCH 9 - PROGRESS: at 54.61% examples, 983610 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:19,091 : INFO : EPOCH 9 - PROGRESS: at 62.55% examples, 985949 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:20,092 : INFO : EPOCH 9 - PROGRESS: at 70.55% examples, 989189 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:49:21,092 : INFO : EPOCH 9 - PROGRESS: at 78.54% examples, 990285 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:49:22,092 : INFO : EPOCH 9 - PROGRESS: at 86.65% examples, 993751 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:23,094 : INFO : EPOCH 9 - PROGRESS: at 94.65% examples, 994896 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:23,752 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:49:23,758 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:49:23,760 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:49:23,761 : INFO : EPOCH - 9 : training on 17005207 raw words (12691713 effective words) took 12.7s, 995792 effective words/s\n",
      "2019-03-19 19:49:24,786 : INFO : EPOCH 10 - PROGRESS: at 7.88% examples, 980873 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:25,791 : INFO : EPOCH 10 - PROGRESS: at 15.70% examples, 981957 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:26,794 : INFO : EPOCH 10 - PROGRESS: at 23.52% examples, 985129 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-19 19:49:27,804 : INFO : EPOCH 10 - PROGRESS: at 31.51% examples, 992525 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:28,810 : INFO : EPOCH 10 - PROGRESS: at 39.39% examples, 994302 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:29,815 : INFO : EPOCH 10 - PROGRESS: at 47.09% examples, 991416 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:30,825 : INFO : EPOCH 10 - PROGRESS: at 54.67% examples, 986976 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:31,829 : INFO : EPOCH 10 - PROGRESS: at 62.67% examples, 990125 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:32,829 : INFO : EPOCH 10 - PROGRESS: at 70.84% examples, 995542 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:33,831 : INFO : EPOCH 10 - PROGRESS: at 78.95% examples, 997305 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:34,833 : INFO : EPOCH 10 - PROGRESS: at 87.07% examples, 999925 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:35,839 : INFO : EPOCH 10 - PROGRESS: at 94.89% examples, 998523 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:36,526 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:49:36,529 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:49:36,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:49:36,533 : INFO : EPOCH - 10 : training on 17005207 raw words (12692534 effective words) took 12.8s, 994667 effective words/s\n",
      "2019-03-19 19:49:37,540 : INFO : EPOCH 11 - PROGRESS: at 8.11% examples, 1019569 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:38,558 : INFO : EPOCH 11 - PROGRESS: at 16.05% examples, 1002673 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:39,570 : INFO : EPOCH 11 - PROGRESS: at 24.16% examples, 1008583 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:40,571 : INFO : EPOCH 11 - PROGRESS: at 31.69% examples, 997488 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:41,578 : INFO : EPOCH 11 - PROGRESS: at 39.33% examples, 991859 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:49:42,581 : INFO : EPOCH 11 - PROGRESS: at 46.50% examples, 978634 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:43,583 : INFO : EPOCH 11 - PROGRESS: at 54.67% examples, 987540 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:44,585 : INFO : EPOCH 11 - PROGRESS: at 62.85% examples, 993638 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:45,589 : INFO : EPOCH 11 - PROGRESS: at 71.08% examples, 999119 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:46,590 : INFO : EPOCH 11 - PROGRESS: at 79.31% examples, 1001957 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:47,592 : INFO : EPOCH 11 - PROGRESS: at 87.30% examples, 1002868 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:49:48,604 : INFO : EPOCH 11 - PROGRESS: at 95.24% examples, 1002034 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:49,184 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:49:49,188 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:49:49,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:49:49,190 : INFO : EPOCH - 11 : training on 17005207 raw words (12691134 effective words) took 12.7s, 1003003 effective words/s\n",
      "2019-03-19 19:49:50,193 : INFO : EPOCH 12 - PROGRESS: at 7.88% examples, 993020 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:51,197 : INFO : EPOCH 12 - PROGRESS: at 15.76% examples, 992831 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:49:52,210 : INFO : EPOCH 12 - PROGRESS: at 24.16% examples, 1014271 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:53,215 : INFO : EPOCH 12 - PROGRESS: at 32.16% examples, 1015776 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:54,217 : INFO : EPOCH 12 - PROGRESS: at 40.27% examples, 1019140 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:55,217 : INFO : EPOCH 12 - PROGRESS: at 48.09% examples, 1015542 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:56,220 : INFO : EPOCH 12 - PROGRESS: at 55.79% examples, 1010540 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:57,223 : INFO : EPOCH 12 - PROGRESS: at 63.84% examples, 1011991 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:58,226 : INFO : EPOCH 12 - PROGRESS: at 72.08% examples, 1015306 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:49:59,230 : INFO : EPOCH 12 - PROGRESS: at 80.31% examples, 1016325 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:00,237 : INFO : EPOCH 12 - PROGRESS: at 88.54% examples, 1018263 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:01,241 : INFO : EPOCH 12 - PROGRESS: at 96.53% examples, 1017284 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:01,681 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:50:01,683 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:50:01,688 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:50:01,689 : INFO : EPOCH - 12 : training on 17005207 raw words (12692466 effective words) took 12.5s, 1015774 effective words/s\n",
      "2019-03-19 19:50:02,703 : INFO : EPOCH 13 - PROGRESS: at 7.82% examples, 975498 words/s, in_qsize 3, out_qsize 2\n",
      "2019-03-19 19:50:03,708 : INFO : EPOCH 13 - PROGRESS: at 15.76% examples, 986842 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:04,709 : INFO : EPOCH 13 - PROGRESS: at 23.69% examples, 994598 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:50:05,713 : INFO : EPOCH 13 - PROGRESS: at 31.69% examples, 1000909 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:06,717 : INFO : EPOCH 13 - PROGRESS: at 39.33% examples, 995149 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:50:07,731 : INFO : EPOCH 13 - PROGRESS: at 46.97% examples, 989443 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:08,739 : INFO : EPOCH 13 - PROGRESS: at 54.91% examples, 991757 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:09,746 : INFO : EPOCH 13 - PROGRESS: at 63.02% examples, 995811 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:50:10,748 : INFO : EPOCH 13 - PROGRESS: at 70.84% examples, 995453 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:11,751 : INFO : EPOCH 13 - PROGRESS: at 78.60% examples, 992621 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:12,754 : INFO : EPOCH 13 - PROGRESS: at 86.54% examples, 993609 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:13,764 : INFO : EPOCH 13 - PROGRESS: at 94.59% examples, 994807 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:14,460 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:50:14,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:50:14,464 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:50:14,465 : INFO : EPOCH - 13 : training on 17005207 raw words (12691895 effective words) took 12.8s, 993629 effective words/s\n",
      "2019-03-19 19:50:15,479 : INFO : EPOCH 14 - PROGRESS: at 7.94% examples, 997912 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:50:16,483 : INFO : EPOCH 14 - PROGRESS: at 16.05% examples, 1009728 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:17,492 : INFO : EPOCH 14 - PROGRESS: at 23.93% examples, 1004531 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:50:18,494 : INFO : EPOCH 14 - PROGRESS: at 31.45% examples, 993899 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:19,498 : INFO : EPOCH 14 - PROGRESS: at 38.74% examples, 980818 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:20,512 : INFO : EPOCH 14 - PROGRESS: at 46.91% examples, 988646 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:21,516 : INFO : EPOCH 14 - PROGRESS: at 55.09% examples, 995955 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:50:22,516 : INFO : EPOCH 14 - PROGRESS: at 63.08% examples, 998517 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:23,520 : INFO : EPOCH 14 - PROGRESS: at 71.08% examples, 1000199 words/s, in_qsize 4, out_qsize 1\n",
      "2019-03-19 19:50:24,529 : INFO : EPOCH 14 - PROGRESS: at 79.01% examples, 998401 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:25,531 : INFO : EPOCH 14 - PROGRESS: at 86.71% examples, 996272 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:26,531 : INFO : EPOCH 14 - PROGRESS: at 94.77% examples, 997980 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:27,171 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:50:27,180 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:50:27,183 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:50:27,184 : INFO : EPOCH - 14 : training on 17005207 raw words (12690519 effective words) took 12.7s, 998650 effective words/s\n",
      "2019-03-19 19:50:28,198 : INFO : EPOCH 15 - PROGRESS: at 7.82% examples, 983792 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:29,199 : INFO : EPOCH 15 - PROGRESS: at 15.93% examples, 1004600 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:30,202 : INFO : EPOCH 15 - PROGRESS: at 24.16% examples, 1017770 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:31,203 : INFO : EPOCH 15 - PROGRESS: at 32.28% examples, 1023257 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:32,209 : INFO : EPOCH 15 - PROGRESS: at 40.51% examples, 1027332 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:33,219 : INFO : EPOCH 15 - PROGRESS: at 48.56% examples, 1025667 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:34,225 : INFO : EPOCH 15 - PROGRESS: at 56.61% examples, 1025294 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:35,239 : INFO : EPOCH 15 - PROGRESS: at 64.79% examples, 1025099 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:50:36,253 : INFO : EPOCH 15 - PROGRESS: at 72.90% examples, 1024435 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:50:37,255 : INFO : EPOCH 15 - PROGRESS: at 81.01% examples, 1023022 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:50:38,255 : INFO : EPOCH 15 - PROGRESS: at 89.01% examples, 1022358 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:39,256 : INFO : EPOCH 15 - PROGRESS: at 97.06% examples, 1021721 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:39,602 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:50:39,607 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:50:39,609 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:50:39,610 : INFO : EPOCH - 15 : training on 17005207 raw words (12692596 effective words) took 12.4s, 1022420 effective words/s\n",
      "2019-03-19 19:50:40,631 : INFO : EPOCH 16 - PROGRESS: at 7.76% examples, 969540 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:41,631 : INFO : EPOCH 16 - PROGRESS: at 15.93% examples, 1001367 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:42,633 : INFO : EPOCH 16 - PROGRESS: at 24.16% examples, 1015807 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:43,640 : INFO : EPOCH 16 - PROGRESS: at 31.86% examples, 1006992 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:44,646 : INFO : EPOCH 16 - PROGRESS: at 39.74% examples, 1005659 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:45,651 : INFO : EPOCH 16 - PROGRESS: at 47.74% examples, 1007064 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:50:46,656 : INFO : EPOCH 16 - PROGRESS: at 55.38% examples, 1002015 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:47,658 : INFO : EPOCH 16 - PROGRESS: at 63.08% examples, 998898 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:48,671 : INFO : EPOCH 16 - PROGRESS: at 71.08% examples, 999586 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:49,679 : INFO : EPOCH 16 - PROGRESS: at 78.95% examples, 997308 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:50,684 : INFO : EPOCH 16 - PROGRESS: at 86.89% examples, 997477 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:51,696 : INFO : EPOCH 16 - PROGRESS: at 94.83% examples, 997078 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:52,330 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:50:52,332 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:50:52,337 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:50:52,338 : INFO : EPOCH - 16 : training on 17005207 raw words (12691972 effective words) took 12.7s, 998078 effective words/s\n",
      "2019-03-19 19:50:53,354 : INFO : EPOCH 17 - PROGRESS: at 7.88% examples, 989401 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:50:54,363 : INFO : EPOCH 17 - PROGRESS: at 14.81% examples, 928564 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:55,372 : INFO : EPOCH 17 - PROGRESS: at 22.69% examples, 949458 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:56,378 : INFO : EPOCH 17 - PROGRESS: at 30.34% examples, 955796 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:57,382 : INFO : EPOCH 17 - PROGRESS: at 38.27% examples, 966882 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:58,390 : INFO : EPOCH 17 - PROGRESS: at 46.27% examples, 974139 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:50:59,402 : INFO : EPOCH 17 - PROGRESS: at 54.26% examples, 979283 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:00,405 : INFO : EPOCH 17 - PROGRESS: at 62.20% examples, 982674 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:01,414 : INFO : EPOCH 17 - PROGRESS: at 70.19% examples, 985476 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:51:02,423 : INFO : EPOCH 17 - PROGRESS: at 78.37% examples, 988337 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:51:03,426 : INFO : EPOCH 17 - PROGRESS: at 86.54% examples, 992347 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:51:04,430 : INFO : EPOCH 17 - PROGRESS: at 94.77% examples, 995987 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:51:05,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:51:05,081 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:51:05,090 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:51:05,091 : INFO : EPOCH - 17 : training on 17005207 raw words (12691580 effective words) took 12.7s, 996111 effective words/s\n",
      "2019-03-19 19:51:06,105 : INFO : EPOCH 18 - PROGRESS: at 7.76% examples, 974728 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:51:07,119 : INFO : EPOCH 18 - PROGRESS: at 15.52% examples, 971410 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:08,121 : INFO : EPOCH 18 - PROGRESS: at 23.40% examples, 980684 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:09,123 : INFO : EPOCH 18 - PROGRESS: at 31.16% examples, 983936 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:10,127 : INFO : EPOCH 18 - PROGRESS: at 38.68% examples, 978795 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:11,129 : INFO : EPOCH 18 - PROGRESS: at 45.74% examples, 965165 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:12,144 : INFO : EPOCH 18 - PROGRESS: at 53.91% examples, 974270 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:13,146 : INFO : EPOCH 18 - PROGRESS: at 61.73% examples, 976640 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:51:14,153 : INFO : EPOCH 18 - PROGRESS: at 69.84% examples, 981911 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:15,161 : INFO : EPOCH 18 - PROGRESS: at 78.01% examples, 985178 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:16,179 : INFO : EPOCH 18 - PROGRESS: at 85.95% examples, 985524 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:17,182 : INFO : EPOCH 18 - PROGRESS: at 94.06% examples, 988681 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:17,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:51:17,920 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:51:17,923 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:51:17,924 : INFO : EPOCH - 18 : training on 17005207 raw words (12691901 effective words) took 12.8s, 989847 effective words/s\n",
      "2019-03-19 19:51:18,944 : INFO : EPOCH 19 - PROGRESS: at 7.47% examples, 931600 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:19,952 : INFO : EPOCH 19 - PROGRESS: at 14.40% examples, 901325 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:20,953 : INFO : EPOCH 19 - PROGRESS: at 21.87% examples, 915821 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:51:21,967 : INFO : EPOCH 19 - PROGRESS: at 29.75% examples, 936468 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:22,975 : INFO : EPOCH 19 - PROGRESS: at 37.51% examples, 946052 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:23,982 : INFO : EPOCH 19 - PROGRESS: at 45.15% examples, 949476 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:24,984 : INFO : EPOCH 19 - PROGRESS: at 53.38% examples, 963693 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:25,998 : INFO : EPOCH 19 - PROGRESS: at 61.38% examples, 968811 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:27,008 : INFO : EPOCH 19 - PROGRESS: at 69.31% examples, 972089 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:28,014 : INFO : EPOCH 19 - PROGRESS: at 77.25% examples, 973722 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:29,023 : INFO : EPOCH 19 - PROGRESS: at 85.19% examples, 975584 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:30,029 : INFO : EPOCH 19 - PROGRESS: at 93.12% examples, 977707 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:30,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:51:30,913 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:51:30,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:51:30,919 : INFO : EPOCH - 19 : training on 17005207 raw words (12692177 effective words) took 13.0s, 977454 effective words/s\n",
      "2019-03-19 19:51:31,925 : INFO : EPOCH 20 - PROGRESS: at 7.35% examples, 924428 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:32,936 : INFO : EPOCH 20 - PROGRESS: at 15.40% examples, 966378 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:33,937 : INFO : EPOCH 20 - PROGRESS: at 23.16% examples, 972696 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:34,951 : INFO : EPOCH 20 - PROGRESS: at 30.63% examples, 965502 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:35,954 : INFO : EPOCH 20 - PROGRESS: at 38.45% examples, 971905 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:36,956 : INFO : EPOCH 20 - PROGRESS: at 46.44% examples, 979314 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:37,963 : INFO : EPOCH 20 - PROGRESS: at 54.44% examples, 984499 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:51:38,982 : INFO : EPOCH 20 - PROGRESS: at 62.43% examples, 986233 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:51:39,997 : INFO : EPOCH 20 - PROGRESS: at 70.25% examples, 985285 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:41,004 : INFO : EPOCH 20 - PROGRESS: at 78.31% examples, 986978 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:42,014 : INFO : EPOCH 20 - PROGRESS: at 86.01% examples, 985058 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:43,016 : INFO : EPOCH 20 - PROGRESS: at 94.00% examples, 987167 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:43,752 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:51:43,754 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:51:43,758 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:51:43,759 : INFO : EPOCH - 20 : training on 17005207 raw words (12693870 effective words) took 12.8s, 988891 effective words/s\n",
      "2019-03-19 19:51:44,775 : INFO : EPOCH 21 - PROGRESS: at 7.88% examples, 980931 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:45,784 : INFO : EPOCH 21 - PROGRESS: at 15.70% examples, 980858 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:46,792 : INFO : EPOCH 21 - PROGRESS: at 23.69% examples, 990770 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:47,798 : INFO : EPOCH 21 - PROGRESS: at 31.75% examples, 999477 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:48,798 : INFO : EPOCH 21 - PROGRESS: at 39.92% examples, 1008148 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:49,807 : INFO : EPOCH 21 - PROGRESS: at 47.74% examples, 1004651 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:50,808 : INFO : EPOCH 21 - PROGRESS: at 55.38% examples, 1000606 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:51,809 : INFO : EPOCH 21 - PROGRESS: at 62.90% examples, 995104 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:51:52,812 : INFO : EPOCH 21 - PROGRESS: at 71.02% examples, 998875 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:53,813 : INFO : EPOCH 21 - PROGRESS: at 78.89% examples, 997374 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:54,818 : INFO : EPOCH 21 - PROGRESS: at 86.95% examples, 998821 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:55,819 : INFO : EPOCH 21 - PROGRESS: at 94.71% examples, 997327 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:56,454 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:51:56,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:51:56,466 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:51:56,467 : INFO : EPOCH - 21 : training on 17005207 raw words (12691593 effective words) took 12.7s, 998972 effective words/s\n",
      "2019-03-19 19:51:57,477 : INFO : EPOCH 22 - PROGRESS: at 7.94% examples, 994379 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:58,478 : INFO : EPOCH 22 - PROGRESS: at 15.58% examples, 979695 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:51:59,484 : INFO : EPOCH 22 - PROGRESS: at 23.40% examples, 982455 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:00,493 : INFO : EPOCH 22 - PROGRESS: at 31.39% examples, 990942 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:01,497 : INFO : EPOCH 22 - PROGRESS: at 38.33% examples, 969419 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:02,498 : INFO : EPOCH 22 - PROGRESS: at 46.03% examples, 971284 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:03,510 : INFO : EPOCH 22 - PROGRESS: at 53.97% examples, 975715 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:04,513 : INFO : EPOCH 22 - PROGRESS: at 61.90% examples, 979866 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:05,513 : INFO : EPOCH 22 - PROGRESS: at 70.02% examples, 985336 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:06,517 : INFO : EPOCH 22 - PROGRESS: at 78.13% examples, 988073 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:07,518 : INFO : EPOCH 22 - PROGRESS: at 86.13% examples, 990263 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:08,528 : INFO : EPOCH 22 - PROGRESS: at 94.18% examples, 991888 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:09,239 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:52:09,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:52:09,245 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:52:09,246 : INFO : EPOCH - 22 : training on 17005207 raw words (12692656 effective words) took 12.8s, 993509 effective words/s\n",
      "2019-03-19 19:52:10,255 : INFO : EPOCH 23 - PROGRESS: at 7.88% examples, 987063 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:11,262 : INFO : EPOCH 23 - PROGRESS: at 15.64% examples, 980467 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:12,273 : INFO : EPOCH 23 - PROGRESS: at 23.57% examples, 986932 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:13,289 : INFO : EPOCH 23 - PROGRESS: at 31.51% examples, 990731 words/s, in_qsize 4, out_qsize 1\n",
      "2019-03-19 19:52:14,289 : INFO : EPOCH 23 - PROGRESS: at 39.09% examples, 986457 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:15,297 : INFO : EPOCH 23 - PROGRESS: at 46.62% examples, 980677 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:16,309 : INFO : EPOCH 23 - PROGRESS: at 54.73% examples, 986826 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:52:17,324 : INFO : EPOCH 23 - PROGRESS: at 63.02% examples, 993384 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:18,331 : INFO : EPOCH 23 - PROGRESS: at 71.13% examples, 996956 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:19,333 : INFO : EPOCH 23 - PROGRESS: at 79.54% examples, 1002072 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:20,336 : INFO : EPOCH 23 - PROGRESS: at 87.65% examples, 1004142 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:21,348 : INFO : EPOCH 23 - PROGRESS: at 95.88% examples, 1006404 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:21,840 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:52:21,850 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:52:21,853 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:52:21,854 : INFO : EPOCH - 23 : training on 17005207 raw words (12692307 effective words) took 12.6s, 1006950 effective words/s\n",
      "2019-03-19 19:52:22,874 : INFO : EPOCH 24 - PROGRESS: at 7.58% examples, 941223 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:23,878 : INFO : EPOCH 24 - PROGRESS: at 15.40% examples, 962707 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:24,886 : INFO : EPOCH 24 - PROGRESS: at 23.28% examples, 973295 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:25,894 : INFO : EPOCH 24 - PROGRESS: at 31.57% examples, 993788 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:26,899 : INFO : EPOCH 24 - PROGRESS: at 39.39% examples, 993875 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:27,907 : INFO : EPOCH 24 - PROGRESS: at 47.03% examples, 989231 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:28,911 : INFO : EPOCH 24 - PROGRESS: at 54.61% examples, 985982 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:29,915 : INFO : EPOCH 24 - PROGRESS: at 62.61% examples, 989150 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:30,925 : INFO : EPOCH 24 - PROGRESS: at 70.66% examples, 991981 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:31,940 : INFO : EPOCH 24 - PROGRESS: at 78.60% examples, 990607 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:32,944 : INFO : EPOCH 24 - PROGRESS: at 86.54% examples, 991602 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:33,946 : INFO : EPOCH 24 - PROGRESS: at 94.06% examples, 988055 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:34,675 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:52:34,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:52:34,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:52:34,687 : INFO : EPOCH - 24 : training on 17005207 raw words (12692439 effective words) took 12.8s, 989330 effective words/s\n",
      "2019-03-19 19:52:35,701 : INFO : EPOCH 25 - PROGRESS: at 7.64% examples, 960166 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:36,703 : INFO : EPOCH 25 - PROGRESS: at 15.17% examples, 954612 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:37,707 : INFO : EPOCH 25 - PROGRESS: at 23.05% examples, 968739 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:38,713 : INFO : EPOCH 25 - PROGRESS: at 30.63% examples, 968483 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:39,715 : INFO : EPOCH 25 - PROGRESS: at 38.15% examples, 967118 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:40,720 : INFO : EPOCH 25 - PROGRESS: at 46.15% examples, 974740 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:41,726 : INFO : EPOCH 25 - PROGRESS: at 54.20% examples, 981677 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:42,735 : INFO : EPOCH 25 - PROGRESS: at 62.20% examples, 984905 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:43,737 : INFO : EPOCH 25 - PROGRESS: at 70.14% examples, 987297 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:44,751 : INFO : EPOCH 25 - PROGRESS: at 78.25% examples, 988816 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:45,760 : INFO : EPOCH 25 - PROGRESS: at 86.36% examples, 991572 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:46,767 : INFO : EPOCH 25 - PROGRESS: at 94.30% examples, 992108 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:47,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:52:47,502 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:52:47,504 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:52:47,505 : INFO : EPOCH - 25 : training on 17005207 raw words (12692940 effective words) took 12.8s, 990999 effective words/s\n",
      "2019-03-19 19:52:48,518 : INFO : EPOCH 26 - PROGRESS: at 7.88% examples, 985935 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:49,519 : INFO : EPOCH 26 - PROGRESS: at 15.81% examples, 994353 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:50,521 : INFO : EPOCH 26 - PROGRESS: at 24.04% examples, 1011499 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:51,523 : INFO : EPOCH 26 - PROGRESS: at 32.22% examples, 1019786 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:52,524 : INFO : EPOCH 26 - PROGRESS: at 39.92% examples, 1012440 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:53,528 : INFO : EPOCH 26 - PROGRESS: at 47.44% examples, 1002886 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:54,546 : INFO : EPOCH 26 - PROGRESS: at 55.50% examples, 1004089 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:55,556 : INFO : EPOCH 26 - PROGRESS: at 63.73% examples, 1008012 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:52:56,557 : INFO : EPOCH 26 - PROGRESS: at 71.66% examples, 1007965 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-19 19:52:57,569 : INFO : EPOCH 26 - PROGRESS: at 79.95% examples, 1009514 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:58,572 : INFO : EPOCH 26 - PROGRESS: at 87.83% examples, 1008262 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:52:59,581 : INFO : EPOCH 26 - PROGRESS: at 95.41% examples, 1003664 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:53:00,145 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:53:00,151 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:53:00,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:53:00,157 : INFO : EPOCH - 26 : training on 17005207 raw words (12691285 effective words) took 12.6s, 1003459 effective words/s\n",
      "2019-03-19 19:53:01,170 : INFO : EPOCH 27 - PROGRESS: at 7.94% examples, 996677 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:02,175 : INFO : EPOCH 27 - PROGRESS: at 15.87% examples, 997444 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:53:03,180 : INFO : EPOCH 27 - PROGRESS: at 23.99% examples, 1007539 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:04,186 : INFO : EPOCH 27 - PROGRESS: at 32.04% examples, 1011957 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:05,188 : INFO : EPOCH 27 - PROGRESS: at 40.09% examples, 1014970 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:06,202 : INFO : EPOCH 27 - PROGRESS: at 48.21% examples, 1015900 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:07,206 : INFO : EPOCH 27 - PROGRESS: at 56.08% examples, 1013943 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:08,210 : INFO : EPOCH 27 - PROGRESS: at 64.26% examples, 1016441 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:53:09,222 : INFO : EPOCH 27 - PROGRESS: at 72.49% examples, 1018495 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:10,239 : INFO : EPOCH 27 - PROGRESS: at 80.66% examples, 1017072 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:53:11,239 : INFO : EPOCH 27 - PROGRESS: at 88.36% examples, 1013500 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:12,242 : INFO : EPOCH 27 - PROGRESS: at 96.36% examples, 1013052 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:12,668 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:53:12,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:53:12,685 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:53:12,686 : INFO : EPOCH - 27 : training on 17005207 raw words (12691561 effective words) took 12.5s, 1013637 effective words/s\n",
      "2019-03-19 19:53:13,699 : INFO : EPOCH 28 - PROGRESS: at 7.88% examples, 991578 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:53:14,702 : INFO : EPOCH 28 - PROGRESS: at 15.70% examples, 988134 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:15,715 : INFO : EPOCH 28 - PROGRESS: at 23.81% examples, 998977 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:16,723 : INFO : EPOCH 28 - PROGRESS: at 31.57% examples, 995769 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:17,727 : INFO : EPOCH 28 - PROGRESS: at 39.21% examples, 990986 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:18,732 : INFO : EPOCH 28 - PROGRESS: at 47.03% examples, 991172 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:53:19,733 : INFO : EPOCH 28 - PROGRESS: at 55.09% examples, 996294 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:20,739 : INFO : EPOCH 28 - PROGRESS: at 63.14% examples, 999062 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:21,751 : INFO : EPOCH 28 - PROGRESS: at 71.43% examples, 1003923 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:22,752 : INFO : EPOCH 28 - PROGRESS: at 79.42% examples, 1003221 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:23,752 : INFO : EPOCH 28 - PROGRESS: at 87.36% examples, 1003398 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:24,754 : INFO : EPOCH 28 - PROGRESS: at 94.89% examples, 999091 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:25,369 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:53:25,377 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:53:25,379 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:53:25,380 : INFO : EPOCH - 28 : training on 17005207 raw words (12691018 effective words) took 12.7s, 1000631 effective words/s\n",
      "2019-03-19 19:53:26,386 : INFO : EPOCH 29 - PROGRESS: at 7.76% examples, 975343 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:53:27,386 : INFO : EPOCH 29 - PROGRESS: at 15.70% examples, 989214 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:28,400 : INFO : EPOCH 29 - PROGRESS: at 23.81% examples, 999285 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:29,403 : INFO : EPOCH 29 - PROGRESS: at 31.69% examples, 1001317 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:53:30,416 : INFO : EPOCH 29 - PROGRESS: at 39.62% examples, 1001352 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:31,428 : INFO : EPOCH 29 - PROGRESS: at 47.74% examples, 1004611 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:32,429 : INFO : EPOCH 29 - PROGRESS: at 55.44% examples, 1001538 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:33,429 : INFO : EPOCH 29 - PROGRESS: at 63.26% examples, 1000676 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:34,447 : INFO : EPOCH 29 - PROGRESS: at 71.13% examples, 998993 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:35,453 : INFO : EPOCH 29 - PROGRESS: at 79.37% examples, 1001268 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:36,460 : INFO : EPOCH 29 - PROGRESS: at 87.18% examples, 999622 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:37,466 : INFO : EPOCH 29 - PROGRESS: at 95.18% examples, 1000187 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:38,118 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:53:38,126 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:53:38,130 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:53:38,132 : INFO : EPOCH - 29 : training on 17005207 raw words (12691507 effective words) took 12.7s, 995577 effective words/s\n",
      "2019-03-19 19:53:39,149 : INFO : EPOCH 30 - PROGRESS: at 7.70% examples, 963289 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:40,151 : INFO : EPOCH 30 - PROGRESS: at 15.29% examples, 960410 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:41,153 : INFO : EPOCH 30 - PROGRESS: at 23.40% examples, 983256 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:42,154 : INFO : EPOCH 30 - PROGRESS: at 31.45% examples, 995624 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:43,158 : INFO : EPOCH 30 - PROGRESS: at 38.92% examples, 986614 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:44,163 : INFO : EPOCH 30 - PROGRESS: at 46.74% examples, 987318 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:45,172 : INFO : EPOCH 30 - PROGRESS: at 54.97% examples, 995049 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:46,183 : INFO : EPOCH 30 - PROGRESS: at 63.32% examples, 1002075 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:47,185 : INFO : EPOCH 30 - PROGRESS: at 71.49% examples, 1005965 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:48,188 : INFO : EPOCH 30 - PROGRESS: at 79.48% examples, 1004919 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:49,190 : INFO : EPOCH 30 - PROGRESS: at 87.65% examples, 1007510 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:50,200 : INFO : EPOCH 30 - PROGRESS: at 95.83% examples, 1009077 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:50,712 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:53:50,714 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:53:50,717 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:53:50,718 : INFO : EPOCH - 30 : training on 17005207 raw words (12691356 effective words) took 12.6s, 1009122 effective words/s\n",
      "2019-03-19 19:53:51,732 : INFO : EPOCH 31 - PROGRESS: at 8.11% examples, 1017913 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:52,732 : INFO : EPOCH 31 - PROGRESS: at 16.17% examples, 1017791 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:53,732 : INFO : EPOCH 31 - PROGRESS: at 24.22% examples, 1020179 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:53:54,736 : INFO : EPOCH 31 - PROGRESS: at 32.04% examples, 1014826 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:53:55,737 : INFO : EPOCH 31 - PROGRESS: at 39.62% examples, 1005477 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:56,740 : INFO : EPOCH 31 - PROGRESS: at 47.33% examples, 1000991 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:57,742 : INFO : EPOCH 31 - PROGRESS: at 55.44% examples, 1005780 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:58,748 : INFO : EPOCH 31 - PROGRESS: at 63.37% examples, 1005473 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:53:59,755 : INFO : EPOCH 31 - PROGRESS: at 71.37% examples, 1006025 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:00,770 : INFO : EPOCH 31 - PROGRESS: at 79.48% examples, 1005237 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:01,775 : INFO : EPOCH 31 - PROGRESS: at 87.42% examples, 1004873 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:54:02,785 : INFO : EPOCH 31 - PROGRESS: at 95.71% examples, 1007788 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:03,323 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:54:03,325 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:54:03,328 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:54:03,329 : INFO : EPOCH - 31 : training on 17005207 raw words (12691724 effective words) took 12.6s, 1007008 effective words/s\n",
      "2019-03-19 19:54:04,335 : INFO : EPOCH 32 - PROGRESS: at 7.70% examples, 970262 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:05,337 : INFO : EPOCH 32 - PROGRESS: at 15.70% examples, 989275 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:54:06,342 : INFO : EPOCH 32 - PROGRESS: at 23.57% examples, 992149 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:54:07,346 : INFO : EPOCH 32 - PROGRESS: at 31.39% examples, 993775 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:08,352 : INFO : EPOCH 32 - PROGRESS: at 38.98% examples, 987457 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:09,358 : INFO : EPOCH 32 - PROGRESS: at 46.80% examples, 988174 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:10,358 : INFO : EPOCH 32 - PROGRESS: at 54.73% examples, 991551 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:11,358 : INFO : EPOCH 32 - PROGRESS: at 62.43% examples, 990051 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:12,363 : INFO : EPOCH 32 - PROGRESS: at 70.25% examples, 989872 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:13,377 : INFO : EPOCH 32 - PROGRESS: at 78.37% examples, 991042 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:14,381 : INFO : EPOCH 32 - PROGRESS: at 86.30% examples, 991999 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:54:15,381 : INFO : EPOCH 32 - PROGRESS: at 94.47% examples, 995522 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:16,139 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:54:16,154 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:54:16,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:54:16,157 : INFO : EPOCH - 32 : training on 17005207 raw words (12689806 effective words) took 12.8s, 989638 effective words/s\n",
      "2019-03-19 19:54:17,164 : INFO : EPOCH 33 - PROGRESS: at 8.05% examples, 1013467 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:18,165 : INFO : EPOCH 33 - PROGRESS: at 15.93% examples, 1004301 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:19,168 : INFO : EPOCH 33 - PROGRESS: at 23.57% examples, 992620 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:20,173 : INFO : EPOCH 33 - PROGRESS: at 31.10% examples, 984225 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:54:21,174 : INFO : EPOCH 33 - PROGRESS: at 38.45% examples, 975236 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:22,180 : INFO : EPOCH 33 - PROGRESS: at 46.21% examples, 976451 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:23,189 : INFO : EPOCH 33 - PROGRESS: at 54.09% examples, 979483 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:24,190 : INFO : EPOCH 33 - PROGRESS: at 62.32% examples, 987939 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:25,193 : INFO : EPOCH 33 - PROGRESS: at 70.49% examples, 993133 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:54:26,212 : INFO : EPOCH 33 - PROGRESS: at 78.13% examples, 987583 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:27,216 : INFO : EPOCH 33 - PROGRESS: at 86.24% examples, 990870 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:28,218 : INFO : EPOCH 33 - PROGRESS: at 94.24% examples, 992502 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:28,928 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:54:28,932 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:54:28,935 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:54:28,936 : INFO : EPOCH - 33 : training on 17005207 raw words (12691693 effective words) took 12.8s, 993480 effective words/s\n",
      "2019-03-19 19:54:29,961 : INFO : EPOCH 34 - PROGRESS: at 7.76% examples, 964166 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:30,984 : INFO : EPOCH 34 - PROGRESS: at 15.52% examples, 961939 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:31,994 : INFO : EPOCH 34 - PROGRESS: at 23.28% examples, 966745 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:32,998 : INFO : EPOCH 34 - PROGRESS: at 30.75% examples, 963560 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:34,001 : INFO : EPOCH 34 - PROGRESS: at 38.57% examples, 970104 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:35,002 : INFO : EPOCH 34 - PROGRESS: at 46.38% examples, 974304 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:36,004 : INFO : EPOCH 34 - PROGRESS: at 53.91% examples, 972132 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:37,007 : INFO : EPOCH 34 - PROGRESS: at 61.67% examples, 973741 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:38,017 : INFO : EPOCH 34 - PROGRESS: at 69.72% examples, 978110 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:39,025 : INFO : EPOCH 34 - PROGRESS: at 78.01% examples, 983300 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:40,036 : INFO : EPOCH 34 - PROGRESS: at 85.89% examples, 983677 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:41,041 : INFO : EPOCH 34 - PROGRESS: at 93.59% examples, 982486 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:41,818 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:54:41,826 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:54:41,830 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:54:41,832 : INFO : EPOCH - 34 : training on 17005207 raw words (12690193 effective words) took 12.9s, 984924 effective words/s\n",
      "2019-03-19 19:54:42,845 : INFO : EPOCH 35 - PROGRESS: at 8.00% examples, 1005946 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:43,849 : INFO : EPOCH 35 - PROGRESS: at 15.87% examples, 998802 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:44,857 : INFO : EPOCH 35 - PROGRESS: at 23.81% examples, 1000075 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:45,861 : INFO : EPOCH 35 - PROGRESS: at 31.80% examples, 1005059 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:46,862 : INFO : EPOCH 35 - PROGRESS: at 39.80% examples, 1008447 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:47,862 : INFO : EPOCH 35 - PROGRESS: at 47.50% examples, 1003925 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:48,862 : INFO : EPOCH 35 - PROGRESS: at 55.26% examples, 1002037 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:54:49,864 : INFO : EPOCH 35 - PROGRESS: at 63.08% examples, 1000851 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:50,865 : INFO : EPOCH 35 - PROGRESS: at 71.08% examples, 1002628 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:51,874 : INFO : EPOCH 35 - PROGRESS: at 79.01% examples, 1000549 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:52,876 : INFO : EPOCH 35 - PROGRESS: at 87.30% examples, 1004950 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:53,878 : INFO : EPOCH 35 - PROGRESS: at 95.30% examples, 1005372 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:54,432 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:54:54,435 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:54:54,439 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:54:54,440 : INFO : EPOCH - 35 : training on 17005207 raw words (12692559 effective words) took 12.6s, 1007504 effective words/s\n",
      "2019-03-19 19:54:55,468 : INFO : EPOCH 36 - PROGRESS: at 7.82% examples, 963265 words/s, in_qsize 4, out_qsize 1\n",
      "2019-03-19 19:54:56,468 : INFO : EPOCH 36 - PROGRESS: at 15.81% examples, 986748 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:57,470 : INFO : EPOCH 36 - PROGRESS: at 23.87% examples, 999100 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:54:58,479 : INFO : EPOCH 36 - PROGRESS: at 32.04% examples, 1008701 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-19 19:54:59,485 : INFO : EPOCH 36 - PROGRESS: at 39.74% examples, 1002796 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:00,485 : INFO : EPOCH 36 - PROGRESS: at 47.15% examples, 992940 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:01,489 : INFO : EPOCH 36 - PROGRESS: at 54.85% examples, 991110 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:02,494 : INFO : EPOCH 36 - PROGRESS: at 62.96% examples, 995591 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:03,499 : INFO : EPOCH 36 - PROGRESS: at 70.96% examples, 997433 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:55:04,504 : INFO : EPOCH 36 - PROGRESS: at 78.84% examples, 995606 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:05,505 : INFO : EPOCH 36 - PROGRESS: at 86.60% examples, 994448 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:06,508 : INFO : EPOCH 36 - PROGRESS: at 94.65% examples, 996098 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:07,164 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:55:07,173 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:55:07,177 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:55:07,178 : INFO : EPOCH - 36 : training on 17005207 raw words (12692628 effective words) took 12.7s, 996730 effective words/s\n",
      "2019-03-19 19:55:08,189 : INFO : EPOCH 37 - PROGRESS: at 7.82% examples, 983802 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:09,199 : INFO : EPOCH 37 - PROGRESS: at 15.70% examples, 984840 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:10,208 : INFO : EPOCH 37 - PROGRESS: at 23.52% examples, 985146 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:11,213 : INFO : EPOCH 37 - PROGRESS: at 31.39% examples, 990029 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:12,226 : INFO : EPOCH 37 - PROGRESS: at 39.21% examples, 989349 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 19:55:13,240 : INFO : EPOCH 37 - PROGRESS: at 47.09% examples, 989448 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:14,247 : INFO : EPOCH 37 - PROGRESS: at 54.85% examples, 988699 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:15,252 : INFO : EPOCH 37 - PROGRESS: at 62.61% examples, 987860 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:16,254 : INFO : EPOCH 37 - PROGRESS: at 70.55% examples, 989945 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:17,266 : INFO : EPOCH 37 - PROGRESS: at 78.72% examples, 991978 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:18,273 : INFO : EPOCH 37 - PROGRESS: at 86.89% examples, 995087 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:19,275 : INFO : EPOCH 37 - PROGRESS: at 95.06% examples, 998289 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:19,859 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:55:19,861 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:55:19,865 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:55:19,867 : INFO : EPOCH - 37 : training on 17005207 raw words (12689040 effective words) took 12.7s, 1000738 effective words/s\n",
      "2019-03-19 19:55:20,873 : INFO : EPOCH 38 - PROGRESS: at 7.88% examples, 991042 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:21,884 : INFO : EPOCH 38 - PROGRESS: at 15.76% examples, 988541 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:22,887 : INFO : EPOCH 38 - PROGRESS: at 23.75% examples, 997175 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:23,892 : INFO : EPOCH 38 - PROGRESS: at 31.51% examples, 995270 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:24,897 : INFO : EPOCH 38 - PROGRESS: at 39.39% examples, 996550 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:25,902 : INFO : EPOCH 38 - PROGRESS: at 47.27% examples, 997098 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:26,906 : INFO : EPOCH 38 - PROGRESS: at 55.09% examples, 996716 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:27,910 : INFO : EPOCH 38 - PROGRESS: at 63.14% examples, 999787 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:28,917 : INFO : EPOCH 38 - PROGRESS: at 71.25% examples, 1002661 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:29,921 : INFO : EPOCH 38 - PROGRESS: at 79.19% examples, 1001006 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:30,929 : INFO : EPOCH 38 - PROGRESS: at 87.07% examples, 1000004 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:31,931 : INFO : EPOCH 38 - PROGRESS: at 94.89% examples, 999040 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:32,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:55:32,537 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:55:32,540 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:55:32,541 : INFO : EPOCH - 38 : training on 17005207 raw words (12693927 effective words) took 12.7s, 1001851 effective words/s\n",
      "2019-03-19 19:55:33,545 : INFO : EPOCH 39 - PROGRESS: at 7.88% examples, 992430 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:34,553 : INFO : EPOCH 39 - PROGRESS: at 15.87% examples, 997629 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:35,558 : INFO : EPOCH 39 - PROGRESS: at 24.04% examples, 1010802 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:36,560 : INFO : EPOCH 39 - PROGRESS: at 31.98% examples, 1011597 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:37,569 : INFO : EPOCH 39 - PROGRESS: at 39.74% examples, 1005914 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:38,573 : INFO : EPOCH 39 - PROGRESS: at 47.44% examples, 1001035 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:39,577 : INFO : EPOCH 39 - PROGRESS: at 55.26% examples, 1000324 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:40,580 : INFO : EPOCH 39 - PROGRESS: at 63.26% examples, 1002032 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:41,593 : INFO : EPOCH 39 - PROGRESS: at 71.25% examples, 1002246 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:42,593 : INFO : EPOCH 39 - PROGRESS: at 79.07% examples, 999628 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:43,611 : INFO : EPOCH 39 - PROGRESS: at 87.18% examples, 1000590 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:44,614 : INFO : EPOCH 39 - PROGRESS: at 95.18% examples, 1001224 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-19 19:55:45,185 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:55:45,188 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:55:45,194 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:55:45,196 : INFO : EPOCH - 39 : training on 17005207 raw words (12691614 effective words) took 12.7s, 1003221 effective words/s\n",
      "2019-03-19 19:55:46,210 : INFO : EPOCH 40 - PROGRESS: at 7.82% examples, 980730 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:47,220 : INFO : EPOCH 40 - PROGRESS: at 15.34% examples, 961291 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:48,228 : INFO : EPOCH 40 - PROGRESS: at 22.93% examples, 959467 words/s, in_qsize 5, out_qsize 1\n",
      "2019-03-19 19:55:49,236 : INFO : EPOCH 40 - PROGRESS: at 31.04% examples, 977839 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:50,239 : INFO : EPOCH 40 - PROGRESS: at 39.09% examples, 987680 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:51,243 : INFO : EPOCH 40 - PROGRESS: at 46.74% examples, 984649 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:52,254 : INFO : EPOCH 40 - PROGRESS: at 54.32% examples, 980962 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:53,256 : INFO : EPOCH 40 - PROGRESS: at 62.20% examples, 983210 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:54,272 : INFO : EPOCH 40 - PROGRESS: at 70.25% examples, 985945 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:55,274 : INFO : EPOCH 40 - PROGRESS: at 77.90% examples, 982722 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:56,276 : INFO : EPOCH 40 - PROGRESS: at 85.71% examples, 983275 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-19 19:55:57,280 : INFO : EPOCH 40 - PROGRESS: at 93.77% examples, 985857 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-19 19:55:58,077 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-19 19:55:58,083 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-19 19:55:58,088 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-19 19:55:58,089 : INFO : EPOCH - 40 : training on 17005207 raw words (12691090 effective words) took 12.9s, 984979 effective words/s\n",
      "2019-03-19 19:55:58,090 : INFO : training on a 680208280 raw words (507668829 effective words) took 509.8s, 995877 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# Init the Doc2Vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "# Build the Volabulary\n",
    "model.build_vocab(train_data)\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06032689  0.18639418 -0.39468604 -0.3626974  -0.09318312  0.04502558\n",
      "  0.14077036 -0.18113542  0.5441968  -0.0596909   0.17224409 -0.0774765\n",
      "  0.25857627  0.15406573 -0.08637781  0.34137353  0.04922651  0.21410146\n",
      "  0.10973113  0.09114561 -0.17765002 -0.54862624 -0.04218041 -0.40009996\n",
      " -0.3396933  -0.27378172  0.10261839  0.42458376 -0.05328482  0.13288994\n",
      "  0.28360254  0.16502663  0.47124362 -0.1931318  -0.04624787  0.12779057\n",
      " -0.28575918 -0.42581347  0.14722782  0.18899105 -0.35213348 -0.41971874\n",
      " -0.06345312  0.62705654 -0.43539104  0.69622445 -0.06633121  0.2987238\n",
      " -0.25494263  0.15704694]\n"
     ]
    }
   ],
   "source": [
    "print(model.infer_vector(['australian', 'captain', 'elected', 'to', 'bowl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute similarity metrics like cosine similarity and soft cosine similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `similarity_matrix` (Method will be removed in 4.0.0, use gensim.models.keyedvectors.WordEmbeddingSimilarityIndex instead).\n",
      "  if __name__ == '__main__':\n",
      "2019-03-19 20:15:20,242 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7f8e2447d278>\n",
      "2019-03-19 20:15:20,244 : INFO : iterating over columns in dictionary order\n",
      "2019-03-19 20:15:20,249 : INFO : PROGRESS: at 0.19% columns (1 / 525, 0.190476% density, 0.190476% projected density)\n",
      "2019-03-19 20:15:46,268 : INFO : constructed a sparse term similarity matrix with 1.316644% density\n",
      "2019-03-19 20:15:46,278 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-03-19 20:15:46,280 : INFO : built Dictionary(14 unique tokens: ['He', 'batsman', 'too', 'chess', 'is']...) from 3 documents (total 26 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import softcossim\n",
    "from gensim import corpora\n",
    "\n",
    "sent_1 = 'Sachin is a cricket player and a opening batsman'.split()\n",
    "sent_2 = 'Dhoni is a cricket player too He is a batsman and keeper'.split()\n",
    "sent_3 = 'Anand is a chess player'.split()\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# Prepare a dictionary and a corpus.\n",
    "documents = [sent_1, sent_2, sent_3]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Convert the sentences into bag-of-words vectors.\n",
    "sent_1 = dictionary.doc2bow(sent_1)\n",
    "sent_2 = dictionary.doc2bow(sent_2)\n",
    "sent_3 = dictionary.doc2bow(sent_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788882958139703\n",
      "0.5051107137297167\n",
      "0.5620230844057823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `softcossim` (Function will be removed in 4.0.0, use gensim.similarities.termsim.SparseTermSimilarityMatrix.inner_product instead).\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `softcossim` (Function will be removed in 4.0.0, use gensim.similarities.termsim.SparseTermSimilarityMatrix.inner_product instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `softcossim` (Function will be removed in 4.0.0, use gensim.similarities.termsim.SparseTermSimilarityMatrix.inner_product instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Compute soft cosine similarity\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))\n",
    "print(softcossim(sent_1, sent_3, similarity_matrix))\n",
    "print(softcossim(sent_2, sent_3, similarity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beetroot\n"
     ]
    }
   ],
   "source": [
    "# Which word from the given list doesn't go with the others?\n",
    "print(fasttext_model300.doesnt_match(['india', 'australia', 'pakistan', 'china', 'beetroot']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22957539558410645\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine distance between two words.\n",
    "print(fasttext_model300.distance('king', 'queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22957546 0.465837   0.547001  ]\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine distances from given word or vector to all words in `other_words`.\n",
    "print(fasttext_model300.distances('king', ['queen', 'man', 'woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77042454 0.534163   0.45299897 0.76572543]\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarities\n",
    "# Note: Queen + Man is very similar to King.\n",
    "print(fasttext_model300.cosine_similarities(fasttext_model300['king'], \n",
    "                                            vectors_all=(fasttext_model300['queen'], \n",
    "                                                        fasttext_model300['man'], \n",
    "                                                        fasttext_model300['woman'],\n",
    "                                                        fasttext_model300['queen'] + fasttext_model300['man'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prince', 'queen', 'monarch']\n"
     ]
    }
   ],
   "source": [
    "# Get the words closer to w1 than w2\n",
    "print(glove_model300.words_closer_than(w1='king', w2='kingdom'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king-', 0.7838029265403748), ('boy-king', 0.7704817652702332), ('queen', 0.7704246044158936), ('prince', 0.7700966596603394), ('kings', 0.7668929696083069)]\n"
     ]
    }
   ],
   "source": [
    "# Find the top-N most similar words.\n",
    "print(fasttext_model300.most_similar(positive='king', negative=None, topn=5, restrict_vocab=None, indexer=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mesoamerican', 1.3182628154754639), ('thrombin', 1.3123780488967896), ('luther', 1.3051434755325317), ('snedden', 1.2918351888656616), ('jerod', 1.2863011360168457)]\n"
     ]
    }
   ],
   "source": [
    "# Find the top-N most similar words, using the multiplicative combination objective,\n",
    "print(glove_model300.most_similar_cosmul(positive='king', negative=None, topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.9199351072311401), ('princess', 0.8403170108795166), ('throne', 0.8287888765335083), ('monarch', 0.8201609253883362), ('elizabeth', 0.8025429248809814)]\n"
     ]
    }
   ],
   "source": [
    "# Find the top-N most similar words, using the multiplicative combination objective,\n",
    "print(glove_model300.most_similar_cosmul(positive=['woman', 'king'], negative=['man'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nurse', 0.929076075553894), ('physician', 0.9234046936035156), ('doctors', 0.9137151837348938), ('pregnant', 0.8704767227172852), ('dentist', 0.8677982687950134)]\n"
     ]
    }
   ],
   "source": [
    "print(glove_model300.most_similar_cosmul(positive=['doctor', 'woman'], negative=['man'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to summarize text documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 20:36:20,204 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-03-19 20:36:20,206 : INFO : built Dictionary(70 unique tokens: ['integr', 'liber', 'state', 'counterpart', 'armi']...) from 11 documents (total 102 corpus positions)\n",
      "2019-03-19 20:36:20,208 : INFO : Building graph\n",
      "2019-03-19 20:36:20,209 : INFO : Filling graph\n",
      "2019-03-19 20:36:20,211 : INFO : Removing unreachable nodes of graph\n",
      "2019-03-19 20:36:20,213 : INFO : Pagerank graph\n",
      "2019-03-19 20:36:20,217 : INFO : Sorting pagerank scores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the PLA Rocket Force national defense science and technology experts panel, '\n",
      " 'according to a report published by the')\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize, keywords\n",
    "from pprint import pprint\n",
    "\n",
    "text = \" \".join((line for line in smart_open('sample.txt', encoding='utf-8')))\n",
    "\n",
    "# Summarize the paragraph\n",
    "pprint(summarize(text, word_count=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experts\n",
      "force\n",
      "zhang\n",
      "pla\n",
      "rocket\n",
      "missiles missile\n"
     ]
    }
   ],
   "source": [
    "# Important keywords from the paragraph\n",
    "print(keywords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'he' is to 'his' as 'she' is to 'her'\n",
      "'big' is to 'bigger' as 'bad' is to 'worse'\n",
      "'going' is to 'went' as 'being' is to 'subsequently'\n"
     ]
    }
   ],
   "source": [
    "# \"boy\" is to \"father\" as \"girl\" is to ...?\n",
    "glove_model300.most_similar(['girl', 'father'], ['boy'], topn=3)\n",
    "more_examples = [\"he his she\", \"big bigger bad\", \"going went being\"]\n",
    "for example in more_examples:\n",
    "    a, b, x = example.split()\n",
    "    predicted = glove_model300.most_similar([x, b], [a])[0][0]\n",
    "    print(\"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which word doesn't go with the others?\n",
    "glove_model300.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
