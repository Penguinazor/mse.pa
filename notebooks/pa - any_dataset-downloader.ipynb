{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Downloader\n",
    "- **Where:** Any notebook\n",
    "- **Datasets:**\n",
    "    - enwiki-latest-pages-articles.xml.bz2 (16GB)\n",
    "\n",
    "## What's going on\n",
    "- Downloading the required datasets for the project\n",
    "\n",
    "## Results\n",
    "- **Timeframe:** \n",
    "- **Errors:** Nil\n",
    "- **Comments:**\n",
    "    - Don't forget to run the Package Installer Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: enwiki-latest-pages-articles.xml.bz2\n",
      "Dataset is missing.\n",
      "Data folder was not present.\n",
      "Downloading the dataset now...\n"
     ]
    }
   ],
   "source": [
    "# Downloading the latest full English Wikipedia Dump\n",
    "folder_name = \"datasets/\"\n",
    "dataset_url = \"https://dumps.wikimedia.org/enwiki/latest/\"\n",
    "datasets_list = [\n",
    "    \"enwiki-latest-pages-articles.xml.bz2\"\n",
    "]\n",
    "    \n",
    "import wget\n",
    "import os\n",
    "\n",
    "SCRIPT_PATH = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "DATA_PATH   = os.path.join(SCRIPT_PATH, folder_name)\n",
    "\n",
    "for datafile_name in datasets_list:\n",
    "    print(\"Processing: \" + datafile_name)\n",
    "    DATA_FILEPATH = os.path.join(DATA_PATH, datafile_name)\n",
    "\n",
    "    if os.path.exists(DATA_FILEPATH):\n",
    "        print('Data file is already in folder')\n",
    "    else:\n",
    "        print('Dataset is missing.')\n",
    "        if not os.path.exists(DATA_PATH):\n",
    "            print(\"Data folder was not present.\")\n",
    "            os.mkdir(DATA_PATH)\n",
    "\n",
    "        print('Downloading the dataset now...')\n",
    "        url = dataset_url+datafile_name\n",
    "        wget.download(url=url,\n",
    "                     out=DATA_FILEPATH)\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: eng_news_2016_1M.tar.gz\n",
      "Data file is already in folder\n",
      "Processing: eng_news_2015_1M.tar.gz\n",
      "Dataset is missing.\n",
      "Downloading the dataset now...\n",
      "Done\n",
      "Processing: eng_news_2014_1M.tar.gz\n",
      "Dataset is missing.\n",
      "Downloading the dataset now...\n",
      "Done\n",
      "Processing: eng_news_2013_1M.tar.gz\n",
      "Dataset is missing.\n",
      "Downloading the dataset now...\n",
      "Done\n",
      "Processing: eng_news_2010_1M.tar.gz\n",
      "Dataset is missing.\n",
      "Downloading the dataset now...\n",
      "Done\n",
      "Processing: eng_news_2009_1M.tar.gz\n",
      "Dataset is missing.\n",
      "Downloading the dataset now...\n",
      "Done\n",
      "Processing: eng_news_2008_1M.tar.gz\n",
      "Dataset is missing.\n",
      "Downloading the dataset now...\n",
      "Done\n",
      "Processing: eng_news_2007_1M.tar.gz\n",
      "Dataset is missing.\n",
      "Downloading the dataset now...\n",
      "Done\n",
      "Processing: eng_news_2006_1M.tar.gz\n",
      "Dataset is missing.\n",
      "Downloading the dataset now...\n",
      "Done\n",
      "Processing: eng_news_2005_1M.tar.gz\n",
      "Dataset is missing.\n",
      "Downloading the dataset now...\n",
      "Done\n",
      "Processing: eng_news-typical_2016_1M.tar.gz\n",
      "Dataset is missing.\n",
      "Downloading the dataset now...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Downloading the Leipzig Corpora Collection\n",
    "# http://wortschatz.uni-leipzig.de/en/download/\n",
    "folder_name = \"datasets/\"\n",
    "dataset_url = \"http://pcai056.informatik.uni-leipzig.de/downloads/corpora/\"\n",
    "datasets_list = [\n",
    "    \"eng_news_2016_1M.tar.gz\",\n",
    "    \"eng_news_2015_1M.tar.gz\",\n",
    "    \"eng_news_2014_1M.tar.gz\",\n",
    "    \"eng_news_2013_1M.tar.gz\",\n",
    "    \"eng_news_2010_1M.tar.gz\",\n",
    "    \"eng_news_2009_1M.tar.gz\",\n",
    "    \"eng_news_2008_1M.tar.gz\",\n",
    "    \"eng_news_2007_1M.tar.gz\",\n",
    "    \"eng_news_2006_1M.tar.gz\",\n",
    "    \"eng_news_2005_1M.tar.gz\",\n",
    "    \"eng_news-typical_2016_1M.tar.gz\"\n",
    "]\n",
    "    \n",
    "import wget\n",
    "import os\n",
    "\n",
    "SCRIPT_PATH = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "DATA_PATH   = os.path.join(SCRIPT_PATH, folder_name)\n",
    "\n",
    "for datafile_name in datasets_list:\n",
    "    print(\"Processing: \" + datafile_name)\n",
    "    DATA_FILEPATH = os.path.join(DATA_PATH, datafile_name)\n",
    "\n",
    "    if os.path.exists(DATA_FILEPATH):\n",
    "        print('Data file is already in folder')\n",
    "    else:\n",
    "        print('Dataset is missing.')\n",
    "        if not os.path.exists(DATA_PATH):\n",
    "            print(\"Data folder was not present.\")\n",
    "            os.mkdir(DATA_PATH)\n",
    "\n",
    "        print('Downloading the dataset now...')\n",
    "        url = dataset_url+datafile_name\n",
    "        wget.download(url=url,\n",
    "                     out=DATA_FILEPATH)\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
