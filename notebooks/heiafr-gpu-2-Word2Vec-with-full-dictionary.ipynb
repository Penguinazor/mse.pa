{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Training Experiments\n",
    "- **Where:** HEIA-FR GPU-2\n",
    "- **Dataset:** enwiki-latest-pages-articles.xml.bz2 (16GB)\n",
    "- **Dictionary:** enwiki-english-wordids.txt.bz2 (16MB)\n",
    "\n",
    "## What's going on\n",
    "- Training a Word2Vec on the full wikipedia english dataset using its full pre-extracted dictionary.\n",
    "\n",
    "## Results\n",
    "- **Timeframe:** ~17h\n",
    "- **Errors:** kernel crash when *resetting layer weights*\n",
    "- **Comments:**\n",
    "    - Those 17 hours of training are lost as it was not possible to save the model before the crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constantes\n",
    "datafile_name = \"enwiki-latest-pages-articles.xml.bz2\"\n",
    "dictionary_name = 'enwiki-english-wordids.txt.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start logging process at root level\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary from file\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary.load_from_text(dictionary_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build WikiCorpus based on the dictionary\n",
    "from gensim.corpora import WikiCorpus\n",
    "wiki = WikiCorpus(datafile_name, dictionary=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simple sentence iterator required for the Word2Vec model\n",
    "# Bypass memory errors\n",
    "class SentencesIterator:\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sentence in self.wiki.get_texts():\n",
    "            yield list(map(lambda x: x.decode('utf-8'), sentence))\n",
    "\n",
    "sentences = SentencesIterator(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "model = Word2Vec(sentences=sentences, size=300, min_count=1, window=5, workers=cores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
